---
title: "Fitting Exercise"
---


## Part 1 - Data Processing


Data on a drug candidate called Mavoglurant is available in this [GitHub](https://github.com/metrumresearchgroup/BayesPBPK-tutorial) repository. First, let's process the raw data to make it ready for data exploration and analysis.


```{r}
#Load the required packages
library(dplyr)
library(ggplot2)
library(gtsummary)
library(tidymodels)
library(caret)
library(purrr)

#Load the raw data
mavoglurant <- read.csv("mavoglurant.csv")

#Get an overview and summary of the raw data
str(mavoglurant)
summary(mavoglurant)

#Plot DV as a function of TIME, stratified by DOSE and using ID as a grouping factor
ggplot(mavoglurant, aes(x = TIME, y = DV, group = ID, color = factor(ID))) +
  geom_line() +
  geom_point() +
  facet_wrap(~ DOSE) +
  labs(x = "TIME", y = "DV", title = "DV vs. TIME, Stratified by DOSE and Grouped by ID")

#Keep only observations with OCC = 1
mavoglurant2 <- subset(mavoglurant, OCC == "1")

#Use the dataset with OCC = 1 to create a subset dataset that removes TIME = 0 values for each individual
No_TIME_0 <- subset(mavoglurant2, TIME != 0.000)

#Use the dataset without TIME = 0  to create a new dataset that groups by ID to create a new variable Y that is the sum of the DV variable for each individual
DV_sum <- No_TIME_0 %>% group_by(ID) %>% summarize(Y = sum(DV))

#Use the dataset with OCC = 1 to create a subset dataset that has only TIME = 0
TIME_0 <- subset(mavoglurant2, TIME == 0.000)

#Combine the dataset without TIME = 0 with the dataset with only ID and Y
mavoglurant3 <- merge(TIME_0, DV_sum, by = "ID")

#Convert RACE and SEX to factor variables in the combined dataset
mavoglurant3$RACE <- as.factor(mavoglurant3$RACE)
mavoglurant3$SEX <- as.factor(mavoglurant3$SEX)

#Use the combined dataset and keep only the following variables: Y, DOSE, RATE, AGE, SEX, RACE, WT, HT
mavoglurant4 <- mavoglurant3[, c("Y", "DOSE", "RATE", "AGE", "SEX", "RACE", "WT", "HT")]

#Get an overview and summary of the processed data
str(mavoglurant4)
summary(mavoglurant4)
```


The processed dataset has 120 observations and 8 variables: Y, DOSE, RATE, AGE, SEX, RACE, WT, and HT.


## Part 2 - Data Exploration


Now let's explore each variable in the processed dataset, starting with the outcome variable, Y.


```{r}
#Attach the processed data
attach(mavoglurant4)

#Summary statistics and histogram for the outcome variable Y
mavoglurant4$Y <- as.numeric(mavoglurant4$Y)
summary(Y)

ggplot(mavoglurant4, aes(x = Y)) +
  geom_histogram() +
  labs(x = "Total Drug, Y", y = "Frequency", title = "Histogram of Total Drug, Y")
```


The mean and median for the outcome variable (total drug, Y) are the same (854). The histogram of Y shows a relatively normal distribution for most of the data (Y < 4000). There are a few observations for Y > 4000.


Now let's explore the variable DOSE.


```{r}
#Summary tables for the variable DOSE
table(DOSE, useNA = "always")
prop.table(table(DOSE))
```


For the variable DOSE, 49.2% of the observations are "25", 10.0% are "37.5", and 40.8% are "50". There are no missing observations for DOSE.


Now let's explore the variable RATE.


```{r}
#Summary tables for the variable RATE
table(RATE, useNA = "always")
prop.table(table(RATE))
```


For the variable RATE, 0.8% of the observations are "75", 48.3% are "150", 10.0% are "225", and 40.8% are "300". There are no missing observations for RATE.


Now let's explore the variable AGE.


```{r}
#Summary statistics and histogram for the variable AGE
mavoglurant4$AGE <- as.numeric(mavoglurant4$AGE)
summary(AGE)

ggplot(mavoglurant4, aes(x = AGE)) +
  geom_histogram() +
  labs(x = "Age", y = "Frequency", title = "Histogram of Age")
```


The mean (33) and median (31) for the variable AGE are similar. The histogram of AGE shows a relatively normal distribution for most of the data (AGE < 4000). There are a few observations for AGE > 4000.


Now let's explore the variable SEX.


```{r}
#Summary tables for the variable SEX
table(SEX, useNA = "always")
prop.table(table(SEX))
```


For the variable SEX, 86.7% of the observations are "1", and 13.3% are "2". There are no missing observations for SEX.


Now let's explore the variable RACE


```{r}
#Summary tables for the variable RACE
table(RACE, useNA = "always")
prop.table(table(RACE))
```


For the variable RACE, 61.7% of the observations are "1", 30.0% are "2", 1.7% are "7", and 6.7% are "88". There are no missing observations for RACE, although "88" may represent missing/unknown.


Now let's explore the variable WT.


```{r}
#Summary statistics and histogram for the variable WT
mavoglurant4$WT <- as.numeric(mavoglurant4$WT)
summary(WT)

ggplot(mavoglurant4, aes(x = WT)) +
  geom_histogram() +
  labs(x = "Weight", y = "Frequency", title = "Histogram of Weight")
```


The mean (83) and median (82) for the variable WT are similar. The histogram of WT shows a relatively normal distribution for the entire data.


Now let's explore the variable HT.


```{r}
#Summary statistics and histogram for the variable HT
mavoglurant4$HT <- as.numeric(mavoglurant4$HT)
summary(HT)

ggplot(mavoglurant4, aes(x = HT)) +
  geom_histogram() +
  labs(x = "Height", y = "Frequency", title = "Histogram of Height")
```


The mean and median for the variable HT are the same (1.8). The histogram of HT shows a left-skewing distribution.


Here is a summary table of the outcome and predictors, stratified by dose.


```{r}
#Create a summary table of the variables
mavoglurant4 %>% tbl_summary(by = DOSE, statistic = list(all_continuous() ~ "{mean} ({sd})")) %>%
  modify_caption("Summary Table of Mavoglurant Data, Stratified by Dose")
```


Finally, let's create some scatterplots between the main outcome (total drug, Y) and the continuous predictors, as well as a correlation table.


```{r}
#Create scatterplots between Y and the continuous predictors
pairs(cbind(Y, AGE, WT, HT))

#Create a correlation matrix  of the continuous variables
cor(cbind(Y, AGE, WT, HT))

#Create a boxplot of DOSE and Y
ggplot(mavoglurant4, aes(x = factor(DOSE), y = Y)) +
  geom_boxplot() +
  labs(x = "Dose", y = "Drug Total", title = "Boxplot of Dose and Total Drug")

#Create a boxplot of RATE and Y
ggplot(mavoglurant4, aes(x = factor(RATE), y = Y)) +
  geom_boxplot() +
  labs(x = "Rate", y = "Drug Total", title = "Boxplot of Rate and Total Drug")

#Create a boxplot of SEX and Y
ggplot(mavoglurant4, aes(x = factor(SEX), y = Y)) +
  geom_boxplot() +
  labs(x = "Sex", y = "Drug Total", title = "Boxplot of Sex and Total Drug")

#Create a boxplot of RACE and Y
ggplot(mavoglurant4, aes(x = factor(RACE), y = Y)) +
  geom_boxplot() +
  labs(x = "Race", y = "Drug Total", title = "Boxplot of Race and Total Drug")
```


Based on the scatterplots and the correlation matrix, there doesn't seem to be a strong correlation between the outcome (total drug, Y) and any of the continuous predictors. The boxplots of total drug, Y, and the categorical variables shows a relatively normal distribution.


## Part 3 - Model Fitting


Finally, let's do some model fitting. First, let's fit a linear model to the outcome, Y, using DOSE as the predictor.


```{r}
#Fit a linear model to the outcome Y using DOSE as the predictor
linear_model <- linear_reg() %>% set_engine("lm")
linear_fit1 <- linear_model %>% fit(Y ~ DOSE, data = mavoglurant4)
tidy(linear_fit1)

#Compute RMSE and R-squared
metrics1 <- linear_fit1 %>% 
  predict(mavoglurant4) %>% 
  bind_cols(mavoglurant4) %>% 
  metrics(truth = Y, estimate = .pred)

#Print RMSE and R-squared
print(metrics1)
```


In Model 1, the variable DOSE is significantly associated with the outcome Y (p-value < 0.001). The total drug (Y) increases by 58.21 units for every unit change in DOSE. R-squared = 0.5156 indicates that 51.56% of the variability in total drug (Y) is explained by the model. RMSE = 666.46 represents the average distance between the predicted values from the model and the actual values in the dataset.


Now let's fit a linear model to the outcome, Y, using all other variables as the predictors.


```{r}
#Fit a linear model to the outcome Y using all other variables as the predictors
linear_model <- linear_reg() %>% set_engine("lm")
linear_fit2 <- linear_model %>% fit(Y ~ ., data = mavoglurant4)
tidy(linear_fit2)

#Compute RMSE and R-squared
metrics2 <- linear_fit2 %>% 
  predict(mavoglurant4) %>% 
  bind_cols(mavoglurant4) %>% 
  metrics(truth = Y, estimate = .pred)

#Print RMSE and R-squared
print(metrics2)
```


In Model 2, the variable DOSE is significantly associated with the outcome Y (p-value = 0.005) when the other predictors are included in the model. Adjusting for the other variables, the total drug (Y) increases by 146.87 units for every unit change in DOSE. R-squared = 0.6292 indicates that 62.92% of the variability in total drug (Y) is explained by the model. RMSE = 583.09 represents the average distance between the predicted values from the model and the actual values in the dataset. 

Both Model 1 and Model 2, DOSE is significantly associated with Y, which makes sense because we would expect an increase in total drug (Y) when increasing dose. The model with all predictors seems to be better than the model with just DOSE.


Next let's fit a logistic model to the outcome, SEX, using DOSE as the predictor


```{r}
#Fit a logistic model to the outcome SEX using DOSE as the predictor
logistic_model <- logistic_reg() %>% set_engine("glm")
logistic_fit1 <- logistic_model %>% fit(SEX ~ DOSE, data = mavoglurant4)
tidy(logistic_fit1)

#Compute accuracy
accuracy1 <- logistic_fit1 %>% 
  predict(mavoglurant4) %>% 
  bind_cols(mavoglurant4) %>% 
  metrics(truth = SEX, estimate = .pred_class) %>% 
  filter(.metric == "accuracy") 

#Compute ROC-AUC
auc1 <-  logistic_fit1 %>%
  predict(mavoglurant4, type = "prob") %>%
  bind_cols(mavoglurant4) %>%
  roc_auc(truth = SEX, .pred_1)

#Print accuracy and ROC-AUC
print(accuracy1)
print(auc1)
```


In Model 3, the variable DOSE is not significantly associated with the outcome SEX (p-value = 0.19). The odds of the outcome for a dose is 0.97 times the odds of the outcome for a dose one unit less. The accuracy is 0.87, and the ROC-AUC is 0.59.


Finally, let's fit a logistic model to the outcome, SEX, using all other variables as the predictors.


```{r}
#Fit a logistic model to the outcome SEX using all other variables as the predictors
logistic_model <- logistic_reg() %>% set_engine("glm")
logistic_fit2 <- logistic_model %>% fit(SEX ~ ., data = mavoglurant4)
tidy(logistic_fit2)

#Compute accuracy
accuracy2 <- logistic_fit2 %>% 
  predict(mavoglurant4) %>% 
  bind_cols(mavoglurant4) %>% 
  metrics(truth = SEX, estimate = .pred_class) %>% 
  filter(.metric %in% c("accuracy"))

#Compute ROC-AUC
auc2 <-  logistic_fit2 %>%
  predict(mavoglurant4, type = "prob") %>%
  bind_cols(mavoglurant4) %>%
  roc_auc(truth = SEX, .pred_1)

#Print accuracy and ROC-AUC
print(accuracy2)
print(auc2)
```


In Model 4, the variable DOSE is not significantly associated with the outcome SEX (p-value = 0.996) when the other predictors are included in the model. Adjusting for the other variables, the odds of the outcome for a dose is 0.43 times the odds of the outcome for a dose one unit less. The accuracy is 0.94, and the ROC-AUC is 0.98.

Both Model 3 and Model 4, DOSE is not significantly associated with SEX, which makes sense because we would not necessarily expect a relationship between those two variables. The model with all predictors is more accurate than the model with just DOSE.


## Part 4 - Model Improvement


Let's do some more changes to the data. First, let's remove the RATE and RACE variables since some values are coded "7" or "88", which most likely indicates missing values. Then let's set a seed and split the data randomly into a 75% train and 25% test set.


```{r}
#Create a subset dataset without RACE
mavoglurant5 <- mavoglurant4[, c("Y", "DOSE", "AGE", "SEX", "WT", "HT")]

#Set a seed
rngseed = 1234
set.seed(rngseed)

#Split the data randomly into 75% train and 25% test set
data_split <- initial_split(mavoglurant5, prop = 3/4)
train_data <- training(data_split)
test_data <- testing(data_split)
```


Now let's do some model fitting with the training data. First, let's fit a linear model to the outcome, Y, using DOSE as the predictor.


```{r}
#Fit a linear model to the outcome Y using DOSE as the predictor
linear_model <- linear_reg() %>% set_engine("lm")
linear_fit1 <- linear_model %>% fit(Y ~ DOSE, data = train_data)
tidy(linear_fit1)

#Compute RMSE and R-squared
metrics1 <- linear_fit1 %>% 
  predict(train_data) %>% 
  bind_cols(train_data) %>% 
  metrics(truth = Y, estimate = .pred)

#Print RMSE and R-squared
print(metrics1)
```


In Model 1, the variable DOSE is significantly associated with the outcome Y (p-value < 0.001). The total drug (Y) increases by 53.42 units for every unit change in DOSE. R-squared = 0.4508 indicates that 45.08% of the variability in total drug (Y) is explained by the model. RMSE = 702.81 represents the average distance between the predicted values from the model and the actual values in the dataset.


Now let's fit a linear model to the outcome, Y, using all other variables as the predictors.


```{r}
#Fit a linear model to the outcome Y using all other variables as the predictors
linear_model <- linear_reg() %>% set_engine("lm")
linear_fit2 <- linear_model %>% fit(Y ~ ., data = train_data)
tidy(linear_fit2)

#Compute RMSE and R-squared
metrics2 <- linear_fit2 %>% 
  predict(train_data) %>% 
  bind_cols(train_data) %>% 
  metrics(truth = Y, estimate = .pred)

#Print RMSE and R-squared
print(metrics2)
```


In Model 2, the variable DOSE is significantly associated with the outcome Y (p-value < 0.001) when the other predictors are included in the model. Adjusting for the other variables, the total drug (Y) increases by 55.34 units for every unit change in DOSE. R-squared = 0.5623 indicates that 56.23% of the variability in total drug (Y) is explained by the model. RMSE = 627.44 represents the average distance between the predicted values from the model and the actual values in the dataset. 

Both Model 1 and Model 2, DOSE is significantly associated with Y, which makes sense because we would expect an increase in total drug (Y) when increasing dose. The model with all predictors seems to be better than the model with just DOSE.


Next let's compare the RMSE values for the null model, Model 1, and Model 2. 


```{r}
#Fit a null model to the outcome Y
linear_model <- linear_reg() %>% set_engine("lm")
null_model <- linear_model %>% fit(Y ~ 1, data = train_data)
tidy(null_model)

#Compute RMSE for the null model
metrics_null <- null_model %>% 
  predict(train_data) %>% 
  bind_cols(train_data) %>% 
  metrics(truth = Y, estimate = .pred)

#Print RMSE for the null model, Model 1, and Model 2
print(metrics_null)
print(metrics1)
print(metrics2)
```


Using the training data and comparing the RMSE values for the three models, the best-performing model is Model 2 with all predictors (RMSE = 627.44), and the worst-performing model is the null model (RMSE = 948.35).


Now let's do a 10-fold cross-validation to compute performance on unseen data. First, let's define the training control as cross-validation and K = 10 folds. Then let's compute the RMSE values for Model 1 and Model using 10-fold cross-validation.


```{r}
#Set a seed
rngseed = 1234
set.seed(rngseed)

#Define the training control as cross-validation and K = 10 folds
train_control <- trainControl(method = "cv", number = 10)

#Fit a linear model to the outcome Y using DOSE as the predictor
model1 <- train(Y ~ DOSE, data = train_data, method = "lm", trCrontol = train_control)

#Fit a linear model to the outcome Y using all other variables as the predictors
model2 <- train(Y ~ ., data = train_data, method = "lm", trCrontrol = train_control)

#Print RMSE for Model 1 and Model 2
print(model1)
print(model2)
```


Using 10-fold cross-validation on the training data and comparing the RMSE values for the two models, Model 1 with just DOSE (RMSE = 691.80) performs slightly better than Model 2 with all predictors (RMSE = 696.23). In the previous analysis without cross-validation, Model 2 (RMSE = 627.44) performed better than Model 1 (RMSE = 702.81).


Finally, let's re-create the cross-validation folds and re-fit the linear models using a different value for the random seed.


```{r}
#Set a seed
rngseed = 5678
set.seed(rngseed)

#Split the data randomly into 75% train and 25% test set
data_split <- initial_split(mavoglurant5, prop = 3/4)
train_data <- training(data_split)
test_data <- testing(data_split)

#Fit a linear model to the outcome Y using DOSE as the predictor
linear_model <- linear_reg() %>% set_engine("lm")
linear_fit1 <- linear_model %>% fit(Y ~ DOSE, data = train_data)
tidy(linear_fit1)

#Fit a linear model to the outcome Y using all other variables as the predictors
linear_model <- linear_reg() %>% set_engine("lm")
linear_fit2 <- linear_model %>% fit(Y ~ ., data = train_data)
tidy(linear_fit2)

#Compute RMSE
metrics1 <- linear_fit1 %>% 
  predict(train_data) %>% 
  bind_cols(train_data) %>% 
  metrics(truth = Y, estimate = .pred)

metrics2 <- linear_fit2 %>% 
  predict(train_data) %>% 
  bind_cols(train_data) %>% 
  metrics(truth = Y, estimate = .pred)

#Print RMSE
print(metrics1)
print(metrics2)

#Define the training control as cross-validation and K = 10 folds
train_control <- trainControl(method = "cv", number = 10)

#Fit a linear model to the outcome Y using DOSE as the predictor
model1 <- train(Y ~ DOSE, data = train_data, method = "lm", trCrontol = train_control)

#Fit a linear model to the outcome Y using all other variables as the predictors
model2 <- train(Y ~ ., data = train_data, method = "lm", trCrontrol = train_control)

#Print RMSE for Model 1 and Model 2
print(model1)
print(model2)
```


Using a different random seed for the training data and comparing the RMSE values for the two models, Model 2 with all predictors (RMSE = 616.21) performs better than Model 1 with just DOSE (RMSE = 693.89). Using the same seed for the 10-fold cross-validation, Model 1 with just DOSE (RMSE = 693.70) performs slightly better than Model 2 with all predictors (RMSE = 697.21). The overall pattern between changes in the RMSE values for model fitting to the training data with and without cross-validation is the same when using different random seeds.


## THIS SECTION WAS ADDED BY EMMA HARDIN-PARKER 

Plotting the observed vs. predicted values for the three original model fits, including the null model. 

```{r}
# Augmenting predicted & observed values
a_1 <- augment(linear_fit1, new_data = train_data)
a_2 <- augment(linear_fit2, new_data = train_data)
a_n <- augment(null_model, new_data = train_data)

# Combining into dataframe 
combined_augmented <- bind_rows(
  a_1 %>% mutate(Model = "Linear Fit 1"),
  a_2 %>% mutate(Model = "Linear Fit 2"),
  a_n %>% mutate(Model = "Null Model")
)


# Create the plot
obs_pred_plot <- ggplot(combined_augmented, aes(x = Y, y = .pred, color = Model, shape = Model)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  xlim(0, 5000) +
  ylim(0, 5000) +
  labs(
    title = "Observed vs Predicted Values",
    x = "Observed",
    y = "Predicted",
    color = "Model",
    shape = "Model"
  ) +
  theme_minimal()

# Display the plot
print(obs_pred_plot)

```

Using bootstrapping to sample the data, fit models to the data, and calculate uncertainty. 

Setting the seed, again. 
```{r}
rngseed = 1234
set.seed(rngseed)
```

Creating 100 bootstraps using the bootstrap() function from the rsample package and creating a plot of the observed vs. predicted values along with Bootstrap Confidence Intervals.

```{r}
# Create 100 bootstraps using the training data
bootstraps <- bootstraps(data = train_data, times = 100)

# Create empty vector to store predictions 
preds_bs <- list()

# Loop through each bootstrap sample

for (i in 1:length(bootstraps$splits)) {
  # Get a single bootstrap sample
  dat_sample <- rsample::analysis(bootstraps$splits[[i]])
  
  # Fit the model to the bootstrap sample
  linear_fit_bs <- linear_model %>% fit(Y ~ ., data = dat_sample)
  
  # Make predictions for the original training data
  pred <- predict(linear_fit_bs, new_data = train_data)
  
  # Store predictions in the list
  preds_bs[[i]] <- pred
}

# Convert list of predictions to a matrix
pred_matrix <- do.call(cbind, preds_bs)

# Compute mean and confidence intervals
preds <- apply(pred_matrix, 1, quantile, c(0.055, 0.5, 0.945)) %>% t()
preds <- data.frame(preds)


# Plotting the figure
obs_pred_plot_2 <- ggplot() +
  geom_point(data = train_data, aes(x = Y, y = preds[, 2]), color = "black", size = 3) +  # Point estimate
  geom_point(data = train_data, aes(x = Y, y = preds[, 1]), color = "blue", size = 2) +    # Lower bound
  geom_point(data = train_data, aes(x = Y, y = preds[, 3]), color = "red", size = 2) +   # Upper bound
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +                           # 45-degree line
  xlim(0, 5000) +
  ylim(0, 5000) +
  labs(
    title = "Observed vs Predicted Values with Bootstrap Confidence Intervals",
    x = "Observed",
    y = "Predicted",
    color = "Estimates"
  ) +
  theme_minimal() +
  labs(x = "Observed Values", y = "Predicted Values", title = "Observed vs. Predicted Values for Model 2") 


# Show the plot
print(obs_pred_plot_2)

```


```{r}
#Create a data frame with the observed and predicted values for Model 2
mod2_df <- lm(Y ~ ., data = train_data)
mod2_df2 <- fortify(mod2_df)

#Add the median and confidence intervals to the same data frame as the observed and predicted values for Model 2
mod2_df2$lower <- preds$X5.5.
mod2_df2$median <- preds$X50.
mod2_df2$upper <- preds$X94.5.

#Create a figure that plots observed values on the x-axis and predicted values on the y-axis
obs_mod2 <- ggplot(mod2_df2, aes(x = Y)) +
  geom_point(aes(y = .fitted, color = "Point Estimates"), alpha = 0.7) +
  geom_point(aes(y = lower, color = "Lower Bounds"), alpha = 0.7) +
  geom_point(aes(y = median, color = "Medians"), alpha = 0.7) +
  geom_point(aes(y = upper, color = "Upper Bounds"), alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  xlim(0, 5000) +
  ylim(0, 5000) +
  labs(x = "Observed Values", y = "Predicted Values", title = "Observed vs. Predicted Values for Model 2") +
  theme_minimal()
obs_mod2
```

I ultimately decided to create the plot in two different ways as a way to confirm that I was achieving the intended results. All of the lower bound points, median points, point estimates, and upper bound points tend to follow along the 45 degree line included in the plot. This indicates that the model is performing well, has high level of goodness of fit, and consistency in which the model is not systematically overestimating or underestimating the outcomes.


## Final Evaluation


Let's do a final model evaluation, this time using the test data. First, let's use the fit of Model 2 on the training data to make predictions for the test data. Then let's make a plot that shows predicted versus observed values for both the training data and the test data.


```{r}
#Create a data frame with the observed and predicted values for the training data
mod2_df <- lm(Y ~ ., data = train_data)
mod2_df2 <- fortify(mod2_df)

#Make predictions for the test data
pred_test <- as.data.frame(predict(mod2_df, newdata = test_data))

#Add the Y values to the same data frame as the test data predictions
mod2_df2_test <- fortify(lm(Y ~ ., data = test_data))

#Combine the data frames with the observed and predicted values for the test data
combined_test <- cbind(mod2_df2_test, pred_test)

#Merge the data frames with the training data and test data predictions
merged <- merge(mod2_df2, combined_test, by = "Y", all = TRUE)

#Rename the test data predictions variable
merged <- merged %>% rename(pred_test = `predict(mod2_df, newdata = test_data)`)

#Create a figure that plots observed values on the x-axis and predicted values on the y-axis
ggplot(merged, aes(x = Y)) +
  geom_point(aes(y = .fitted.x, color = "Training Data"), alpha = 0.7) +
  geom_point(aes(y = pred_test, color = "Test Data"), alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  xlim(0, 5000) +
  ylim(0, 5000) +
  labs(x = "Observed Values", y = "Predicted Values", title = "Observed vs. Predicted Values for the Training and Test Data") +
  theme_minimal()
```


The plot shows that the observed and predicted values for the test data are mixed in with the training data, which is what we want to see. If the test data points were systematically "off", then it would indicate a problem, such as overfitting to the training data.


Finally, let's critique all models. Both Model 1 and Model 2 perform better than the null model based on the RMSE values. Model 1 with only DOSE as the predictor improves the results over the null model (i.e., smaller RMSE). For both Model 1 and Model 2, DOSE is significantly associated with Y, which makes sense because we would expect an increase in total drug (Y) when increasing dose. Because the inclusion of other predictors in the model helps to better explain the outcome pattern, we would not consider Model 1 usable for any real-life purpose. Additional analyses, including the bootstrap method and making predictions for the test data, indicate that Model 2 with all predictors seems to be better than Model 1 with just DOSE. 


