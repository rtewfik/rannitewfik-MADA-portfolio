[
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Introduction\nData on 2023 and 2024 U.S. solar eclipses are available in this TidyTuesday GitHub repository. The data source is NASA’s Scientific Visualization Studio. TidyTuesday is organized by the Data Science Learning Community.\n\n\nData Loading & Processing\nFirst, let’s load the four datasets, add some new variables, and combine them into one dataset for data exploration.\n\n#Load the required packages\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(hms)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.3.2\n\n\n#BlackLivesMatter\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ infer        1.0.5     ✔ tidyr        1.3.0\n✔ modeldata    1.2.0     ✔ tune         1.1.2\n✔ parsnip      1.1.1     ✔ workflows    1.1.3\n✔ purrr        1.0.2     ✔ workflowsets 1.0.1\n✔ recipes      1.0.8     ✔ yardstick    1.2.0\n\n\nWarning: package 'dials' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.2\n\n\nWarning: package 'modeldata' was built under R version 4.3.2\n\n\nWarning: package 'parsnip' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'recipes' was built under R version 4.3.2\n\n\nWarning: package 'rsample' was built under R version 4.3.2\n\n\nWarning: package 'tune' was built under R version 4.3.2\n\n\nWarning: package 'workflows' was built under R version 4.3.2\n\n\nWarning: package 'workflowsets' was built under R version 4.3.2\n\n\nWarning: package 'yardstick' was built under R version 4.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ recipes::all_double()  masks gtsummary::all_double()\n✖ recipes::all_factor()  masks gtsummary::all_factor()\n✖ recipes::all_integer() masks gtsummary::all_integer()\n✖ recipes::all_logical() masks gtsummary::all_logical()\n✖ recipes::all_numeric() masks gtsummary::all_numeric()\n✖ purrr::discard()       masks scales::discard()\n✖ dplyr::filter()        masks stats::filter()\n✖ dplyr::lag()           masks stats::lag()\n✖ recipes::step()        masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.2\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(purrr)\nlibrary(rsample)\nlibrary(boot)\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:lattice':\n\n    melanoma\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.3.3\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:boot':\n\n    logit\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nlibrary(glmnet)\n\nWarning: package 'glmnet' was built under R version 4.3.3\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.3.2\n\n#Load the raw data\nannular2023 &lt;- read.csv(\"eclipse_annular_2023.csv\")\npartial2023 &lt;- read.csv(\"eclipse_partial_2023.csv\")\npartial2024 &lt;- read.csv(\"eclipse_partial_2024.csv\")\ntotal2024 &lt;- read.csv(\"eclipse_total_2024.csv\")\n\n#Get a glimpse of the raw data\nglimpse(annular2023)\n\nRows: 811\nColumns: 10\n$ state     &lt;chr&gt; \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", …\n$ name      &lt;chr&gt; \"Chilchinbito\", \"Chinle\", \"Del Muerto\", \"Dennehotso\", \"Fort …\n$ lat       &lt;dbl&gt; 36.49200, 36.15115, 36.18739, 36.82900, 35.74750, 36.71717, …\n$ lon       &lt;dbl&gt; -110.0492, -109.5787, -109.4359, -109.8757, -109.0680, -110.…\n$ eclipse_1 &lt;chr&gt; \"15:10:50\", \"15:11:10\", \"15:11:20\", \"15:10:50\", \"15:11:40\", …\n$ eclipse_2 &lt;chr&gt; \"15:56:20\", \"15:56:50\", \"15:57:00\", \"15:56:20\", \"15:57:40\", …\n$ eclipse_3 &lt;chr&gt; \"16:30:29\", \"16:31:21\", \"16:31:13\", \"16:29:50\", \"16:32:28\", …\n$ eclipse_4 &lt;chr&gt; \"16:33:31\", \"16:34:06\", \"16:34:31\", \"16:34:07\", \"16:34:35\", …\n$ eclipse_5 &lt;chr&gt; \"17:09:40\", \"17:10:30\", \"17:10:40\", \"17:09:40\", \"17:11:30\", …\n$ eclipse_6 &lt;chr&gt; \"18:02:10\", \"18:03:20\", \"18:03:30\", \"18:02:00\", \"18:04:30\", …\n\nglimpse(partial2023)\n\nRows: 31,363\nColumns: 9\n$ state     &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", …\n$ name      &lt;chr&gt; \"Abanda\", \"Abbeville\", \"Adamsville\", \"Addison\", \"Akron\", \"Al…\n$ lat       &lt;dbl&gt; 33.09163, 31.56471, 33.60231, 34.20268, 32.87907, 33.21435, …\n$ lon       &lt;dbl&gt; -85.52703, -85.25912, -86.97153, -87.17800, -87.74090, -86.8…\n$ eclipse_1 &lt;chr&gt; \"15:41:20\", \"15:42:30\", \"15:38:20\", \"15:37:50\", \"15:37:20\", …\n$ eclipse_2 &lt;chr&gt; \"16:23:30\", \"16:25:50\", \"16:20:50\", \"16:19:50\", \"16:20:40\", …\n$ eclipse_3 &lt;chr&gt; \"17:11:10\", \"17:13:50\", \"17:07:50\", \"17:06:50\", \"17:07:30\", …\n$ eclipse_4 &lt;chr&gt; \"18:00:00\", \"18:03:10\", \"17:56:30\", \"17:55:10\", \"17:56:00\", …\n$ eclipse_5 &lt;chr&gt; \"18:45:10\", \"18:49:30\", \"18:42:10\", \"18:40:30\", \"18:42:50\", …\n\nglimpse(partial2024)\n\nRows: 28,844\nColumns: 9\n$ state     &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", …\n$ name      &lt;chr&gt; \"Abanda\", \"Abbeville\", \"Adamsville\", \"Addison\", \"Akron\", \"Al…\n$ lat       &lt;dbl&gt; 33.09163, 31.56471, 33.60231, 34.20268, 32.87907, 33.21435, …\n$ lon       &lt;dbl&gt; -85.52703, -85.25912, -86.97153, -87.17800, -87.74090, -86.8…\n$ eclipse_1 &lt;chr&gt; \"17:43:00\", \"17:41:40\", \"17:41:00\", \"17:41:30\", \"17:38:40\", …\n$ eclipse_2 &lt;chr&gt; \"18:24:10\", \"18:21:40\", \"18:23:10\", \"18:24:10\", \"18:20:40\", …\n$ eclipse_3 &lt;chr&gt; \"19:02:00\", \"19:00:30\", \"19:00:00\", \"19:00:30\", \"18:58:00\", …\n$ eclipse_4 &lt;chr&gt; \"19:39:20\", \"19:38:50\", \"19:36:40\", \"19:36:40\", \"19:35:00\", …\n$ eclipse_5 &lt;chr&gt; \"20:18:50\", \"20:17:20\", \"20:17:30\", \"20:18:00\", \"20:15:50\", …\n\nglimpse(total2024)\n\nRows: 3,330\nColumns: 10\n$ state     &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", …\n$ name      &lt;chr&gt; \"Acorn\", \"Adona\", \"Alexander\", \"Alicia\", \"Alix\", \"Alleene\", …\n$ lat       &lt;dbl&gt; 34.63879, 35.03993, 34.61859, 35.89291, 35.42200, 33.76482, …\n$ lon       &lt;dbl&gt; -94.20011, -92.89913, -92.45122, -91.08345, -93.72878, -94.2…\n$ eclipse_1 &lt;chr&gt; \"17:30:40\", \"17:33:20\", \"17:33:20\", \"17:37:30\", \"17:32:50\", …\n$ eclipse_2 &lt;chr&gt; \"18:15:50\", \"18:18:30\", \"18:18:30\", \"18:22:40\", \"18:17:50\", …\n$ eclipse_3 &lt;chr&gt; \"18:47:35\", \"18:50:08\", \"18:51:09\", \"18:54:29\", \"18:49:54\", …\n$ eclipse_4 &lt;chr&gt; \"18:51:37\", \"18:54:22\", \"18:53:38\", \"18:58:05\", \"18:53:00\", …\n$ eclipse_5 &lt;chr&gt; \"19:23:40\", \"19:26:10\", \"19:26:20\", \"19:29:50\", \"19:25:20\", …\n$ eclipse_6 &lt;chr&gt; \"20:08:30\", \"20:10:50\", \"20:11:10\", \"20:14:10\", \"20:10:00\", …\n\n#Add the year and eclipse type to each dataset\nannular2023$year &lt;- \"2023\"\nannular2023$type &lt;- \"annular\"\n\npartial2023$year &lt;- \"2023\"\npartial2023$type &lt;- \"partial\"\n\npartial2024$year &lt;- \"2024\"\npartial2024$type &lt;- \"partial\"\n\ntotal2024$year &lt;- \"2024\"\ntotal2024$type &lt;- \"total\"\n\n#Combine the four datasets into one dataset\neclipse &lt;- bind_rows(annular2023, partial2023, partial2024, total2024)\n\n#Convert the time variables into hms format\neclipse$eclipse_1 &lt;- as_hms(eclipse$eclipse_1)\neclipse$eclipse_2 &lt;- as_hms(eclipse$eclipse_2)\neclipse$eclipse_3 &lt;- as_hms(eclipse$eclipse_3)\neclipse$eclipse_4 &lt;- as_hms(eclipse$eclipse_4)\neclipse$eclipse_5 &lt;- as_hms(eclipse$eclipse_5)\neclipse$eclipse_6 &lt;- as_hms(eclipse$eclipse_6)\n\n#Create a new variable \"duration\" that represents the total time the moon contacts the sun for each eclipse type\neclipse$duration &lt;- ifelse(eclipse$type %in% c(\"annular\", \"total\"), \n                        as.numeric(eclipse$eclipse_6 - eclipse$eclipse_1, \"hours\"),\n                        as.numeric(eclipse$eclipse_5 - eclipse$eclipse_1, \"hours\"))\n\n#Get a glimpse of the processed data\nglimpse(eclipse) \n\nRows: 64,348\nColumns: 13\n$ state     &lt;chr&gt; \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", …\n$ name      &lt;chr&gt; \"Chilchinbito\", \"Chinle\", \"Del Muerto\", \"Dennehotso\", \"Fort …\n$ lat       &lt;dbl&gt; 36.49200, 36.15115, 36.18739, 36.82900, 35.74750, 36.71717, …\n$ lon       &lt;dbl&gt; -110.0492, -109.5787, -109.4359, -109.8757, -109.0680, -110.…\n$ eclipse_1 &lt;time&gt; 15:10:50, 15:11:10, 15:11:20, 15:10:50, 15:11:40, 15:10:40,…\n$ eclipse_2 &lt;time&gt; 15:56:20, 15:56:50, 15:57:00, 15:56:20, 15:57:40, 15:56:00,…\n$ eclipse_3 &lt;time&gt; 16:30:29, 16:31:21, 16:31:13, 16:29:50, 16:32:28, 16:29:54,…\n$ eclipse_4 &lt;time&gt; 16:33:31, 16:34:06, 16:34:31, 16:34:07, 16:34:35, 16:33:21,…\n$ eclipse_5 &lt;time&gt; 17:09:40, 17:10:30, 17:10:40, 17:09:40, 17:11:30, 17:09:10,…\n$ eclipse_6 &lt;time&gt; 18:02:10, 18:03:20, 18:03:30, 18:02:00, 18:04:30, 18:01:30,…\n$ year      &lt;chr&gt; \"2023\", \"2023\", \"2023\", \"2023\", \"2023\", \"2023\", \"2023\", \"202…\n$ type      &lt;chr&gt; \"annular\", \"annular\", \"annular\", \"annular\", \"annular\", \"annu…\n$ duration  &lt;dbl&gt; 2.855556, 2.869444, 2.869444, 2.852778, 2.880556, 2.847222, …\n\n\nThe processed dataset has 64,348 observations and 13 variables: “state”, “name”, “lat”, “lon”, “eclipse_1”, “eclipse_2”, “eclipse_3”, “eclipse_4”, “eclipse_5”, “eclipse_6”, “year”, “type”, and “duration”.\n\n\nData Exploration\nNow let’s explore the variables in the processed dataset, starting with the variable “duration”, which represents the total time the moon contacts the sun.\n\n#Summary statistics and histogram for the variable \"duration\"\nsummary(eclipse$duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   2.467   2.583   2.582   2.786   3.292 \n\nggplot(eclipse, aes(x = duration)) +\n  geom_histogram() +\n  labs(x = \"Eclipse Duration (hours)\", y = \"Frequency\", title = \"Histogram of Eclipse Duration\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean and median for the variable “duration” are the same (2.58). The histogram of “duration” shows a relatively normal distribution for most of the data (duration &gt; 1.5 hours). There are a few observations for duration less than 1.5 hours.\nNext let’s explore the variable “eclipse_3”, which represents the time at which the eclipse reaches 100% of the location’s maximum.\n\n#Summary statistics and histogram for the variable \"eclipse_3\"\neclipse$eclipse_3 &lt;- as.numeric(eclipse$eclipse_3)\nsummary(eclipse$eclipse_3)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0   61090   63060   64281   68483   70480 \n\nggplot(eclipse, aes(x = eclipse_3)) +\n  geom_histogram() +\n  labs(x = \"Time of Maximum Eclipse\", y = \"Frequency\", title = \"Histogram of Time of Maximum Eclipse\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean time is greater than the median time for the variable “eclipse_3”. The histogram of “eclipse_3” shows a bimodal distribution with two peaks that represent the two most frequent times for maximum eclipse.\nThen let’s explore the variable “state”.\n\n#Summary tables for the variable \"state\"\ntable(eclipse$state, useNA = \"always\")\n\n\n  AK   AL   AR   AZ   CA   CO   CT   DC   DE   FL   GA   HI   IA   ID   IL   IN \n 710 1186 1250  934 3222  962  430    2  158 1910 1350  326 2050  472 2924 1944 \n  KS   KY   LA   MA   MD   ME   MI   MN   MO   MS   MT   NC   ND   NE   NH   NJ \n1480 1108  976  496 1072  310 1490 1828 2164  854  994 1552  812 1180  200 1400 \n  NM   NV   NY   OH   OK   OR   PA   PR   RI   SC   SD   TN   TX   UT   VA   VT \n1054  266 2584 2530 1690  852 3776  584   72  950  970 1008 3718  666 1344  358 \n  WA   WI   WV   WY &lt;NA&gt; \n1278 1614  878  410    0 \n\nprop.table(table(eclipse$state))\n\n\n         AK          AL          AR          AZ          CA          CO \n0.011033754 0.018431031 0.019425623 0.014514826 0.050071486 0.014949960 \n         CT          DC          DE          FL          GA          HI \n0.006682414 0.000031081 0.002455399 0.029682352 0.020979673 0.005066203 \n         IA          ID          IL          IN          KS          KY \n0.031858022 0.007335115 0.045440418 0.030210729 0.022999938 0.017218872 \n         LA          MA          MD          ME          MI          MN \n0.015167527 0.007708087 0.016659414 0.004817555 0.023155343 0.028408031 \n         MO          MS          MT          NC          ND          NE \n0.033629639 0.013271586 0.015447256 0.024118854 0.012618885 0.018337788 \n         NH          NJ          NM          NV          NY          OH \n0.003108100 0.021756698 0.016379685 0.004133773 0.040156648 0.039317461 \n         OK          OR          PA          PR          RI          SC \n0.026263443 0.013240505 0.058680922 0.009075651 0.001118916 0.014763474 \n         SD          TN          TX          UT          VA          VT \n0.015074284 0.015664823 0.057779574 0.010349972 0.020886430 0.005563498 \n         WA          WI          WV          WY \n0.019860757 0.025082365 0.013644558 0.006371604 \n\n\nFor the variable “state”, the most represented state is Pennsylvania, with 3,776 observations (5.9%). The least represented state (not including D.C.) is Rhode Island, with 72 observations (0.1%).There are no missing observations for “state”.\nLet’s also explore the variable “year”.\n\n#Summary tables for the variable \"year\"\ntable(eclipse$year, useNA = \"always\")\n\n\n 2023  2024  &lt;NA&gt; \n32174 32174     0 \n\nprop.table(table(eclipse$year))\n\n\n2023 2024 \n 0.5  0.5 \n\n\nFor the variable “year”, exactly half of the observations are from 2023 (32,174), and half of the observations are from 2024 (32,174).There are no missing observations for “year”.\nFinally, let’s explore the variable “type”.\n\n#Summary tables for the variable \"type\"\ntable(eclipse$type, useNA = \"always\")\n\n\nannular partial   total    &lt;NA&gt; \n    811   60207    3330       0 \n\nprop.table(table(eclipse$type))\n\n\n   annular    partial      total \n0.01260334 0.93564680 0.05174986 \n\n\nFor the variable “type”, 93.6% of the observations are partial eclipses, 5.2% are total eclipses, and 1.3% are annular eclipses. There are no missing observations for “type”.\nHere is a summary table of the variables, stratified by eclipse type.\n\n#Create a summary table of the variables\neclipse %&gt;% tbl_summary(by = type, \n                        include = c(duration, eclipse_3, state, year), \n                        statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;%\n  modify_caption(\"Summary Table of U.S. Solar Eclipse Data, Stratified by Eclipse Type\")\n\n\n\n\n\nSummary Table of U.S. Solar Eclipse Data, Stratified by Eclipse Type\n\n\n\n\n\n\n\n\nCharacteristic\nannular, N = 8111\npartial, N = 60,2071\ntotal, N = 3,3301\n\n\n\n\nduration\n2.91 (0.20)\n2.58 (0.40)\n2.55 (0.08)\n\n\neclipse_3\n59,749 (747)\n64,105 (6,372)\n68,554 (881)\n\n\nstate\n\n\n\n\n\n\n\n\n    AK\n0 (0%)\n710 (1.2%)\n0 (0%)\n\n\n    AL\n0 (0%)\n1,186 (2.0%)\n0 (0%)\n\n\n    AR\n0 (0%)\n885 (1.5%)\n365 (11%)\n\n\n    AZ\n22 (2.7%)\n912 (1.5%)\n0 (0%)\n\n\n    CA\n11 (1.4%)\n3,211 (5.3%)\n0 (0%)\n\n\n    CO\n7 (0.9%)\n955 (1.6%)\n0 (0%)\n\n\n    CT\n0 (0%)\n430 (0.7%)\n0 (0%)\n\n\n    DC\n0 (0%)\n2 (&lt;0.1%)\n0 (0%)\n\n\n    DE\n0 (0%)\n158 (0.3%)\n0 (0%)\n\n\n    FL\n0 (0%)\n1,910 (3.2%)\n0 (0%)\n\n\n    GA\n0 (0%)\n1,350 (2.2%)\n0 (0%)\n\n\n    HI\n0 (0%)\n326 (0.5%)\n0 (0%)\n\n\n    IA\n0 (0%)\n2,050 (3.4%)\n0 (0%)\n\n\n    ID\n0 (0%)\n472 (0.8%)\n0 (0%)\n\n\n    IL\n0 (0%)\n2,672 (4.4%)\n252 (7.6%)\n\n\n    IN\n0 (0%)\n1,358 (2.3%)\n586 (18%)\n\n\n    KS\n0 (0%)\n1,480 (2.5%)\n0 (0%)\n\n\n    KY\n0 (0%)\n1,070 (1.8%)\n38 (1.1%)\n\n\n    LA\n0 (0%)\n976 (1.6%)\n0 (0%)\n\n\n    MA\n0 (0%)\n496 (0.8%)\n0 (0%)\n\n\n    MD\n0 (0%)\n1,072 (1.8%)\n0 (0%)\n\n\n    ME\n0 (0%)\n282 (0.5%)\n28 (0.8%)\n\n\n    MI\n0 (0%)\n1,487 (2.5%)\n3 (&lt;0.1%)\n\n\n    MN\n0 (0%)\n1,828 (3.0%)\n0 (0%)\n\n\n    MO\n0 (0%)\n1,996 (3.3%)\n168 (5.0%)\n\n\n    MS\n0 (0%)\n854 (1.4%)\n0 (0%)\n\n\n    MT\n0 (0%)\n994 (1.7%)\n0 (0%)\n\n\n    NC\n0 (0%)\n1,552 (2.6%)\n0 (0%)\n\n\n    ND\n0 (0%)\n812 (1.3%)\n0 (0%)\n\n\n    NE\n0 (0%)\n1,180 (2.0%)\n0 (0%)\n\n\n    NH\n0 (0%)\n196 (0.3%)\n4 (0.1%)\n\n\n    NJ\n0 (0%)\n1,400 (2.3%)\n0 (0%)\n\n\n    NM\n302 (37%)\n752 (1.2%)\n0 (0%)\n\n\n    NV\n26 (3.2%)\n240 (0.4%)\n0 (0%)\n\n\n    NY\n0 (0%)\n2,181 (3.6%)\n403 (12%)\n\n\n    OH\n0 (0%)\n1,837 (3.1%)\n693 (21%)\n\n\n    OK\n0 (0%)\n1,649 (2.7%)\n41 (1.2%)\n\n\n    OR\n130 (16%)\n722 (1.2%)\n0 (0%)\n\n\n    PA\n0 (0%)\n3,705 (6.2%)\n71 (2.1%)\n\n\n    PR\n0 (0%)\n584 (1.0%)\n0 (0%)\n\n\n    RI\n0 (0%)\n72 (0.1%)\n0 (0%)\n\n\n    SC\n0 (0%)\n950 (1.6%)\n0 (0%)\n\n\n    SD\n0 (0%)\n970 (1.6%)\n0 (0%)\n\n\n    TN\n0 (0%)\n1,008 (1.7%)\n0 (0%)\n\n\n    TX\n241 (30%)\n2,886 (4.8%)\n591 (18%)\n\n\n    UT\n72 (8.9%)\n594 (1.0%)\n0 (0%)\n\n\n    VA\n0 (0%)\n1,344 (2.2%)\n0 (0%)\n\n\n    VT\n0 (0%)\n271 (0.5%)\n87 (2.6%)\n\n\n    WA\n0 (0%)\n1,278 (2.1%)\n0 (0%)\n\n\n    WI\n0 (0%)\n1,614 (2.7%)\n0 (0%)\n\n\n    WV\n0 (0%)\n878 (1.5%)\n0 (0%)\n\n\n    WY\n0 (0%)\n410 (0.7%)\n0 (0%)\n\n\nyear\n\n\n\n\n\n\n\n\n    2023\n811 (100%)\n31,363 (52%)\n0 (0%)\n\n\n    2024\n0 (0%)\n28,844 (48%)\n3,330 (100%)\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n\n\n\nHere is another summary table of the variables, stratified year.\n\n#Create a summary table of the variables\neclipse %&gt;% tbl_summary(by = year, \n                        include = c(duration, eclipse_3, state, type), \n                        statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;%\n  modify_caption(\"Summary Table of U.S. Solar Eclipse Data, Stratified by Year\")\n\n\n\n\n\nSummary Table of U.S. Solar Eclipse Data, Stratified by Year\n\n\n\n\n\n\n\nCharacteristic\n2023, N = 32,1741\n2024, N = 32,1741\n\n\n\n\nduration\n2.76 (0.37)\n2.41 (0.34)\n\n\neclipse_3\n60,950 (3,143)\n67,611 (6,819)\n\n\nstate\n\n\n\n\n\n\n    AK\n355 (1.1%)\n355 (1.1%)\n\n\n    AL\n593 (1.8%)\n593 (1.8%)\n\n\n    AR\n625 (1.9%)\n625 (1.9%)\n\n\n    AZ\n467 (1.5%)\n467 (1.5%)\n\n\n    CA\n1,611 (5.0%)\n1,611 (5.0%)\n\n\n    CO\n481 (1.5%)\n481 (1.5%)\n\n\n    CT\n215 (0.7%)\n215 (0.7%)\n\n\n    DC\n1 (&lt;0.1%)\n1 (&lt;0.1%)\n\n\n    DE\n79 (0.2%)\n79 (0.2%)\n\n\n    FL\n955 (3.0%)\n955 (3.0%)\n\n\n    GA\n675 (2.1%)\n675 (2.1%)\n\n\n    HI\n163 (0.5%)\n163 (0.5%)\n\n\n    IA\n1,025 (3.2%)\n1,025 (3.2%)\n\n\n    ID\n236 (0.7%)\n236 (0.7%)\n\n\n    IL\n1,462 (4.5%)\n1,462 (4.5%)\n\n\n    IN\n972 (3.0%)\n972 (3.0%)\n\n\n    KS\n740 (2.3%)\n740 (2.3%)\n\n\n    KY\n554 (1.7%)\n554 (1.7%)\n\n\n    LA\n488 (1.5%)\n488 (1.5%)\n\n\n    MA\n248 (0.8%)\n248 (0.8%)\n\n\n    MD\n536 (1.7%)\n536 (1.7%)\n\n\n    ME\n155 (0.5%)\n155 (0.5%)\n\n\n    MI\n745 (2.3%)\n745 (2.3%)\n\n\n    MN\n914 (2.8%)\n914 (2.8%)\n\n\n    MO\n1,082 (3.4%)\n1,082 (3.4%)\n\n\n    MS\n427 (1.3%)\n427 (1.3%)\n\n\n    MT\n497 (1.5%)\n497 (1.5%)\n\n\n    NC\n776 (2.4%)\n776 (2.4%)\n\n\n    ND\n406 (1.3%)\n406 (1.3%)\n\n\n    NE\n590 (1.8%)\n590 (1.8%)\n\n\n    NH\n100 (0.3%)\n100 (0.3%)\n\n\n    NJ\n700 (2.2%)\n700 (2.2%)\n\n\n    NM\n527 (1.6%)\n527 (1.6%)\n\n\n    NV\n133 (0.4%)\n133 (0.4%)\n\n\n    NY\n1,292 (4.0%)\n1,292 (4.0%)\n\n\n    OH\n1,265 (3.9%)\n1,265 (3.9%)\n\n\n    OK\n845 (2.6%)\n845 (2.6%)\n\n\n    OR\n426 (1.3%)\n426 (1.3%)\n\n\n    PA\n1,888 (5.9%)\n1,888 (5.9%)\n\n\n    PR\n292 (0.9%)\n292 (0.9%)\n\n\n    RI\n36 (0.1%)\n36 (0.1%)\n\n\n    SC\n475 (1.5%)\n475 (1.5%)\n\n\n    SD\n485 (1.5%)\n485 (1.5%)\n\n\n    TN\n504 (1.6%)\n504 (1.6%)\n\n\n    TX\n1,859 (5.8%)\n1,859 (5.8%)\n\n\n    UT\n333 (1.0%)\n333 (1.0%)\n\n\n    VA\n672 (2.1%)\n672 (2.1%)\n\n\n    VT\n179 (0.6%)\n179 (0.6%)\n\n\n    WA\n639 (2.0%)\n639 (2.0%)\n\n\n    WI\n807 (2.5%)\n807 (2.5%)\n\n\n    WV\n439 (1.4%)\n439 (1.4%)\n\n\n    WY\n205 (0.6%)\n205 (0.6%)\n\n\ntype\n\n\n\n\n\n\n    annular\n811 (2.5%)\n0 (0%)\n\n\n    partial\n31,363 (97%)\n28,844 (90%)\n\n\n    total\n0 (0%)\n3,330 (10%)\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n\n\n\nNow let’s create a scatterplot between the variables “duration” and “eclipse_3”, as well as a correlation table.\n\n#Create scatterplot between \"duration\" and \"ecplise_3\"\nggplot(eclipse, aes(x = duration, y = eclipse_3)) +\n  geom_point() +\n  labs(x = \"Duration\", y = \"Time of Maximum Eclipse\") +\n  ggtitle(\"Scatterplot of Duration vs. Time of Maximum Eclipse\")\n\n\n\n\n\n\n\n#Create a correlation matrix between \"duration\" and \"eclipse_3\"\ncor(eclipse$duration, eclipse$eclipse_3)\n\n[1] 0.1960338\n\n\nBased on the scatterplot and correlation matrix, there doesn’t seem to be a strong correlation between “duration” and “eclipse_3”.\nNext let’s create some boxplots between the variable “duration” and the variables “year” and “type”.\n\n#Create a boxplot of \"duration\" and \"year\"\nggplot(eclipse, aes(x = factor(year), y = duration)) +\n  geom_boxplot() +\n  labs(x = \"Year\", y = \"Eclipse Duration (hours)\", title = \"Boxplot of Year and Eclipse Duration\")\n\n\n\n\n\n\n\n#Create a boxplot of \"duration\" and \"type\"\nggplot(eclipse, aes(x = factor(type), y = duration)) +\n  geom_boxplot() +\n  labs(x = \"Eclipse Type\", y = \"Eclipse Duration (hours)\", title = \"Boxplot of Eclipse Type and Eclipse Duration\")\n\n\n\n\n\n\n\n\nThe boxplots between “duration” and “year” and “type” show relatively normal distributions. The mean eclipse duration is greatest for 2023 and annular eclipses.\nFinally, let’s look at the stratified boxplots.\n\n#Create a boxplot of \"duration\" and \"year\", stratified by \"type\"\nggplot(eclipse, aes(x = factor(year), y = duration, fill = factor(type))) +\n  geom_boxplot() +\n  labs(x = \"Year\", y = \"Eclipse Duration (hours)\", title = \"Boxplot of Year and Eclipse Duration, Stratified by Eclipse Type\") +\n  scale_fill_discrete(name = \"Eclipse Type\")\n\n\n\n\n\n\n\n#Create a boxplot of \"duration\" and \"type\", stratified by \"year\"\nggplot(eclipse, aes(x = factor(type), y = duration, fill = factor(year))) +\n  geom_boxplot() +\n  labs(x = \"Eclipse Type\", y = \"Eclipse Duration (hours)\", title = \"Boxplot of Eclipse Type and Eclipse Duration, Stratified, by Year\") +\n  scale_fill_discrete(name = \"Year\")\n\n\n\n\n\n\n\n\nThe stratified boxplots show relatively normal distributions across all strata. For each year, partial eclipses had shorter eclipse duration than annular/total eclipses. The comparisons of eclipse by year shows that 2024 partial eclipses had shorter eclipse duration than 2023 partial eclipses.\n\n\nQuestion/Hypothesis\nIs eclipse type associated with eclipse duration among solar eclipses?\n\n\nData Processing (Part 2)\nLet’s further process the data and split it into train/test data.\n\n#Create a subset dataset with only \"eclipse_3\", \"year\", \"type\" and \"duration\"\neclipse2 &lt;- eclipse[, c(\"eclipse_3\", \"year\", \"type\", \"duration\")]\n\n#Rename the \"eclipse_3\" variable to \"maxtime\"\neclipse2 &lt;- eclipse2 %&gt;% rename(maxtime = eclipse_3)\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Split the data randomly into 75% train and 25% test set\ndata_split &lt;- initial_split(eclipse2, prop = 3/4)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n\n\nModel Fitting\nNow let’s fit some linear models using “duration” as the outcome and 10-fold cross-validation for model training. First, let’s start with the predictor of interest, “type”.\n\n#Fit a linear model to the outcome \"duration\" using \"type\" as the predictor\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinear_fit1 &lt;- linear_model %&gt;% fit(duration ~ type, data = train_data)\ntidy(linear_fit1)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    2.91     0.0158     185.  0       \n2 typepartial   -0.329    0.0159     -20.7 3.50e-95\n3 typetotal     -0.359    0.0176     -20.4 2.42e-92\n\n#Define the training control as cross-validation and K = 10 folds\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n#Fit a linear model to the outcome \"duration\" using \"type\" as the predictor\nmodel1 &lt;- train(duration ~ type, data = train_data, method = \"lm\", trCrontol = train_control)\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\n#Print RMSE and R-squared for Model 1\nprint(model1)\n\nLinear Regression \n\n48261 samples\n    1 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 48261, 48261, 48261, 48261, 48261, 48261, ... \nResampling results:\n\n  RMSE       Rsquared     MAE      \n  0.3906976  0.009169278  0.2340768\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nIn Model 1, the predictor “type” is significantly associated with the outcome “duration” (p-values &lt; 0.001). Compared to annular eclipses, the predicted eclipse duration decreases by 0.33 hours for partial eclipses and decreases by 0.36 hours for total eclipses. R-squared = 0.009 indicates that 0.9% of the variability in eclipse duration is explained by the model. RMSE = 0.39 represents the average distance between the predicted values from the model and the actual values in the dataset.\nNext let’s fit a linear model with the outcome, “duration”, and the predictor variables “type” and “year”.\n\n#Fit a linear model to the outcome \"duration\" using \"type\" and \"year\" as the predictors\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinear_fit2 &lt;- linear_model %&gt;% fit(duration ~ type + year, data = train_data)\ntidy(linear_fit2)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  2.91       0.0141   206.     0       \n2 typepartial -0.156      0.0143   -10.9    8.18e-28\n3 typetotal    0.000951   0.0161     0.0592 9.53e- 1\n4 year2024    -0.360      0.00329 -110.     0       \n\n#Define the training control as cross-validation and K = 10 folds\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n#Fit a linear model to the outcome \"duration\" using \"type\" and \"year\" as the predictors\nmodel2 &lt;- train(duration ~ type + year, data = train_data, method = \"lm\", trCrontol = train_control)\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\n#Print RMSE and R-squared for Model 2\nprint(model2)\n\nLinear Regression \n\n48261 samples\n    2 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 48261, 48261, 48261, 48261, 48261, 48261, ... \nResampling results:\n\n  RMSE      Rsquared   MAE      \n  0.348128  0.2073426  0.1985806\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nIn Model 2, the predictors “type” and “year” are significantly associated with the outcome “duration” (p-values &lt; 0.001). Compared to annular eclipses and controlling for year, the predicted eclipse duration decreases by 0.15 hours for partial eclipses and increase by 0.0001 hours for total eclipses. R-squared = 0.206 indicates that 20.6% of the variability in eclipse duration is explained by the model. RMSE = 0.35 represents the average distance between the predicted values from the model and the actual values in the dataset.\nThen let’s fit a linear model with the outcome, “duration”, and the predictor variables “type”, “year”, and “maxtime”.\n\n#Fit a linear model to the outcome \"duration\" using \"type\", \"year\", and \"maxtime\" as the predictors\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinear_fit3 &lt;- linear_model %&gt;% fit(duration ~ type + year + maxtime, data = train_data)\ntidy(linear_fit3)\n\n# A tibble: 5 × 5\n  term          estimate   std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.664     0.0187          35.4  1.50e-271\n2 typepartial -0.203     0.0117         -17.3  1.52e- 66\n3 typetotal   -0.0847    0.0132          -6.40 1.52e- 10\n4 year2024    -0.605     0.00314       -193.   0        \n5 maxtime      0.0000376 0.000000247    152.   0        \n\n#Define the training control as cross-validation and K = 10 folds\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n#Fit a linear model to the outcome \"duration\" using \"type\", \"year\", and \"maxtime\" as the predictors\nmodel3 &lt;- train(duration ~ type + year + maxtime, data = train_data, method = \"lm\", trCrontol = train_control)\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\n#Print RMSE and R-squared for Model 3\nprint(model3)\n\nLinear Regression \n\n48261 samples\n    3 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 48261, 48261, 48261, 48261, 48261, 48261, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.2871096  0.4657204  0.1751599\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nIn Model 3, the predictors “type”, “year”, and “maxtime” are significantly associated with the outcome “duration” (p-values &lt; 0.001). Compared to annular eclipses and controlling for year and time of maximum eclipse, the predicted eclipse duration decreases by 0.20 hours for partial eclipses and decreases by 0.08 hours for total eclipses. R-squared = 0.467 indicates that 46.7% of the variability in eclipse duration is explained by the model. RMSE = 0.29 represents the average distance between the predicted values from the model and the actual values in the dataset.\nIt’s beneficial to consider both RMSE (prioritizes prediction accuracy) and R-squared (priortizes explaining variance) when evaluating model performance. Based on the lowest RMSE value and the highest R-squared value, the model with the best performance is Model 3 with all predictors.\n\n\nPlotting Residuals and Observed and Predicted Values\nFirst let’s create a figure that plots observed values versus predicted values for the three original model fits to all of the training data.\n\n#Compute the predicted values for the three models\npredicted1 &lt;- predict(linear_fit1, new_data = train_data)\npredicted2 &lt;- predict(linear_fit2, new_data = train_data)\npredicted3 &lt;- predict(linear_fit3, new_data = train_data)\n\n#Create a data frame with the observed values and predicted values from the three models\npredictions &lt;- data.frame(\n  observed = train_data$duration,\n  model1 = predicted1,\n  model2 = predicted2,\n  model3 = predicted3)\n\n#Create a figure that plots observed values on the x-axis and predicted values on the y-axis\nggplot(predictions, aes(x = observed)) +\n  geom_point(aes(y = .pred, color = \"Model 1\")) +\n  geom_point(aes(y = .pred.1, color = \"Model 2\")) +\n  geom_point(aes(y = .pred.2, color = \"Model 3\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for Three Models\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNone of the three models shows data points that fall along the dashed diagonal line that represents agreement between observed and predicted values. Model 3 with all predictors looks relatively the best as some points fall relatively along the dashed diagonal line. Perhaps there are aspects of the outcome pattern that the model cannot explain.\nNow let’s create a figure that plots predicted values versus residuals for each model.\n\n#Fit the linear model and create a data frame\nlinmodel1 &lt;- lm(duration ~ type, data = train_data)\nlinmodel1f &lt;- fortify(linmodel1)\n\n#Create a figure that plots predicted values on the x-axis and residuals on the y-axis\nggplot(linmodel1f, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-3, 3) +\n  labs(x = \"Predicted Values\", y = \"Residuals\", title = \"Predicted Values vs. Residuals for Model 1\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere is no discernible pattern in the plot of predicted values versus residuals for Model 1.\n\n#Fit the linear model and create a data frame\nlinmodel2 &lt;- lm(duration ~ type + year, data = train_data)\nlinmodel2f &lt;- fortify(linmodel2)\n\n#Create a figure that plots predicted values on the x-axis and residuals on the y-axis\nggplot(linmodel2f, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-3, 3) +\n  labs(x = \"Predicted Values\", y = \"Residuals\", title = \"Predicted Values vs. Residuals for Model 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere is no discernible pattern in the plot of predicted values versus residuals for Model 2.\n\n#Fit the linear model and create a data frame\nlinmodel3 &lt;- lm(duration ~ type + year + maxtime, data = train_data)\nlinmodel3f &lt;- fortify(linmodel3)\n\n#Create a figure that plots predicted values on the x-axis and residuals on the y-axis\nggplot(linmodel3f, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-3, 3) +\n  labs(x = \"Predicted Values\", y = \"Residuals\", title = \"Predicted Values vs. Residuals for Model 3\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere is no discernible pattern in the plot of predicted values versus residuals for Model 3.\nFinally, let’s use the bootstrap method to sample the data, fit each model to the data, and measure uncertainty in our predictions. Let’s create a figure that plots observed versus predicted values for each model.\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Create 100 bootstraps using the training data\ndat_bs &lt;- bootstraps(train_data, times = 100)\n\n#Create an empty matrix to store predictions\npred_bs &lt;- matrix(nrow = nrow(train_data), ncol = 100)\n\n#Use a loop to fit Model 1 to each bootstrap sample and make predictions for the training data\nfor(i in 1:100) {\n  dat_sample &lt;- analysis(dat_bs$splits[[i]])\n  model &lt;- lm(duration ~ type, data = dat_sample)\n  predictions &lt;- predict(model, newdata = train_data)\n  pred_bs[, i] &lt;- predictions\n}\n\n#Compute the median and 95% confidence intervals\npreds &lt;- apply(pred_bs, 1, function(x) {quantile(x, c(0.025, 0.5, 0.975))}) %&gt;% t()\npreds &lt;- data.frame(preds)\n\n#Add the median and confidence intervals to the same data frame as the observed and predicted values for Model 1\nlinmodel1f$lower &lt;- preds$X2.5.\nlinmodel1f$median &lt;- preds$X50.\nlinmodel1f$upper &lt;- preds$X97.5.\n\n#Create a figure that plots observed values on the x-axis and predicted values on the y-axis\nggplot(linmodel1f, aes(x = duration)) +\n  geom_point(aes(y = .fitted, color = \"Point Estimates\")) +\n  geom_point(aes(y = lower, color = \"Lower Bounds\")) +\n  geom_point(aes(y = median, color = \"Medians\")) +\n  geom_point(aes(y = upper, color = \"Upper Bounds\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for Model 1\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn general, the medians and confidence intervals in the plot follow the same pattern as the point estimates for Model 1, although the pattern is not diagonal. There may be aspects of the outcome that the model cannot explain.\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Create 100 bootstraps using the training data\ndat_bs &lt;- bootstraps(train_data, times = 100)\n\n#Create an empty matrix to store predictions\npred_bs &lt;- matrix(nrow = nrow(train_data), ncol = 100)\n\n#Use a loop to fit Model 1 to each bootstrap sample and make predictions for the training data\nfor(i in 1:100) {\n  dat_sample &lt;- analysis(dat_bs$splits[[i]])\n  model &lt;- lm(duration ~ type + year, data = dat_sample)\n  predictions &lt;- predict(model, newdata = train_data)\n  pred_bs[, i] &lt;- predictions\n}\n\n#Compute the median and 95% confidence intervals\npreds &lt;- apply(pred_bs, 1, function(x) {quantile(x, c(0.025, 0.5, 0.975))}) %&gt;% t()\npreds &lt;- data.frame(preds)\n\n#Add the median and confidence intervals to the same data frame as the observed and predicted values for Model 1\nlinmodel2f$lower &lt;- preds$X2.5.\nlinmodel2f$median &lt;- preds$X50.\nlinmodel2f$upper &lt;- preds$X97.5.\n\n#Create a figure that plots observed values on the x-axis and predicted values on the y-axis\nggplot(linmodel2f, aes(x = duration)) +\n  geom_point(aes(y = .fitted, color = \"Point Estimates\")) +\n  geom_point(aes(y = lower, color = \"Lower Bounds\")) +\n  geom_point(aes(y = median, color = \"Medians\")) +\n  geom_point(aes(y = upper, color = \"Upper Bounds\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for Model 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn general, the medians and confidence intervals in the plot follow the same pattern as the point estimates for Model 2, although the pattern is not diagonal. There may be aspects of the outcome that the model cannot explain.\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Create 100 bootstraps using the training data\ndat_bs &lt;- bootstraps(train_data, times = 100)\n\n#Create an empty matrix to store predictions\npred_bs &lt;- matrix(nrow = nrow(train_data), ncol = 100)\n\n#Use a loop to fit Model 1 to each bootstrap sample and make predictions for the training data\nfor(i in 1:100) {\n  dat_sample &lt;- analysis(dat_bs$splits[[i]])\n  model &lt;- lm(duration ~ type + year + maxtime, data = dat_sample)\n  predictions &lt;- predict(model, newdata = train_data)\n  pred_bs[, i] &lt;- predictions\n}\n\n#Compute the median and 95% confidence intervals\npreds &lt;- apply(pred_bs, 1, function(x) {quantile(x, c(0.025, 0.5, 0.975))}) %&gt;% t()\npreds &lt;- data.frame(preds)\n\n#Add the median and confidence intervals to the same data frame as the observed and predicted values for Model 1\nlinmodel3f$lower &lt;- preds$X2.5.\nlinmodel3f$median &lt;- preds$X50.\nlinmodel3f$upper &lt;- preds$X97.5.\n\n#Create a figure that plots observed values on the x-axis and predicted values on the y-axis\nggplot(linmodel3f, aes(x = duration)) +\n  geom_point(aes(y = .fitted, color = \"Point Estimates\")) +\n  geom_point(aes(y = lower, color = \"Lower Bounds\")) +\n  geom_point(aes(y = median, color = \"Medians\")) +\n  geom_point(aes(y = upper, color = \"Upper Bounds\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for Model 3\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn general, the medians and confidence intervals in the plot follow the same pattern as the point estimates for Model 3, although the pattern is not diagonal. There may be aspects of the outcome that the model cannot explain.\n\n\nFitting Models (Part 2)\nLet’s try fitting a random forest model.\n\n#Fit the random forest model\nrf_recipe &lt;- recipe(duration ~ type + year + maxtime, data = train_data)\n\nrf_spec &lt;- rand_forest(mode = \"regression\") %&gt;% set_engine(\"ranger\", seed = rngseed)\n\nwflow4 &lt;- workflow() %&gt;%\n  add_recipe(rf_recipe) %&gt;%\n  add_model(rf_spec)\n\nfit4 &lt;- wflow4 %&gt;% fit(data = train_data)\n\n#Make predictions\npredictions_rf &lt;- augment(fit4, new_data = train_data) %&gt;%\n  mutate(observed = train_data$duration) %&gt;%\n  rename(predicted = .pred)\n\n#Compute RMSE and R-squared\nmetrics_rf &lt;- fit4 %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;% \n  metrics(truth = duration, estimate = .pred)\n\nprint(metrics_rf)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.279\n2 rsq     standard       0.517\n3 mae     standard       0.167\n\n#Plot observed vs. predicted values\nggplot(predictions_rf, aes(x = observed, y = predicted)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for the Random Forest Model\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn Model 4, R-squared = 0.517 indicates that 51.7% of the variability in eclipse duration is explained by the model. RMSE = 0.28 represents the average distance between the predicted values from the model and the actual values in the dataset.\nComparing all four models, the RMSE values are lowest for the linear regression model with all predictors (Model 3) and the random forest model (Model 4). We conclude that these models perform the best. Because the RMSE values for Model 3 and Model 4 are nearly identical, let’s pick Model 3 as the overall best since it’s simpler and easier to interpret.\n\n\nEvaluating with the Test Data\nLet’s do a final model evaluation, this time using the test data. First, let’s use the fit of Model 3 on the training data to make predictions for the test data. Then let’s make a plot that shows predicted versus observed values for both the training data and the test data.\n\n#Create a data frame with the observed and predicted values for the training data\nmod3_df &lt;- lm(duration ~ type + year + maxtime, data = train_data)\nmod3_df3 &lt;- fortify(mod3_df)\n\n#Make predictions for the test data\npred_test &lt;- as.data.frame(predict(mod3_df, newdata = test_data))\n\n#Add the Y values to the same data frame as the test data predictions\nmod3_df3_test &lt;- fortify(lm(duration ~ type + year + maxtime, data = test_data))\n\n#Combine the data frames with the observed and predicted values for the test data\ncombined_test &lt;- cbind(mod3_df3_test, pred_test)\n\n#Merge the data frames with the training data and test data predictions\nmerged &lt;- merge(mod3_df3, combined_test, by = \"duration\", all = TRUE)\n\n#Rename the test data predictions variable\nmerged &lt;- merged %&gt;% rename(pred_test = `predict(mod3_df, newdata = test_data)`)\n\n#Create a figure that plots observed values on the x-axis and predicted values on the y-axis\nggplot(merged, aes(x = duration)) +\n  geom_point(aes(y = .fitted.x, color = \"Training Data\"), alpha = 0.7) +\n  geom_point(aes(y = pred_test, color = \"Test Data\"), alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for the Training and Test Data\") +\n  theme_minimal()\n\nWarning: Removed 51 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 375 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe plot shows that the observed and predicted values for the test data are mixed in with the training data, which is what we want to see. If the test data points were systematically “off”, then it would indicate a problem, such as overfitting to the training data.\nLet’s also create a figure that plots predicted values versus residuals for Model 3.\n\n#Fit the linear model and create a data frame\nlinmodel3 &lt;- lm(duration ~ type + year + maxtime, data = test_data)\nlinmodel3f &lt;- fortify(linmodel3)\n\n#Create a figure that plots predicted values on the x-axis and residuals on the y-axis\nggplot(linmodel3f, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-3, 3) +\n  labs(x = \"Predicted Values\", y = \"Residuals\", title = \"Predicted Values vs. Residuals for Model 3\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere is no discernible pattern in the plot of predicted values versus residuals for Model 3.\nNext let’s look at model performance metrics and uncertainty for Model 3.\n\n#Fit a linear model to the outcome \"duration\" using \"type\", \"year\", and \"maxtime\" as the predictors\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinear_fit3 &lt;- linear_model %&gt;% fit(duration ~ type + year + maxtime, data = test_data)\ntidy(linear_fit3)\n\n# A tibble: 5 × 5\n  term          estimate   std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  0.661     0.0331          20.0  6.63e-88\n2 typepartial -0.207     0.0210          -9.86 7.34e-23\n3 typetotal   -0.0904    0.0235          -3.84 1.23e- 4\n4 year2024    -0.606     0.00554       -109.   0       \n5 maxtime      0.0000377 0.000000432     87.3  0       \n\n#Uncertainty\nmodel &lt;- lm(duration ~ type + year + maxtime, data = test_data)\nConfint(model)\n\n                 Estimate         2.5 %        97.5 %\n(Intercept)  6.614347e-01  0.5966054880  7.262638e-01\ntypepartial -2.068041e-01 -0.2479235181 -1.656847e-01\ntypetotal   -9.041632e-02 -0.1365602068 -4.427243e-02\nyear2024    -6.058723e-01 -0.6167273030 -5.950172e-01\nmaxtime      3.765857e-05  0.0000368127  3.850444e-05\n\n#Compute RMSE and R-squared\nmetrics &lt;- linear_fit3 %&gt;% \n  predict(test_data) %&gt;% \n  bind_cols(test_data) %&gt;% \n  metrics(truth = duration, estimate = .pred)\n\n#Print RMSE and R-squared\nprint(metrics)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.292\n2 rsq     standard       0.458\n3 mae     standard       0.177\n\n\nIn Model 3, the predictors “type”, “year”, and “maxtime” are significantly associated with the outcome “duration” (p-values &lt; 0.001). Compared to annular eclipses and controlling for year and time of maximum eclipse, the predicted eclipse duration decreases by 0.21 hours for partial eclipses (95% CI: -0.25, -0.17) and decreases by 0.09 hours for total eclipses (95% CI: -0.14, -0.04). R-squared = 0.458 indicates that 45.8% of the variability in eclipse duration is explained by the model. RMSE = 0.29 represents the average distance between the predicted values from the model and the actual values in the dataset.\n\n\nDiscussion\nWe used data on 2023-2024 U.S. solar eclipses to investigate whether eclipse type is associated with eclipse duration among solar eclipses. The other predictors were year and time of maximum eclipse. We looked at three linear regression models and a random forest model. The linear regression model with all three predictors and the random forest model with all three predictors performed the best based on RMSE values. We chose the linear regression model for simplicity of interpretation. All three predictors were significantly associated with eclipse duration. Compared to annular eclipses and controlling for year and time of maximum eclipse, the predicted eclipse duration decreases by 0.20 hours for partial eclipses (95% CI: -0.22, -0.18) and decreases by 0.09 hours for total eclipses (95% CI: -0.11, -0.06) (see table below).\n\n#Fit the linear regression model\nmodel &lt;- lm(duration ~ type + year + maxtime, data = eclipse2)\n\n#Get the coefficients\ncoefficients &lt;- tidy(model)\n\n#Obtain 95% confidence intervals for coefficients\nconf_intervals &lt;- confint(model)\n\n#Combine coefficients and confidence intervals by matching row names\ncombined_data &lt;- cbind(coefficients, conf_intervals)\n\n#Remove the intercept row\ncombined_data &lt;- combined_data %&gt;%\n  filter(term != \"(Intercept)\")\n\n#Rename and relabel columns\ncombined_data &lt;- combined_data %&gt;%\n  mutate(term = ifelse(term == \"typepartial\", \"Partial Eclipse\", \n                       ifelse(term == \"typetotal\", \"Total Eclipse\", \n                              ifelse(term == \"year2024\", \"Year 2024\", \n                                     ifelse(term == \"maxtime\", \"Time of Maximum Eclipse\", term))))) %&gt;%\n  rename(\"Beta Estimate\" = estimate,\n         \"Lower 95% CI\" = `2.5 %`,\n         \"Upper 95% CI\" = `97.5 %`)\n\n#Keep only the desired columns\ncombined_data &lt;- subset(combined_data, select = -c(std.error, statistic, p.value))\n\n#Display the summary table\nprint(combined_data)\n\n                               term Beta Estimate  Lower 95% CI  Upper 95% CI\ntypepartial         Partial Eclipse -2.036828e-01 -2.237689e-01 -0.1835967796\ntypetotal             Total Eclipse -8.613460e-02 -1.087370e-01 -0.0635321872\nyear2024                  Year 2024 -6.053392e-01 -6.106968e-01 -0.5999815305\nmaxtime     Time of Maximum Eclipse  3.759016e-05  3.717041e-05  0.0000380099"
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Ranni Tewfik Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Starter Analysis Exercise Report",
    "section": "",
    "text": "Chaohua Li contributed to this exercise.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\nWarning: package 'here' was built under R version 4.3.2"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Starter Analysis Exercise Report",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Starter Analysis Exercise Report",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Starter Analysis Exercise Report",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Starter Analysis Exercise Report",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Starter Analysis Exercise Report",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Starter Analysis Exercise Report",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Starter Analysis Exercise Report",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nBlood\n0\n1\nFALSE\n4\nA: 4, B: 2, O: 2, AB: 1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nIQ\n0\n1\nNA\nNA\nNA\n108.33333\n10.39230\n90\n103\n105\n116\n125\n▂▅▇▅▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Starter Analysis Exercise Report",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Starter Analysis Exercise Report",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nTable 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Starter Analysis Exercise Report",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Starter Analysis Exercise Report",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Starter Analysis Exercise Report",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Ranni Tewfik Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Ranni Tewfik Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at C:/Users/ranni/Desktop/MADA/rannitewfik-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n4 IQ              intelligence quotient (IQ) score      numeric value &gt;0 or NA\n5 Blood           blood type                            A/B/AB/O/NA           \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n$ IQ     &lt;dbl&gt; 111, 105, 101, 118, 93, 105, 90, 103, 97, 86, 75, 102, 125, 116\n$ Blood  &lt;chr&gt; \"AB\", \"B\", \"A\", \"O\", \"A\", \"B\", \"O\", \"A\", \"O\", \"O\", \"B\", \"A\", \"A…\n\nsummary(rawdata)\n\n    Height              Weight          Gender                IQ       \n Length:14          Min.   :  45.0   Length:14          Min.   : 75.0  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.: 94.0  \n Mode  :character   Median :  70.0   Mode  :character   Median :102.5  \n                    Mean   : 602.7                      Mean   :101.9  \n                    3rd Qu.:  90.0                      3rd Qu.:109.5  \n                    Max.   :7000.0                      Max.   :125.0  \n                    NA's   :1                                          \n    Blood          \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender    IQ Blood\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 180        80 M        111 AB   \n2 175        70 O        105 B    \n3 sixty      60 F        101 A    \n4 178        76 F        118 O    \n5 192        90 NA        93 A    \n6 6          55 F        105 B    \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nBlood\n0\n1\n1\n2\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70.0\n90.0\n7000\n▇▁▁▁▁\n\n\nIQ\n0\n1.00\n101.93\n13.35\n75\n94\n102.5\n109.5\n125\n▁▃▇▁▃\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nBlood\n0\n1\n1\n2\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nIQ\n0\n1.00\n102.00\n13.89\n75\n93.00\n103\n111\n125\n▂▅▇▂▅\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nBlood\n0\n1\n1\n2\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nIQ\n0\n1.00\n102.00\n13.89\n75\n93.00\n103\n111\n125\n▂▅▇▂▅\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nBlood\n0\n1\n1\n2\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nIQ\n0\n1\n105.91\n10.78\n90\n99.5\n105\n113.5\n125\n▇▅▇▅▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nBlood\n0\n1\n1\n2\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nIQ\n0\n1\n105.91\n10.78\n90\n99.5\n105\n113.5\n125\n▇▅▇▅▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nBlood\n0\n1\n1\n2\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nIQ\n0\n1\n108.33\n10.39\n90\n103\n105\n116\n125\n▂▅▇▅▂\n\n\n\n\n\nConvert ‘Blood’ into a factor variable\n\nd4$Blood &lt;- as.factor(d4$Blood)  \nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\nBlood\n0\n1\nFALSE\n4\nA: 4, B: 2, O: 2, AB: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nIQ\n0\n1\n108.33\n10.39\n90\n103\n105\n116\n125\n▂▅▇▅▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata2 &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata2, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Ranni Tewfik Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at C:/Users/ranni/Desktop/MADA/rannitewfik-MADA-portfolio\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "The article “The Most Common Unisex Names In America: Is Yours One Of Them?” on the FiveThirtyEight website provides information on the most common unisex names in America in which at least one-third of recipients were male and at least one-third were female, through the year 2013. To read the original article, please visit the FiveThirtyEight website.\nThe “unisex_names_table.csv” file contains the data used for this exercise. The dataset has 919 observations and five variables: “name” (first names from the Social Security Administration), “total” (total number of living Americans with the name), “male_share” (percentage of people with the name who are male), “female_share” (percentage of people with the name who are female), and “gap” (gap between male_share and female_share). To learn more about the data, please visit the GitHub website."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#part-1---about-the-data",
    "href": "presentation-exercise/presentation-exercise.html#part-1---about-the-data",
    "title": "Presentation Exercise",
    "section": "",
    "text": "The article “The Most Common Unisex Names In America: Is Yours One Of Them?” on the FiveThirtyEight website provides information on the most common unisex names in America in which at least one-third of recipients were male and at least one-third were female, through the year 2013. To read the original article, please visit the FiveThirtyEight website.\nThe “unisex_names_table.csv” file contains the data used for this exercise. The dataset has 919 observations and five variables: “name” (first names from the Social Security Administration), “total” (total number of living Americans with the name), “male_share” (percentage of people with the name who are male), “female_share” (percentage of people with the name who are female), and “gap” (gap between male_share and female_share). To learn more about the data, please visit the GitHub website."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#part-2---re-create-the-original-graphs",
    "href": "presentation-exercise/presentation-exercise.html#part-2---re-create-the-original-graphs",
    "title": "Presentation Exercise",
    "section": "Part 2 - Re-create the Original Graphs",
    "text": "Part 2 - Re-create the Original Graphs\nFirst, I used ChatGPT to help me try to re-create the original graph in the article “The Most Common Unisex Names In America: Is Yours One Of Them?” on the FiveThirtyEight website.\nChatGPT prompt:\n“Write R code that re-creates the same graph titled”The Most Common Unisex Names In America” on the webpage https://fivethirtyeight.com/features/there-are-922-unisex-names-in-america-is-yours-one-of-them/ using the data of the first 20 observations from the unisex_names_table.csv file on the webpage https://github.com/fivethirtyeight/data/tree/master/unisex-names. Add thorough documentation to the code.”\n\n# Load necessary libraries\nlibrary(readr)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n# Load the data from the CSV file\ndata_url &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/unisex-names/unisex_names_table.csv\"\nunisex_data &lt;- read_csv(data_url)\n\nNew names:\nRows: 919 Columns: 6\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): name dbl (5): ...1, total, male_share, female_share, gap\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n# Take only the first 20 observations\ntop_20_names &lt;- head(unisex_data, 20)\n\n# Convert the 'name' column to a factor to ensure correct ordering\ntop_20_names$name &lt;- factor(top_20_names$name, levels = unique(top_20_names$name))\n\n# Create the bar plot\nplot &lt;- ggplot(top_20_names, aes(y = name)) +\n  geom_bar(aes(x = male_share, fill = \"Male\"), stat = \"identity\", orientation = \"y\") +\n  geom_bar(aes(x = -female_share, fill = \"Female\"), stat = \"identity\", orientation = \"y\") +\n  scale_fill_manual(values = c(\"Male\" = \"#008FD5\", \"Female\" = \"#FFA500\")) +\n  labs(title = \"Top 20 Unisex Names: Male and Female Share\",\n       x = \"Percentage\",\n       y = \"Name\",\n       fill = \"Gender\") +\n  theme_minimal() +\n  theme(legend.position = \"top\", \n        axis.text.y = element_text(size = 10),\n        axis.title.y = element_blank(),\n        axis.title.x = element_text(size = 12))\n\n# Display the plot\nplot\n\n\n\n\n\n\n\n\nThen I manually edited the code produced by ChatGPT to re-create the original graph as close as possible.\n\n# Load necessary libraries\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Load the data from the CSV file\nunisex_data &lt;- read_csv(\"unisex_names_table.csv\")\n\nNew names:\n• `` -&gt; `...1`\n\n\nRows: 919 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): name\ndbl (5): ...1, total, male_share, female_share, gap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Create a new longitudinal dataset from \"unisex_data\" \nunisex_long &lt;- unisex_data %&gt;% pivot_longer(c(male_share, female_share), names_to = \"gender\", values_to = \"shares\")\n\n# Take only the first 40 observations (2 observations [male share and female share] for each of the 20 names)\nunisex_long_40 &lt;- head(unisex_long, 40)\n\n# Relabel the categories for the \"gender\" column\nunisex_long_40 &lt;- unisex_long_40 %&gt;% \n  mutate(gender = ifelse(gender == \"male_share\", \"MALE SHARE\", \"FEMALE SHARE\"))\n\n# Create the stacked bar chart\nplot &lt;- ggplot(unisex_long_40, aes(y = reorder(name, total))) +\n  geom_bar(aes(x = shares, fill = factor(gender), group = factor(gender)), width = 0.75, position = \"stack\", stat = \"identity\", orientation = \"y\") +\n  geom_text(aes(x = shares, y = reorder(name, total), group = factor(gender), label = round(100*shares)), stat = \"identity\", orientation = \"y\", size = 2, hjust = 2, vjust = 0, position = \"stack\") +\n  scale_fill_manual(values = c(\"MALE SHARE\" = \"#008FD5\", \"FEMALE SHARE\" = \"#FFA500\"), name = NULL) +\n  labs(title = \"The Most Common Unisex Names in America\", subtitle = \"Names for which at least one-third of recipients were male and at \\n least one-third were female, through 2013\",\n       x = NULL,\n       y = NULL,\n       fill = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"top\", \n        axis.text.y = element_text(size = 10, hjust = 0),\n        axis.title.y = element_text(angle = 0, vjust = 1),\n        axis.title.x = element_text(),\n        axis.ticks = element_blank(),\n        plot.title = element_text(hjust = 0, face = \"bold\")) +\n  guides(fill = guide_legend(reverse=TRUE))\n\nWarning in geom_text(aes(x = shares, y = reorder(name, total), group =\nfactor(gender), : Ignoring unknown parameters: `orientation`\n\n# Display the plot\nplot\n\n\n\n\n\n\n\n\nHere is the original graph for comparison:"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#part-3---create-a-publication-quality-table",
    "href": "presentation-exercise/presentation-exercise.html#part-3---create-a-publication-quality-table",
    "title": "Presentation Exercise",
    "section": "Part 3 - Create a Publication Quality Table",
    "text": "Part 3 - Create a Publication Quality Table\nFirst, I used ChatGPT to help me try to re-create the original table in the article “The Most Common Unisex Names In America: Is Yours One Of Them?” on the FiveThirtyEight website.\nChatGPT prompt:\n“Write R code that creates a professional, publication quality table with 14 rows and 5 columns. The first row is the header row with bold capitalized font and contains the following input: NAME, TOTAL, MALE SHARE, FEMALE SHARE, and GAP. The first row is separated from the rest of the table with a bold line. The second row has the following input: Bless; 280, 50.00%; 50.00%; and 0.01. The third row has the following input: Camdyn; 3,275; 49.99%, 50.01%, and 0.02. The fourth row has the following input: Kodee; 533; 50.02%; 49.98%; and 0.03.”\n\n# Load necessary libraries\nlibrary(knitr)\nlibrary(kableExtra)\n\nWarning: package 'kableExtra' was built under R version 4.3.2\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n# Create the data frame\ndata &lt;- data.frame(\n  NAME = c(\"Bless\", \"Camdyn\", \"Kodee\", \"Callaway\", \"Tkai\", \"Nike\", \"Christian\", \"Kam\", \"Jonel\", \"Kris\", \"Myrl\", \"Daine\", \"Shaya\"),\n  TOTAL = c(280, 3275, 533, 292, 143, 206, 2685, 729, 613, 24956, 869, 554, 1373),\n  MALE_SHARE = c(\"50.00%\", \"49.99%\", \"50.02%\", \"49.98%\", \"49.97%\", \"49.96%\", \"50.11%\", \"50.12%\", \"49.88%\", \"49.86%\", \"50.18%\", \"49.80%\", \"49.80%\"),\n  FEMALE_SHARE = c(\"50.00%\", \"50.01%\", \"49.98%\", \"50.02%\", \"50.03%\", \"50.04%\", \"49.89%\", \"49.88%\", \"50.12%\", \"50.14%\", \"49.82%\", \"50.20%\", \"50.20%\"),\n  GAP = c(\"0.01\", \"0.02\", \"0.03\", \"0.05\", \"0.06\", \"0.08\", \"0.23\", \"0.23\", \"0.24\", \"0.27\", \"0.37\", \"0.41\", \"0.41\")\n)\n\n# Format the data frame\ndata$MALE_SHARE &lt;- paste0(data$MALE_SHARE, \"%\")\ndata$FEMALE_SHARE &lt;- paste0(data$FEMALE_SHARE, \"%\")\n\n# Create the table with bold header and separator line\ntable &lt;- data %&gt;%\n  kable(\"html\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n# Print the table\ntable\n\n\n\n\nNAME\nTOTAL\nMALE_SHARE\nFEMALE_SHARE\nGAP\n\n\n\n\nBless\n280\n50.00%%\n50.00%%\n0.01\n\n\nCamdyn\n3275\n49.99%%\n50.01%%\n0.02\n\n\nKodee\n533\n50.02%%\n49.98%%\n0.03\n\n\nCallaway\n292\n49.98%%\n50.02%%\n0.05\n\n\nTkai\n143\n49.97%%\n50.03%%\n0.06\n\n\nNike\n206\n49.96%%\n50.04%%\n0.08\n\n\nChristian\n2685\n50.11%%\n49.89%%\n0.23\n\n\nKam\n729\n50.12%%\n49.88%%\n0.23\n\n\nJonel\n613\n49.88%%\n50.12%%\n0.24\n\n\nKris\n24956\n49.86%%\n50.14%%\n0.27\n\n\nMyrl\n869\n50.18%%\n49.82%%\n0.37\n\n\nDaine\n554\n49.80%%\n50.20%%\n0.41\n\n\nShaya\n1373\n49.80%%\n50.20%%\n0.41\n\n\n\n\n\n\n\nThen I manually edited the code produced by ChatGPT to re-create the original table as close as possible.\n\n# Load necessary libraries\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Create the data frame\ndata &lt;- data.frame(\n  NAME = c(\"Bless\", \"Camdyn\", \"Kodee\", \"Callaway\", \"Tkai\", \"Nike\", \"Christian\", \"Kam\", \"Jonel\", \"Kris\", \"Myrl\", \"Daine\", \"Shaya\"),\n  TOTAL = c(\"280\", \"3,275\", \"533\", \"292\", \"143\", \"206\", \"2,685\", \"729\", \"613\", \"24,956\", \"869\", \"554\", \"1,373\"),\n  \"MALE SHARE\" = c(\"50.00%\", \"49.99%\", \"50.02%\", \"49.98%\", \"49.97%\", \"49.96%\", \"50.11%\", \"50.12%\", \"49.88%\", \"49.86%\", \"50.18%\", \"49.80%\", \"49.80%\"),\n  \"FEMALE SHARE\" = c(\"50.00%\", \"50.01%\", \"49.98%\", \"50.02%\", \"50.03%\", \"50.04%\", \"49.89%\", \"49.88%\", \"50.12%\", \"50.14%\", \"49.82%\", \"50.20%\", \"50.20%\"),\n  GAP = c(\"0.01\", \"0.02\", \"0.03\", \"0.05\", \"0.06\", \"0.08\", \"0.23\", \"0.23\", \"0.24\", \"0.27\", \"0.37\", \"0.41\", \"0.41\")\n)\n\n# Create the table with bold header and separator line\ntable &lt;- data %&gt;%\n  kable(\"html\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n# Print the table\ntable\n\n\n\n\nNAME\nTOTAL\nMALE.SHARE\nFEMALE.SHARE\nGAP\n\n\n\n\nBless\n280\n50.00%\n50.00%\n0.01\n\n\nCamdyn\n3,275\n49.99%\n50.01%\n0.02\n\n\nKodee\n533\n50.02%\n49.98%\n0.03\n\n\nCallaway\n292\n49.98%\n50.02%\n0.05\n\n\nTkai\n143\n49.97%\n50.03%\n0.06\n\n\nNike\n206\n49.96%\n50.04%\n0.08\n\n\nChristian\n2,685\n50.11%\n49.89%\n0.23\n\n\nKam\n729\n50.12%\n49.88%\n0.23\n\n\nJonel\n613\n49.88%\n50.12%\n0.24\n\n\nKris\n24,956\n49.86%\n50.14%\n0.27\n\n\nMyrl\n869\n50.18%\n49.82%\n0.37\n\n\nDaine\n554\n49.80%\n50.20%\n0.41\n\n\nShaya\n1,373\n49.80%\n50.20%\n0.41\n\n\n\n\n\n\n\nHere is the original table for comparison:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ranni Tewfik’s Website and Data Analysis Portfolio",
    "section": "",
    "text": "Welcome to my website and portfolio!\n\n\nPlease use the Menu Bar above to look around.\nHave fun!\n\nYou can contact me at rtewfik@uga.edu"
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "First, let’s generate a synthetic dataset with 100 observations and 5 variables: age, sex, hypertension, cholesterol, and heart_health.\n\n#Load required packages\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.3.2\n\n#Set seed for reproducibility\nset.seed(123)\n\n#Set sample size\nn &lt;- 100\n\n#Create ages between 18 and 79\nage &lt;- sample(18:79, n, replace = TRUE)\n\n#Create sexes (male or female)\nsex &lt;- sample(c(\"male\", \"female\"), n, replace = TRUE)\n\n#Create hypertension status (yes or no)\nhypertension &lt;- sample(c(\"yes\", \"no\"), n, replace = TRUE)\n\n#Create total cholesterol levels between 125 and 275, assuming males have higher total cholesterol than females\ncholesterol &lt;- ifelse(sex == \"male\", rnorm(n, mean = 220, sd = 20), rnorm(n, mean = 200, sd = 20))\ncholesterol &lt;- pmax(pmin(cholesterol, 275), 125)\n\n#Create heart health scores between 25 and 90, assuming those with higher total cholesterol have higher heart health scores\nheart_health &lt;- 50 + 0.2 * cholesterol + rnorm(n, mean = 0, sd = 10)\nheart_health &lt;- pmax(pmin(heart_health, 90), 25)\n\n#Create a data frame to store the generated data\nheart &lt;- data.frame(age = age, sex = sex, hypertension = hypertension, cholesterol = cholesterol, heart_health = heart_health)\n\n#Get an overview and summary of the data\nstr(heart)\n\n'data.frame':   100 obs. of  5 variables:\n $ age         : int  48 32 68 31 20 59 67 71 60 54 ...\n $ sex         : chr  \"male\" \"female\" \"female\" \"male\" ...\n $ hypertension: chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ cholesterol : num  232 182 200 220 218 ...\n $ heart_health: num  90 89.7 86.9 90 90 ...\n\nsummary(heart)\n\n      age            sex            hypertension        cholesterol   \n Min.   :20.00   Length:100         Length:100         Min.   :158.2  \n 1st Qu.:32.00   Class :character   Class :character   1st Qu.:194.2  \n Median :46.00   Mode  :character   Mode  :character   Median :209.5  \n Mean   :47.56                                         Mean   :209.3  \n 3rd Qu.:60.50                                         3rd Qu.:225.9  \n Max.   :79.00                                         Max.   :263.9  \n  heart_health  \n Min.   :57.23  \n 1st Qu.:84.12  \n Median :90.00  \n Mean   :85.84  \n 3rd Qu.:90.00  \n Max.   :90.00"
  },
  {
    "objectID": "data-exercise/data-exercise.html#part-1---generating-synthetic-dataset-and-checking-data",
    "href": "data-exercise/data-exercise.html#part-1---generating-synthetic-dataset-and-checking-data",
    "title": "Data Exercise",
    "section": "",
    "text": "library(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n#Set seed for reproducibility\nset.seed(123)\n\n#Set sample size\nn &lt;- 100\n\n#Create ages between 18 and 79\nage &lt;- sample(18:79, n, replace = TRUE)\n\n#Create sexes (male or female)\nsex &lt;- sample(c(\"male\", \"female\"), n, replace = TRUE)\n\n#Create hypertension status (yes or no)\nhypertension &lt;- sample(c(\"yes\", \"no\"), n, replace = TRUE)\n\n#Create total cholesterol levels between 125 and 275, assuming males have higher total cholesterol than females\ncholesterol &lt;- ifelse(sex == \"male\", rnorm(n, mean = 220, sd = 20), rnorm(n, mean = 200, sd = 20))\ncholesterol &lt;- pmax(pmin(cholesterol, 275), 125)\n\n#Create heart health scores between 25 and 90, assuming those with higher total cholesterol have higher heart health scores\nheart_health &lt;- 50 + 0.2 * cholesterol + rnorm(n, mean = 0, sd = 10)\nheart_health &lt;- pmax(pmin(heart_health, 90), 25)\n\n#Create a data frame to store the generated data\nheart &lt;- data.frame(age = age, sex = sex, hypertension = hypertension, cholesterol = cholesterol, heart_health = heart_health)\n\n#Get an overview and summary of the data\nstr(heart)\n\n'data.frame':   100 obs. of  5 variables:\n $ age         : int  48 32 68 31 20 59 67 71 60 54 ...\n $ sex         : chr  \"male\" \"female\" \"female\" \"male\" ...\n $ hypertension: chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ cholesterol : num  232 182 200 220 218 ...\n $ heart_health: num  90 89.7 86.9 90 90 ...\n\nsummary(heart)\n\n      age            sex            hypertension        cholesterol   \n Min.   :20.00   Length:100         Length:100         Min.   :158.2  \n 1st Qu.:32.00   Class :character   Class :character   1st Qu.:194.2  \n Median :46.00   Mode  :character   Mode  :character   Median :209.5  \n Mean   :47.56                                         Mean   :209.3  \n 3rd Qu.:60.50                                         3rd Qu.:225.9  \n Max.   :79.00                                         Max.   :263.9  \n  heart_health  \n Min.   :57.23  \n 1st Qu.:84.12  \n Median :90.00  \n Mean   :85.84  \n 3rd Qu.:90.00  \n Max.   :90.00"
  },
  {
    "objectID": "data-exercise/data-exercise.html#part-2---processing-data",
    "href": "data-exercise/data-exercise.html#part-2---processing-data",
    "title": "Data Exercise",
    "section": "Part 2 - Processing Data",
    "text": "Part 2 - Processing Data\nThen let’s create two subset datasets, one with only males and one with only females.\n\n#Assign only males in the \"heart\" dataset to a new object\nheart_males &lt;- subset(heart, sex == \"male\")\n\n#Assign only females in the \"heart\" dataset to a new object\nheart_females &lt;- subset(heart, sex == \"female\")\n\n#Get an overview and summary of the new objects\nstr(heart_males)\n\n'data.frame':   45 obs. of  5 variables:\n $ age         : int  48 31 20 59 69 31 44 68 74 45 ...\n $ sex         : chr  \"male\" \"male\" \"male\" \"male\" ...\n $ hypertension: chr  \"yes\" \"no\" \"yes\" \"no\" ...\n $ cholesterol : num  232 220 218 188 231 ...\n $ heart_health: num  90 90 90 90 90 ...\n\nsummary(heart_males)\n\n      age            sex            hypertension        cholesterol   \n Min.   :20.00   Length:45          Length:45          Min.   :183.4  \n 1st Qu.:35.00   Class :character   Class :character   1st Qu.:204.9  \n Median :48.00   Mode  :character   Mode  :character   Median :225.7  \n Mean   :48.16                                         Mean   :221.3  \n 3rd Qu.:59.00                                         3rd Qu.:233.9  \n Max.   :77.00                                         Max.   :263.9  \n  heart_health  \n Min.   :66.22  \n 1st Qu.:87.89  \n Median :90.00  \n Mean   :87.38  \n 3rd Qu.:90.00  \n Max.   :90.00  \n\nstr(heart_females)\n\n'data.frame':   55 obs. of  5 variables:\n $ age         : int  32 68 67 71 60 54 71 42 43 22 ...\n $ sex         : chr  \"female\" \"female\" \"female\" \"female\" ...\n $ hypertension: chr  \"no\" \"no\" \"no\" \"no\" ...\n $ cholesterol : num  182 200 181 215 218 ...\n $ heart_health: num  89.7 86.9 83.3 66.2 81.2 ...\n\nsummary(heart_females)\n\n      age            sex            hypertension        cholesterol   \n Min.   :22.00   Length:55          Length:55          Min.   :158.2  \n 1st Qu.:32.00   Class :character   Class :character   1st Qu.:185.2  \n Median :44.00   Mode  :character   Mode  :character   Median :201.5  \n Mean   :47.07                                         Mean   :199.4  \n 3rd Qu.:63.50                                         3rd Qu.:215.8  \n Max.   :79.00                                         Max.   :229.4  \n  heart_health  \n Min.   :57.23  \n 1st Qu.:81.09  \n Median :88.70  \n Mean   :84.58  \n 3rd Qu.:90.00  \n Max.   :90.00  \n\n\nLet’s create a third subset dataset with only those with hypertension.\n\n#Create a new object by extracting only the data for those with hypertension from the \"heart\" dataset\nheart_hypertension &lt;- subset(heart, hypertension == \"yes\")\n\n#Get an overview and summary of the new object\nstr(heart_hypertension)\n\n'data.frame':   52 obs. of  5 variables:\n $ age         : int  48 20 60 54 43 22 44 74 45 46 ...\n $ sex         : chr  \"male\" \"male\" \"female\" \"female\" ...\n $ hypertension: chr  \"yes\" \"yes\" \"yes\" \"yes\" ...\n $ cholesterol : num  232 218 218 193 210 ...\n $ heart_health: num  90 90 81.2 90 90 ...\n\nsummary(heart_hypertension)\n\n      age            sex            hypertension        cholesterol   \n Min.   :20.00   Length:52          Length:52          Min.   :158.2  \n 1st Qu.:38.75   Class :character   Class :character   1st Qu.:196.3  \n Median :48.00   Mode  :character   Mode  :character   Median :212.5  \n Mean   :48.77                                         Mean   :212.3  \n 3rd Qu.:60.50                                         3rd Qu.:226.2  \n Max.   :79.00                                         Max.   :263.9  \n  heart_health  \n Min.   :57.23  \n 1st Qu.:84.81  \n Median :90.00  \n Mean   :86.36  \n 3rd Qu.:90.00  \n Max.   :90.00"
  },
  {
    "objectID": "data-exercise/data-exercise.html#part-3---plotting-data",
    "href": "data-exercise/data-exercise.html#part-3---plotting-data",
    "title": "Data Exercise",
    "section": "Part 3 - Plotting Data",
    "text": "Part 3 - Plotting Data\n\n#Plot heart health as a function of age for males and females\nggplot(heart_males, aes(x = age, y = heart_health)) + geom_point() + ggtitle(\"Age and Heart Health Score Among Males\")\n\n\n\n\n\n\n\nggplot(heart_females, aes(x = age, y = heart_health)) + geom_point() + ggtitle(\"Age and Heart Health Score Among Females\")\n\n\n\n\n\n\n\n\nThere is no noticeable correlation between age and heart health score for either males or females. There are several data points at heart health = 90, which is more than expected. This study sample may be healthier than the average population.\n\n#Plot heart health as a function of cholesterol for males and females\nggplot(heart_males, aes(x = cholesterol, y = heart_health)) + geom_point() + ggtitle(\"Total Cholesterol and Heart Health Score Among Males\")\n\n\n\n\n\n\n\nggplot(heart_females, aes(x = cholesterol, y = heart_health)) + geom_point() + ggtitle(\"Total Cholesterol and Heart Health Score Among Females\")\n\n\n\n\n\n\n\n\nThere is a positive correlation between total cholesterol and heart health score for males but no noticeable correlation or females. There are several data points at heart health = 90, which is more than expected. This study sample may be healthier than the average population."
  },
  {
    "objectID": "data-exercise/data-exercise.html#part-4---more-data-processing",
    "href": "data-exercise/data-exercise.html#part-4---more-data-processing",
    "title": "Data Exercise",
    "section": "Part 4 - More Data Processing",
    "text": "Part 4 - More Data Processing\n\n#Create a new object by extracting only the data for those with hypertension from the \"heart\" dataset\nheart_hypertension &lt;- subset(heart, hypertension == \"yes\")\n\n#Get an overview and summary of the new object\nstr(heart_hypertension)\n\n'data.frame':   52 obs. of  5 variables:\n $ age         : int  48 20 60 54 43 22 44 74 45 46 ...\n $ sex         : chr  \"male\" \"male\" \"female\" \"female\" ...\n $ hypertension: chr  \"yes\" \"yes\" \"yes\" \"yes\" ...\n $ cholesterol : num  232 218 218 193 210 ...\n $ heart_health: num  90 90 81.2 90 90 ...\n\nsummary(heart_hypertension)\n\n      age            sex            hypertension        cholesterol   \n Min.   :20.00   Length:52          Length:52          Min.   :158.2  \n 1st Qu.:38.75   Class :character   Class :character   1st Qu.:196.3  \n Median :48.00   Mode  :character   Mode  :character   Median :212.5  \n Mean   :48.77                                         Mean   :212.3  \n 3rd Qu.:60.50                                         3rd Qu.:226.2  \n Max.   :79.00                                         Max.   :263.9  \n  heart_health  \n Min.   :57.23  \n 1st Qu.:84.81  \n Median :90.00  \n Mean   :86.36  \n 3rd Qu.:90.00  \n Max.   :90.00"
  },
  {
    "objectID": "data-exercise/data-exercise.html#part-5---more-data-plotting",
    "href": "data-exercise/data-exercise.html#part-5---more-data-plotting",
    "title": "Data Exercise",
    "section": "Part 5 - More Data Plotting",
    "text": "Part 5 - More Data Plotting\n\n#Plot heart health as a function of age for those with hypertension\nggplot(heart_hypertension, aes(x = age, y = heart_health)) + geom_point() + ggtitle(\"Age and Heart Health Score Among Those With Hypertension\")\n\n\n\n\n\n\n\n\nThere is no noticeable correlation between age and heart health score for those with hypertension. There are several data points at heart health = 90, which is more than expected. This study sample may be healthier than the average population.\n\n#Plot heart health as a function of cholesterol for those with hypertension\nggplot(heart_hypertension, aes(x = cholesterol, y = heart_health)) + geom_point() + ggtitle(\"Total Cholesterol and Heart Health Score Among Those With Hypertension\")\n\n\n\n\n\n\n\n\nThere is a positive correlation between total cholesterol and heart health score for those with hypertension. There are several data points at heart health = 90, which is more than expected. This study sample may be healthier than the average population."
  },
  {
    "objectID": "data-exercise/data-exercise.html#part-6---fitting-simple-models",
    "href": "data-exercise/data-exercise.html#part-6---fitting-simple-models",
    "title": "Data Exercise",
    "section": "Part 6 - Fitting Simple Models",
    "text": "Part 6 - Fitting Simple Models\n\n#Fit heart health as the outcome and age and cholesterol as the predictors for males\nmales &lt;- lm(heart_health ~ age + cholesterol, data = heart_males)\nsummary(males)\n\n\nCall:\nlm(formula = heart_health ~ age + cholesterol, data = heart_males)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.072  -1.271   1.130   2.627   7.094 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 62.31682    8.77786   7.099 1.05e-08 ***\nage          0.02096    0.04466   0.469  0.64128    \ncholesterol  0.10870    0.03826   2.841  0.00691 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.848 on 42 degrees of freedom\nMultiple R-squared:  0.1646,    Adjusted R-squared:  0.1248 \nF-statistic: 4.138 on 2 and 42 DF,  p-value: 0.0229\n\n\nAmong males, total cholesterol is significantly associated with heart health score (p-value = 0.01), controlling for age; however, age is not significantly associated with hearth health score, controlling for total cholesterol (p-value = 0.64). The low R-squared value indicates that only 16% of the variation in heart health score is explained by the model.\n\n#Fit heart health as the outcome and age and cholesterol as the predictors for females\nfemales &lt;- lm(heart_health ~ age + cholesterol, data = heart_females)\nsummary(females)\n\n\nCall:\nlm(formula = heart_health ~ age + cholesterol, data = heart_females)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.401  -2.929   3.104   4.628   7.485 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 67.57861   12.25700   5.513 1.12e-06 ***\nage         -0.06280    0.06099  -1.030   0.3079    \ncholesterol  0.10005    0.05554   1.801   0.0774 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.405 on 52 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.06671 \nF-statistic:  2.93 on 2 and 52 DF,  p-value: 0.06227\n\n\nAmong females, neither age (p-value = 0.31) nor total cholesterol (p-value = 0.08) is significantly associated with heart health, controlling for other variables. The low R-squared value indicates that only 10% of the variation in heart health score is explained by the model.\n\n#Fit heart health as the outcome and age and cholesterol as the predictors for those with hypertension\nhtn &lt;- lm(heart_health ~ age + cholesterol, data = heart_hypertension)\nsummary(htn)\n\n\nCall:\nlm(formula = heart_health ~ age + cholesterol, data = heart_hypertension)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.578  -1.344   2.032   4.033   6.385 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 64.19107    9.56511   6.711 1.86e-08 ***\nage         -0.03406    0.05325  -0.640   0.5255    \ncholesterol  0.11228    0.04255   2.639   0.0111 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.276 on 49 degrees of freedom\nMultiple R-squared:  0.1338,    Adjusted R-squared:  0.09839 \nF-statistic: 3.783 on 2 and 49 DF,  p-value: 0.02966\n\n\nAmong those with hypertension, total cholesterol is significantly associated with heart health score (p-value = 0.01), controlling for age; however, age is not significantly associated with hearth health score, controlling for total cholesterol (p-value = 0.53). The low R-squared value indicates that only 13% of the variation in heart health score is explained by the model."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The “NCHS - Leading Causes of Death: United States” dataset used for this exercise provides data on the age-adjusted death rates for the 10 leading causes of death in the U.S. (1999-2017). The source is the mortality data from the National Vital Statistics System (CDC/NCHS).\nResident death certificates from all 50 states and D.C. were used to collect the data on demographic and medical characteristics. Age-adjusted death rates (per 100,000 population) are based on the 2000 U.S. standard population. Populations used for computing death rates after 2010 are postcensal estimates based on the 2010 census.\nThe dataset has 10,868 observations and six variables: year, X113 cause name (specific cause of death), cause name (categorical cause of death), state, deaths, and age-adjusted death rate. To learn more about the data, please visit the CDC website."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#part-1---about-the-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#part-1---about-the-data",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The “NCHS - Leading Causes of Death: United States” dataset used for this exercise provides data on the age-adjusted death rates for the 10 leading causes of death in the U.S. (1999-2017). The source is the mortality data from the National Vital Statistics System (CDC/NCHS).\nResident death certificates from all 50 states and D.C. were used to collect the data on demographic and medical characteristics. Age-adjusted death rates (per 100,000 population) are based on the 2000 U.S. standard population. Populations used for computing death rates after 2010 are postcensal estimates based on the 2010 census.\nThe dataset has 10,868 observations and six variables: year, X113 cause name (specific cause of death), cause name (categorical cause of death), state, deaths, and age-adjusted death rate. To learn more about the data, please visit the CDC website."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#part-2---processing-the-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#part-2---processing-the-data",
    "title": "CDC Data Exercise",
    "section": "Part 2 - Processing the Data",
    "text": "Part 2 - Processing the Data\n\n#Load the required packages\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n#Load the raw data\ncdc_raw &lt;- read.csv(\"NCHS_Leading_Causes_of_Death.csv\")\n\n#Get an overview and summary of the raw data\nstr(cdc_raw)\n\n'data.frame':   10868 obs. of  6 variables:\n $ Year                   : int  2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 ...\n $ X113.Cause.Name        : chr  \"Accidents (unintentional injuries) (V01-X59,Y85-Y86)\" \"Accidents (unintentional injuries) (V01-X59,Y85-Y86)\" \"Accidents (unintentional injuries) (V01-X59,Y85-Y86)\" \"Accidents (unintentional injuries) (V01-X59,Y85-Y86)\" ...\n $ Cause_Name             : chr  \"Unintentional injuries\" \"Unintentional injuries\" \"Unintentional injuries\" \"Unintentional injuries\" ...\n $ State                  : chr  \"United States\" \"Alabama\" \"Alaska\" \"Arizona\" ...\n $ Deaths                 : int  169936 2703 436 4184 1625 13840 3037 2078 608 427 ...\n $ Age_Adjusted_Death_Rate: num  49.4 53.8 63.7 56.2 51.8 33.2 53.6 53.2 61.9 61 ...\n\nsummary(cdc_raw)\n\n      Year      X113.Cause.Name     Cause_Name           State          \n Min.   :1999   Length:10868       Length:10868       Length:10868      \n 1st Qu.:2003   Class :character   Class :character   Class :character  \n Median :2008   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2008                                                           \n 3rd Qu.:2013                                                           \n Max.   :2017                                                           \n     Deaths        Age_Adjusted_Death_Rate\n Min.   :     21   Min.   :   2.6         \n 1st Qu.:    612   1st Qu.:  19.2         \n Median :   1718   Median :  35.9         \n Mean   :  15460   Mean   : 127.6         \n 3rd Qu.:   5756   3rd Qu.: 151.7         \n Max.   :2813503   Max.   :1087.3         \n\n#Check for weird symbols or missing values in all six variables\ntable(cdc_raw$Year, useNA = \"always\")\n\n\n1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 \n 572  572  572  572  572  572  572  572  572  572  572  572  572  572  572  572 \n2015 2016 2017 &lt;NA&gt; \n 572  572  572    0 \n\ntable(cdc_raw$X113.Cause.Name, useNA = \"always\")\n\n\n                 Accidents (unintentional injuries) (V01-X59,Y85-Y86) \n                                                                  988 \n                                                           All Causes \n                                                                  988 \n                                            Alzheimer's disease (G30) \n                                                                  988 \n                                   Cerebrovascular diseases (I60-I69) \n                                                                  988 \n                         Chronic lower respiratory diseases (J40-J47) \n                                                                  988 \n                                          Diabetes mellitus (E10-E14) \n                                                                  988 \n                          Diseases of heart (I00-I09,I11,I13,I20-I51) \n                                                                  988 \n                                    Influenza and pneumonia (J09-J18) \n                                                                  988 \n                 Intentional self-harm (suicide) (*U03,X60-X84,Y87.0) \n                                                                  988 \n                                        Malignant neoplasms (C00-C97) \n                                                                  988 \nNephritis, nephrotic syndrome and nephrosis (N00-N07,N17-N19,N25-N27) \n                                                                  988 \n                                                                 &lt;NA&gt; \n                                                                    0 \n\ntable(cdc_raw$Cause_Name, useNA = \"always\")\n\n\n             All causes     Alzheimer's disease                  Cancer \n                    988                     988                     988 \n                   CLRD                Diabetes           Heart disease \n                    988                     988                     988 \nInfluenza and pneumonia          Kidney disease                  Stroke \n                    988                     988                     988 \n                Suicide  Unintentional injuries                    &lt;NA&gt; \n                    988                     988                       0 \n\ntable(cdc_raw$State, useNA = \"always\")\n\n\n             Alabama               Alaska              Arizona \n                 209                  209                  209 \n            Arkansas           California             Colorado \n                 209                  209                  209 \n         Connecticut             Delaware District of Columbia \n                 209                  209                  209 \n             Florida              Georgia               Hawaii \n                 209                  209                  209 \n               Idaho             Illinois              Indiana \n                 209                  209                  209 \n                Iowa               Kansas             Kentucky \n                 209                  209                  209 \n           Louisiana                Maine             Maryland \n                 209                  209                  209 \n       Massachusetts             Michigan            Minnesota \n                 209                  209                  209 \n         Mississippi             Missouri              Montana \n                 209                  209                  209 \n            Nebraska               Nevada        New Hampshire \n                 209                  209                  209 \n          New Jersey           New Mexico             New York \n                 209                  209                  209 \n      North Carolina         North Dakota                 Ohio \n                 209                  209                  209 \n            Oklahoma               Oregon         Pennsylvania \n                 209                  209                  209 \n        Rhode Island       South Carolina         South Dakota \n                 209                  209                  209 \n           Tennessee                Texas        United States \n                 209                  209                  209 \n                Utah              Vermont             Virginia \n                 209                  209                  209 \n          Washington        West Virginia            Wisconsin \n                 209                  209                  209 \n             Wyoming                 &lt;NA&gt; \n                 209                    0 \n\nsummary(cdc_raw$Deaths)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     21     612    1718   15460    5756 2813503 \n\nsummary(cdc_raw$Age_Adjusted_Death_Rate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.6    19.2    35.9   127.6   151.7  1087.3 \n\n#Drop the \"X113.Cause.Name\" variable for the processed data\ncdc &lt;- select(cdc_raw, -X113.Cause.Name)\nattach(cdc)\n\n#Get an overview and summary of the processed data\nstr(cdc)\n\n'data.frame':   10868 obs. of  5 variables:\n $ Year                   : int  2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 ...\n $ Cause_Name             : chr  \"Unintentional injuries\" \"Unintentional injuries\" \"Unintentional injuries\" \"Unintentional injuries\" ...\n $ State                  : chr  \"United States\" \"Alabama\" \"Alaska\" \"Arizona\" ...\n $ Deaths                 : int  169936 2703 436 4184 1625 13840 3037 2078 608 427 ...\n $ Age_Adjusted_Death_Rate: num  49.4 53.8 63.7 56.2 51.8 33.2 53.6 53.2 61.9 61 ...\n\nsummary(cdc)\n\n      Year       Cause_Name           State               Deaths       \n Min.   :1999   Length:10868       Length:10868       Min.   :     21  \n 1st Qu.:2003   Class :character   Class :character   1st Qu.:    612  \n Median :2008   Mode  :character   Mode  :character   Median :   1718  \n Mean   :2008                                         Mean   :  15460  \n 3rd Qu.:2013                                         3rd Qu.:   5756  \n Max.   :2017                                         Max.   :2813503  \n Age_Adjusted_Death_Rate\n Min.   :   2.6         \n 1st Qu.:  19.2         \n Median :  35.9         \n Mean   : 127.6         \n 3rd Qu.: 151.7         \n Max.   :1087.3         \n\n\nThere are no weird symbols or missing values in the raw data. Because the variables for “X113 cause name” and “cause name” provide the same information, the variable for “X113 cause name” is removed from the processed data. The new dataset has 10,868 observations and five variables: year, cause name, state, deaths, and age-adjusted death rate."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#part-3---exploring-the-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#part-3---exploring-the-data",
    "title": "CDC Data Exercise",
    "section": "Part 3 - Exploring the Data",
    "text": "Part 3 - Exploring the Data\n\n#Summary tables for the categorical variable \"Year\"\ntable(Year)\n\nYear\n1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 \n 572  572  572  572  572  572  572  572  572  572  572  572  572  572  572  572 \n2015 2016 2017 \n 572  572  572 \n\nprop.table(table(Year))\n\nYear\n      1999       2000       2001       2002       2003       2004       2005 \n0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 \n      2006       2007       2008       2009       2010       2011       2012 \n0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 \n      2013       2014       2015       2016       2017 \n0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 \n\n\n19 years (1999-2017) are represented in the categorical variable “Year”, each with 572 observations (5.3%).\n\n#Summary tables for the categorical variable \"Cause_Name\"\ntable(Cause_Name)\n\nCause_Name\n             All causes     Alzheimer's disease                  Cancer \n                    988                     988                     988 \n                   CLRD                Diabetes           Heart disease \n                    988                     988                     988 \nInfluenza and pneumonia          Kidney disease                  Stroke \n                    988                     988                     988 \n                Suicide  Unintentional injuries \n                    988                     988 \n\nprop.table(table(Cause_Name))\n\nCause_Name\n             All causes     Alzheimer's disease                  Cancer \n             0.09090909              0.09090909              0.09090909 \n                   CLRD                Diabetes           Heart disease \n             0.09090909              0.09090909              0.09090909 \nInfluenza and pneumonia          Kidney disease                  Stroke \n             0.09090909              0.09090909              0.09090909 \n                Suicide  Unintentional injuries \n             0.09090909              0.09090909 \n\n\n11 cause categories are represented in the categorical variable “Cause_Name”, each with 988 observations (9.1%). The cause categories are “All causes”, “Alzheimer’s disease”, “Cancer”, “CLRD”, “Diabetes”, “Heart disease”, “Influenza and pneumonia”, “Kidney disease”, “Stroke”, “Suicide”, and “Unintentional injuries”.\n\n#Summary tables for the categorical variable \"State\"\ntable(State)\n\nState\n             Alabama               Alaska              Arizona \n                 209                  209                  209 \n            Arkansas           California             Colorado \n                 209                  209                  209 \n         Connecticut             Delaware District of Columbia \n                 209                  209                  209 \n             Florida              Georgia               Hawaii \n                 209                  209                  209 \n               Idaho             Illinois              Indiana \n                 209                  209                  209 \n                Iowa               Kansas             Kentucky \n                 209                  209                  209 \n           Louisiana                Maine             Maryland \n                 209                  209                  209 \n       Massachusetts             Michigan            Minnesota \n                 209                  209                  209 \n         Mississippi             Missouri              Montana \n                 209                  209                  209 \n            Nebraska               Nevada        New Hampshire \n                 209                  209                  209 \n          New Jersey           New Mexico             New York \n                 209                  209                  209 \n      North Carolina         North Dakota                 Ohio \n                 209                  209                  209 \n            Oklahoma               Oregon         Pennsylvania \n                 209                  209                  209 \n        Rhode Island       South Carolina         South Dakota \n                 209                  209                  209 \n           Tennessee                Texas        United States \n                 209                  209                  209 \n                Utah              Vermont             Virginia \n                 209                  209                  209 \n          Washington        West Virginia            Wisconsin \n                 209                  209                  209 \n             Wyoming \n                 209 \n\nprop.table(table(State))\n\nState\n             Alabama               Alaska              Arizona \n          0.01923077           0.01923077           0.01923077 \n            Arkansas           California             Colorado \n          0.01923077           0.01923077           0.01923077 \n         Connecticut             Delaware District of Columbia \n          0.01923077           0.01923077           0.01923077 \n             Florida              Georgia               Hawaii \n          0.01923077           0.01923077           0.01923077 \n               Idaho             Illinois              Indiana \n          0.01923077           0.01923077           0.01923077 \n                Iowa               Kansas             Kentucky \n          0.01923077           0.01923077           0.01923077 \n           Louisiana                Maine             Maryland \n          0.01923077           0.01923077           0.01923077 \n       Massachusetts             Michigan            Minnesota \n          0.01923077           0.01923077           0.01923077 \n         Mississippi             Missouri              Montana \n          0.01923077           0.01923077           0.01923077 \n            Nebraska               Nevada        New Hampshire \n          0.01923077           0.01923077           0.01923077 \n          New Jersey           New Mexico             New York \n          0.01923077           0.01923077           0.01923077 \n      North Carolina         North Dakota                 Ohio \n          0.01923077           0.01923077           0.01923077 \n            Oklahoma               Oregon         Pennsylvania \n          0.01923077           0.01923077           0.01923077 \n        Rhode Island       South Carolina         South Dakota \n          0.01923077           0.01923077           0.01923077 \n           Tennessee                Texas        United States \n          0.01923077           0.01923077           0.01923077 \n                Utah              Vermont             Virginia \n          0.01923077           0.01923077           0.01923077 \n          Washington        West Virginia            Wisconsin \n          0.01923077           0.01923077           0.01923077 \n             Wyoming \n          0.01923077 \n\n\n52 states/districts are represented in the categorical variable “State”, each with 209 observations (1.9%). All 50 states are represented, as well as “District of Columbia” and “United States”.\n\n#Summary statistics and histogram for the continuous variable \"Deaths\"\nsummary(Deaths)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     21     612    1718   15460    5756 2813503 \n\nsd(Deaths)\n\n[1] 112876\n\nggplot(cdc, aes(x = Deaths)) + geom_histogram(aes(y = after_stat(density), fill = \"red\"), bins = 50) + stat_function(fun = dnorm, args = with(cdc, c(mean = mean(Deaths), sd = sd(Deaths)))) + scale_x_continuous(\"Number of Deaths\") + ggtitle(\"Histogram with Normal Curve\")\n\n\n\n\n\n\n\n\nThe histogram for the continuous variable “Deaths” does not show an approximately normal distribution. The mean is 15,460 deaths, and the standard deviation is 112,876 deaths.\n\n#Summary statistics and histogram for the continuous variable \"Age_Adjusted_Death_Rate\"\nsummary(Age_Adjusted_Death_Rate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.6    19.2    35.9   127.6   151.7  1087.3 \n\nsd(Age_Adjusted_Death_Rate)\n\n[1] 223.6398\n\nggplot(cdc, aes(x = Age_Adjusted_Death_Rate)) + geom_histogram(aes(y = after_stat(density), fill = \"red\"), bins = 50) + stat_function(fun = dnorm, args = with(cdc, c(mean = mean(Age_Adjusted_Death_Rate), sd = sd(Age_Adjusted_Death_Rate)))) + scale_x_continuous(\"Age-Adjusted Death Rate\") + ggtitle(\"Histogram with Normal Curve\")\n\n\n\n\n\n\n\n\nThe histogram for the continuous variable “Age_Adjusted_Death_Rate” does not show an approximately normal distribution. The mean is 127.6 deaths per 100,000 population, and the standard deviation is 223.6 deaths per 100,000 population."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#synthetic-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#synthetic-data",
    "title": "CDC Data Exercise",
    "section": "Synthetic Data",
    "text": "Synthetic Data\nTo create a synthetic dataset based on the previous analysis, I provided a LLM AI tool (ChatGPT) the following information, so that it could generate some code to generate data based off of the numbers provided.\n\n\n\n\n\n\n\n\n\n\n‘Year’\n‘Age_Adjusted_Death_Rate’\n’Cause_Name”\n‘State’\n‘Deaths’\n\n\n\n\nMin. : 1999\n1st Qu. : 2003\nMedian : 2008\nMean : 2008\n3rd Qu. : 2013\nMax. : 2017\nMin.   :   2.6\n1st Qu.:  19.2\nMedian :  35.9\nMean   : 127.6\n3rd Qu.: 151.7\nMax.   :1087.3\nLength:10868\nClass :character\nMode :character\nCause categories:\nAll causes, Alzheimer’s disease, Cancer, CLRD, Diabetes, Heart disease, Influenza and pneumonia, Kidney disease, Stroke, Suicide, Unintentional injuries\nLength:10868\nClass :character\nMode :character\nMin. : 21\n1st Qu. : 612\nMedian : 1718\nMean : 15460\n3rd Qu. : 5756\nMax. : 28135503\n\n\n\nBased on what the AI generated I was able to edit and adjust as needed to get the following code:\nLoading the necessary libraries and setting a seed.\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(042)\n\nGenerating numerical values for synthetic dataset.\n\n# Define the number of observations\nnum_obs &lt;- 10000\n\n# Generate synthetic data for 'Year'\nyear &lt;- sample(1999:2017, num_obs, replace = TRUE)\n\n# Generate synthetic data for 'age_adjusted_death_rate'\nage_adjusted_death_rate &lt;- rnorm(num_obs, mean = 127.6, sd = 150)\n\n# Ensure no negative values for death rates\nage_adjusted_death_rate &lt;- pmax(age_adjusted_death_rate, 0)\n\n# Generate synthetic data for 'Deaths'\ndeaths &lt;- round(rnorm(num_obs, mean = 15460, sd = 10000))\n\nGenerating text values for synthetic dataset.\n\n# Generate synthetic data for 'Cause_Name'\ncause_name &lt;- sample(c(\"All causes\", \"Alzheimer's disease\", \"Cancer\", \"CLRD\", \n                       \"Diabetes\", \"Heart disease\", \"Influenza and pneumonia\", \n                       \"Kidney disease\", \"Stroke\", \"Suicide\", \"Unintentional injuries\"),\n                     num_obs, replace = TRUE)\n\n# Generate synthetic data for 'State'\nstates &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n            \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n            \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n            \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n            \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n            \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \n            \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \n            \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n            \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \n            \"West Virginia\", \"Wisconsin\", \"Wyoming\")\n\nstate &lt;- sample(states, num_obs, replace = TRUE)\n\nFinally, putting those values into the dataframe.\n\n# Create a data frame\nsynthetic_data &lt;- data.frame(Year = year,\n                             Age_Adjusted_Death_Rate = age_adjusted_death_rate,\n                             Cause_Name = cause_name,\n                             State = state,\n                             Deaths = deaths)"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#analyzing-the-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#analyzing-the-data",
    "title": "CDC Data Exercise",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\nSummary of synthetic dataset.\n\n# Display summary of synthetic data\nsummary(synthetic_data)\n\n      Year      Age_Adjusted_Death_Rate  Cause_Name           State          \n Min.   :1999   Min.   :  0.00          Length:10000       Length:10000      \n 1st Qu.:2003   1st Qu.: 25.24          Class :character   Class :character  \n Median :2008   Median :127.86          Mode  :character   Mode  :character  \n Mean   :2008   Mean   :143.95                                               \n 3rd Qu.:2013   3rd Qu.:228.17                                               \n Max.   :2017   Max.   :776.81                                               \n     Deaths      \n Min.   :-21699  \n 1st Qu.:  8614  \n Median : 15666  \n Mean   : 15611  \n 3rd Qu.: 22600  \n Max.   : 56124  \n\n\nSummary for the variable “year”.\n\n#Summary tables for the categorical variable \"year\"\ntable(year)\n\nyear\n1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 \n 528  538  512  517  544  509  512  504  497  607  534  550  540  516  495  504 \n2015 2016 2017 \n 562  510  521 \n\n\nSummary for the variable “cause_name”\n\n#Summary tables for the categorical variable \"cause_name\"\ntable(cause_name)\n\ncause_name\n             All causes     Alzheimer's disease                  Cancer \n                    954                     905                     927 \n                   CLRD                Diabetes           Heart disease \n                    921                     886                     873 \nInfluenza and pneumonia          Kidney disease                  Stroke \n                    950                     887                     931 \n                Suicide  Unintentional injuries \n                    875                     891 \n\n\nSummary for the variable “state”.\n\n#Summary tables for the categorical variable \"state\"\ntable(state)\n\nstate\n       Alabama         Alaska        Arizona       Arkansas     California \n           220            196            197            207            189 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n           216            214            196            197            170 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n           199            210            175            195            196 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n           200            206            210            209            217 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n           171            251            193            218            192 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n           242            187            182            201            196 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n           222            203            197            202            192 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n           171            209            195            183            178 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n           226            217            198            174            195 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n           180            186            200            213            207 \n\n\nSummary statistics and histogram for “death”.\n\n#Summary statistics and histogram for the continuous variable \"Deaths\"\nsummary(deaths)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -21699    8614   15666   15611   22600   56124 \n\nsd(deaths)\n\n[1] 10127.2\n\nggplot(synthetic_data, aes(x = deaths)) + \n  geom_histogram(aes(y = after_stat(density), \n  fill = \"red\"), bins = 50) + \n  stat_function(fun = dnorm, args = with(synthetic_data, \n  c(mean = mean(deaths), sd = sd(deaths)))) + \n  scale_x_continuous(\"Number of Deaths\") + \n  ggtitle(\"Histogram with Normal Curve\")\n\n\n\n\n\n\n\n\nThe histogram displays a strong normal distribution. Which varies from what was found in the original dataset. I suspect that this is due to the randomized nature of the synthetic data, vs the real data which contains real-world biases and trends.\n\nSummary statistics and histogram for “age_adjusted_death_rate”.\n\n#Summary statistics and histogram for the continuous variable \"age_adjusted_death_rate\"\nsummary(age_adjusted_death_rate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   25.24  127.86  143.95  228.17  776.81 \n\nsd(age_adjusted_death_rate)\n\n[1] 125.881\n\nggplot(synthetic_data, aes(x = age_adjusted_death_rate)) + \n  geom_histogram(aes(y = after_stat(density),\n  fill = \"red\"), bins = 50) + \n  stat_function(fun = dnorm, args = with(synthetic_data, \n  c(mean = mean(age_adjusted_death_rate),\n  sd = sd(age_adjusted_death_rate)))) + \n  scale_x_continuous(\"Age-Adjusted Death Rate\") + \n  ggtitle(\"Histogram with Normal Curve\")\n\n\n\n\n\n\n\n\nThis histogram is closer in appearance to the one generated from the original dataset. It does not follow an approximately normal distribution, the mean is 143.95 and the standard deviation is 125.881 which while not exactly the same, is in the same sort of range as the original dataset."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#final-thoughts",
    "href": "cdcdata-exercise/cdcdata-exercise.html#final-thoughts",
    "title": "CDC Data Exercise",
    "section": "Final thoughts",
    "text": "Final thoughts\nOverall I noticed that the synthetic dataset had more variation in values than the original. As stated previously I suspect that this is due to the random nature of the data generation within the synthetic dataset. Despite being based on statistics pulled from analysis of the original data, the syntehtic dataset does not have the nuances or biases of the original data."
  },
  {
    "objectID": "aboutme.html#background",
    "href": "aboutme.html#background",
    "title": "About Me",
    "section": "Background",
    "text": "Background\nHi! My name is Ranni Tewfik, and I am a first-year PhD student in Epidemiology at UGA. I have lived in Metro Atlanta all my life, and I look forward to spending the next few years in Athens. Go Dawgs!"
  },
  {
    "objectID": "aboutme.html#education",
    "href": "aboutme.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nPhD, Epidemiology, University of Georgia, 2023 - Present\nMPH, Global Epidemiology, Emory University, 2021 - 2023\nMA, Spanish, Georgia State University, 2012 - 2014\nBS, Biology, Georgia Institute of Technology, 2005 - 2009"
  },
  {
    "objectID": "aboutme.html#research",
    "href": "aboutme.html#research",
    "title": "About Me",
    "section": "Research",
    "text": "Research\nMy research interest is infectious disease epidemiology of vulnerable populations, including low-resource communities, immigrants, and refugees. I’m particularly interested in interventions that reduce morbidity and mortality from diseases in humanitarian emergencies and international settings.\nCurrently, I work with Dr. John McCracken, whose research focuses on the impact of household air pollution on maternal and child health in Guatemala. The Household Air Pollution Intervention Network (HAPIN) Trial is an international, multi-center study that assesses clean cooking fuel exposure and health. As part of HAPIN-Guatemala, we analyze data for the primary outcomes: low birth weight, severe pneumonia incidence, and stunting in children, and blood pressure in older adult women.\nTo learn more about my research group and the data analyses we have done so far, please visit the HAPIN website."
  },
  {
    "objectID": "aboutme.html#data-analysis-experience",
    "href": "aboutme.html#data-analysis-experience",
    "title": "About Me",
    "section": "Data Analysis Experience",
    "text": "Data Analysis Experience\n\nR: intermediate proficiency; coding and data analysis experience as Research Assistant at UGA\nSAS: intermediate proficiency; coding and data analysis experience as Research Assistant at Emory\nStata: beginner proficiency; coding and data analysis experience in specialized course at Emory"
  },
  {
    "objectID": "aboutme.html#mada-course",
    "href": "aboutme.html#mada-course",
    "title": "About Me",
    "section": "MADA Course",
    "text": "MADA Course\nI hope to learn about modern tools for data analysis in the Modern Applied Data Analysis course. I’m a bit familiar with some statistical programs, but I’m less familiar with GitHub and Quarto. Also, I’d like more experience with cleaning and processing real-world data."
  },
  {
    "objectID": "aboutme.html#fun-fact",
    "href": "aboutme.html#fun-fact",
    "title": "About Me",
    "section": "Fun Fact",
    "text": "Fun Fact\nI love to travel. Whether it’s a weeklong trip to India or a weekend getaway to Indiana, count me in! I’ve been to every continent, except Antarctica, but I hope to check that off my list in the future."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "#Load \"dslabs\" package\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.3.2\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#Look at help file for \"gapminder\"\nhelp(gapminder)\n\nstarting httpd help server ... done\n\n#Get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#Get a summary of data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#Determine the type of object \"gapminder\" is\nclass(gapminder)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#part-1---loading-and-checking-data",
    "href": "coding-exercise/coding-exercise.html#part-1---loading-and-checking-data",
    "title": "R Coding Exercise",
    "section": "",
    "text": "#Load \"dslabs\" package\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.3.2\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#Look at help file for \"gapminder\"\nhelp(gapminder)\n\nstarting httpd help server ... done\n\n#Get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#Get a summary of data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#Determine the type of object \"gapminder\" is\nclass(gapminder)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#part-2---processing-data",
    "href": "coding-exercise/coding-exercise.html#part-2---processing-data",
    "title": "R Coding Exercise",
    "section": "Part 2 - Processing Data",
    "text": "Part 2 - Processing Data\n\n#Assign only the African countries in \"gapminder\" to a new object\nafricadata &lt;- subset(gapminder, continent == \"Africa\")\n\n#Get an overview of data structure and data summary for \"africadata\"\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n#Create a new object that only contains infant mortality and life expectancy\nafricadata1 &lt;- africadata %&gt;% select(\"infant_mortality\", \"life_expectancy\")\n  \n#Create a new object that only contains population and life expectancy\nafricadata2 &lt;- africadata %&gt;% select(\"population\", \"life_expectancy\")\n\n#Get an overview of data structure and data summary for \"africadata1\" and \"africadata2\"\nstr(africadata1)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(africadata1)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\nstr(africadata2)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(africadata2)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#part-3---plotting",
    "href": "coding-exercise/coding-exercise.html#part-3---plotting",
    "title": "R Coding Exercise",
    "section": "Part 3 - Plotting",
    "text": "Part 3 - Plotting\n\n#Plot life expectancy as a function of infant mortality\nplot(africadata1$infant_mortality, africadata1$life_expectancy)\n\n\n\n\n\n\n\n\nThere is a negative correlation between infant mortality and life expectancy.\n\n#Plot life expectancy as a function of population size\nplot(africadata2$population, africadata2$life_expectancy, log = \"x\")\n\n\n\n\n\n\n\n\nThere is a positive correlation between population size and life expectancy, however, there are streaks of data that seem to go together. This is because there are different years for individual countries, and that over time these countries increase in population size and also life expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#part-4---more-data-processing",
    "href": "coding-exercise/coding-exercise.html#part-4---more-data-processing",
    "title": "R Coding Exercise",
    "section": "Part 4 - More Data Processing",
    "text": "Part 4 - More Data Processing\n\n#Figure out which years have missing data for infant mortality\nmissing_infant_mortality &lt;- africadata[is.na(africadata$infant_mortality),]\n\n#Create a new object by extracting only the data for the year 2000 from \"africadata\"\nafricadata3 &lt;- subset(africadata, year == \"2000\")\n\n#Get an overview of data structure and data summary for \"africadata3\"\nstr(africadata3)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata3)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#part-5---more-plotting",
    "href": "coding-exercise/coding-exercise.html#part-5---more-plotting",
    "title": "R Coding Exercise",
    "section": "Part 5 - More Plotting",
    "text": "Part 5 - More Plotting\n\n#Plot life expectancy as a function of infant mortality for the year 2000\nplot(africadata3$infant_mortality, africadata3$life_expectancy)\n\n\n\n\n\n\n\n\nThere is a negative correlation between infant mortality and life expectancy for the year 2000.\n\n#Plot life expectancy as a function of population size for the year 2000\nplot(africadata3$population, africadata3$life_expectancy, log = \"x\")\n\n\n\n\n\n\n\n\nThere is no noticeable correlation between population size and life expectancy for the year 2000."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#part-6---simple-model-fits",
    "href": "coding-exercise/coding-exercise.html#part-6---simple-model-fits",
    "title": "R Coding Exercise",
    "section": "Part 6 - Simple Model Fits",
    "text": "Part 6 - Simple Model Fits\n\n#Fit life expectancy as the outcome and infant mortality as the predictor\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africadata3)\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n\nFor the year 2000, infant mortality is significantly associated with life expectancy (p-value &lt; 0.001), however, the low R-squared value indicates that only 47% of the variation in life expectancy is explained by the model with only infant mortality as a predictor.\n\n#Fit life expectancy as the outcome and population size as the predictor\nfit2 &lt;- lm(life_expectancy ~ population, data = africadata3)\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africadata3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nFor the year 2000, population size is not significantly associated with life expectancy (p-value = 0.62), and the very low R-squared value indicates that only 0.52% of the variation in life expectancy is explained by the model with only population size as a predictor."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#this-section-contributed-by-patrick-kaggwa",
    "href": "coding-exercise/coding-exercise.html#this-section-contributed-by-patrick-kaggwa",
    "title": "R Coding Exercise",
    "section": "This section contributed by Patrick Kaggwa",
    "text": "This section contributed by Patrick Kaggwa\n\nA. Processing the Data\nUsing “murders” data frame from dslabs.\n\n# Explore the murders dataframe\n\n#Looking at the structure of US gun murders by state in 2010\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ...\n\n#I will use head() function to take a quick look at my data\n\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\nsummary(murders)\n\n    state               abb                      region     population      \n Length:51          Length:51          Northeast    : 9   Min.   :  563626  \n Class :character   Class :character   South        :17   1st Qu.: 1696962  \n Mode  :character   Mode  :character   North Central:12   Median : 4339367  \n                                       West         :13   Mean   : 6075769  \n                                                          3rd Qu.: 6636084  \n                                                          Max.   :37253956  \n     total       \n Min.   :   2.0  \n 1st Qu.:  24.5  \n Median :  97.0  \n Mean   : 184.4  \n 3rd Qu.: 268.0  \n Max.   :1257.0  \n\n\n\n# Now I will look through my data frame to see if have missing values using naniar from naniar package\nnaniar::gg_miss_var(murders)\n\n\n\n\n\n\n\n\nI see that we don’t have any missing data.\n\n# Creating a new variable called 'gunmurders' from total variable from murders data frame\n\nmurders &lt;- murders %&gt;% \n  mutate(gunmurders = total)\n# checking for the new created new variable\ncolnames(murders)\n\n[1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n[6] \"gunmurders\"\n\n\n\n\nB. Plotting\n\n#Using a histogram to visualize the gunmurders using the ggplot function from the ggplot2 package\nggplot(murders, aes(x = gunmurders)) +\n  geom_histogram(binwidth = 40, fill = \"blue\", color = \"black\") +\n  labs(title = \"Distribution of Gun Murders in the US in 2010\", x = \"Gun Murders\", y = \"Frequency\")\n\n\n\n\n\n\n\n\nLooking at the distribution, its skewed on the left\n\n# Now I will create a table with a state with the highest and lowest gun murders\n\n# Here I will use filter, select functions from the dylyr package\n\n# Finding the state with the maximum gun murders\nmax_state &lt;- murders %&gt;%\n  filter(gunmurders == max(gunmurders)) %&gt;%\n  select(state)\n\n# Finding the state with the minimum gun murders\nmin_state &lt;- murders %&gt;%\n  filter(gunmurders == min(gunmurders)) %&gt;%\n  select(state)\n\n# Create a summary table\nsummary_table &lt;- data.frame(\n  State = c(\"Maximum Gun Murders\", \"Minimum Gun Murders\"),\n  State_Name = c(max_state$state, min_state$state),\n  Gun_Murders = c(max(murders$gunmurders), min(murders$gunmurders))\n)\n\n# Print the summary table\nprint(summary_table)\n\n                State State_Name Gun_Murders\n1 Maximum Gun Murders California        1257\n2 Minimum Gun Murders    Vermont           2\n\n\nAccording to the table California and Vermont have the maximum and minimum gun murders respectively in 2010.\n\n#Then I will look at the gun murders by region  \n\n#I will visualise using a box plot show the Range of each region\nggplot(murders, aes(x = region, y = gunmurders, fill = region)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of Gun Murders by Region in USA 2010\",\n       x = \"Region\", y = \"Gun Murders\") +\n  scale_fill_manual(values = c(\"Northeast\" = \"blue\", \"South\" = \"green\", \"North Central\" = \"red\", \"West\" = \"purple\"))\n\n\n\n\n\n\n\n\n\n\nC.Simple Model Fits\n\n#Now I will do some simple regression of with each of the valiable \n\n# Fitting a linear regression model\nmodel1 &lt;- lm(gunmurders ~ population, data = murders)\nsummary(model1)\n\n\nCall:\nlm(formula = gunmurders ~ population, data = murders)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-112.889  -25.656   -3.687   25.505  217.780 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.713e+01  1.198e+01   -1.43    0.159    \npopulation   3.316e-05  1.315e-06   25.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 63.77 on 49 degrees of freedom\nMultiple R-squared:  0.9285,    Adjusted R-squared:  0.9271 \nF-statistic: 636.5 on 1 and 49 DF,  p-value: &lt; 2.2e-16\n\n\nOverall, it seems like the population variable is a statistically significant predictor of gun murders in the linear regression model with a p-value &lt; 0.01\n\n# Fitting a linear regression model\nmodel2 &lt;- lm(gunmurders ~ region, data = murders)\nsummary(model2)\n\n\nCall:\nlm(formula = gunmurders ~ region, data = murders)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-219.76 -135.38  -82.00   64.51 1110.00 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)           163.22      79.70   2.048   0.0462 *\nregionSouth            83.54      98.57   0.848   0.4010  \nregionNorth Central   -10.89     105.44  -0.103   0.9182  \nregionWest            -16.22     103.68  -0.156   0.8763  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 239.1 on 47 degrees of freedom\nMultiple R-squared:  0.03611,   Adjusted R-squared:  -0.02541 \nF-statistic: 0.587 on 3 and 47 DF,  p-value: 0.6265\n\n\nThis implies that the differences in gun murders between different regions (South, North Central, West) are not statistically significant based on the current model2.\n\n# Fitting a linear multiple regression model between region and total population \nmodel3 &lt;- lm(gunmurders ~ region + population, data = murders)\nsummary(model3)\n\n\nCall:\nlm(formula = gunmurders ~ region + population, data = murders)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-93.124 -42.386  -1.239  24.105 178.883 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -3.881e+01  2.114e+01  -1.836   0.0728 .  \nregionSouth          6.191e+01  2.444e+01   2.534   0.0148 *  \nregionNorth Central  7.818e+00  2.614e+01   0.299   0.7662    \nregionWest           3.897e+00  2.570e+01   0.152   0.8801    \npopulation           3.287e-05  1.225e-06  26.823   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.25 on 46 degrees of freedom\nMultiple R-squared:  0.9421,    Adjusted R-squared:  0.937 \nF-statistic:   187 on 4 and 46 DF,  p-value: &lt; 2.2e-16\n\n\nRegion South’ is statistically significant with p value &lt;0.01 The population variable remains highly significant, suggesting that population is a strong predictor of gun murders in this model."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting Exercise",
    "section": "",
    "text": "Data on a drug candidate called Mavoglurant is available in this GitHub repository. First, let’s process the raw data to make it ready for data exploration and analysis.\n\n#Load the required packages\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.3.2\n\n\n#BlackLivesMatter\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ infer        1.0.5     ✔ tidyr        1.3.0\n✔ modeldata    1.2.0     ✔ tune         1.1.2\n✔ parsnip      1.1.1     ✔ workflows    1.1.3\n✔ purrr        1.0.2     ✔ workflowsets 1.0.1\n✔ recipes      1.0.8     ✔ yardstick    1.2.0\n\n\nWarning: package 'dials' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.2\n\n\nWarning: package 'modeldata' was built under R version 4.3.2\n\n\nWarning: package 'parsnip' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'recipes' was built under R version 4.3.2\n\n\nWarning: package 'rsample' was built under R version 4.3.2\n\n\nWarning: package 'tune' was built under R version 4.3.2\n\n\nWarning: package 'workflows' was built under R version 4.3.2\n\n\nWarning: package 'workflowsets' was built under R version 4.3.2\n\n\nWarning: package 'yardstick' was built under R version 4.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ recipes::all_double()  masks gtsummary::all_double()\n✖ recipes::all_factor()  masks gtsummary::all_factor()\n✖ recipes::all_integer() masks gtsummary::all_integer()\n✖ recipes::all_logical() masks gtsummary::all_logical()\n✖ recipes::all_numeric() masks gtsummary::all_numeric()\n✖ purrr::discard()       masks scales::discard()\n✖ dplyr::filter()        masks stats::filter()\n✖ dplyr::lag()           masks stats::lag()\n✖ recipes::step()        masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.2\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(purrr)\n\n#Load the raw data\nmavoglurant &lt;- read.csv(\"mavoglurant.csv\")\n\n#Get an overview and summary of the raw data\nstr(mavoglurant)\n\n'data.frame':   2678 obs. of  17 variables:\n $ ID  : int  793 793 793 793 793 793 793 793 793 793 ...\n $ CMT : int  1 2 2 2 2 2 2 2 2 2 ...\n $ EVID: int  1 0 0 0 0 0 0 0 0 0 ...\n $ EVI2: int  1 0 0 0 0 0 0 0 0 0 ...\n $ MDV : int  1 0 0 0 0 0 0 0 0 0 ...\n $ DV  : num  0 491 605 556 310 237 147 101 72.4 52.6 ...\n $ LNDV: num  0 6.2 6.41 6.32 5.74 ...\n $ AMT : num  25 0 0 0 0 0 0 0 0 0 ...\n $ TIME: num  0 0.2 0.25 0.367 0.533 0.7 1.2 2.2 3.2 4.2 ...\n $ DOSE: num  25 25 25 25 25 25 25 25 25 25 ...\n $ OCC : int  1 1 1 1 1 1 1 1 1 1 ...\n $ RATE: int  75 0 0 0 0 0 0 0 0 0 ...\n $ AGE : int  42 42 42 42 42 42 42 42 42 42 ...\n $ SEX : int  1 1 1 1 1 1 1 1 1 1 ...\n $ RACE: int  2 2 2 2 2 2 2 2 2 2 ...\n $ WT  : num  94.3 94.3 94.3 94.3 94.3 94.3 94.3 94.3 94.3 94.3 ...\n $ HT  : num  1.77 1.77 1.77 1.77 1.77 ...\n\nsummary(mavoglurant)\n\n       ID             CMT             EVID              EVI2       \n Min.   :793.0   Min.   :1.000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:832.0   1st Qu.:2.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :860.0   Median :2.000   Median :0.00000   Median :0.0000  \n Mean   :858.8   Mean   :1.926   Mean   :0.07394   Mean   :0.1613  \n 3rd Qu.:888.0   3rd Qu.:2.000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :915.0   Max.   :2.000   Max.   :1.00000   Max.   :4.0000  \n      MDV                DV               LNDV            AMT        \n Min.   :0.00000   Min.   :   0.00   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:0.00000   1st Qu.:  23.52   1st Qu.:3.158   1st Qu.: 0.000  \n Median :0.00000   Median :  74.20   Median :4.306   Median : 0.000  \n Mean   :0.09373   Mean   : 179.93   Mean   :4.085   Mean   : 2.763  \n 3rd Qu.:0.00000   3rd Qu.: 283.00   3rd Qu.:5.645   3rd Qu.: 0.000  \n Max.   :1.00000   Max.   :1730.00   Max.   :7.456   Max.   :50.000  \n      TIME             DOSE            OCC             RATE       \n Min.   : 0.000   Min.   :25.00   Min.   :1.000   Min.   :  0.00  \n 1st Qu.: 0.583   1st Qu.:25.00   1st Qu.:1.000   1st Qu.:  0.00  \n Median : 2.250   Median :37.50   Median :1.000   Median :  0.00  \n Mean   : 5.851   Mean   :37.37   Mean   :1.378   Mean   : 16.55  \n 3rd Qu.: 6.363   3rd Qu.:50.00   3rd Qu.:2.000   3rd Qu.:  0.00  \n Max.   :48.217   Max.   :50.00   Max.   :2.000   Max.   :300.00  \n      AGE            SEX             RACE              WT        \n Min.   :18.0   Min.   :1.000   Min.   : 1.000   Min.   : 56.60  \n 1st Qu.:26.0   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.: 73.30  \n Median :31.0   Median :1.000   Median : 1.000   Median : 82.60  \n Mean   :32.9   Mean   :1.128   Mean   : 7.415   Mean   : 83.16  \n 3rd Qu.:40.0   3rd Qu.:1.000   3rd Qu.: 2.000   3rd Qu.: 90.60  \n Max.   :50.0   Max.   :2.000   Max.   :88.000   Max.   :115.30  \n       HT       \n Min.   :1.520  \n 1st Qu.:1.710  \n Median :1.780  \n Mean   :1.762  \n 3rd Qu.:1.820  \n Max.   :1.930  \n\n#Plot DV as a function of TIME, stratified by DOSE and using ID as a grouping factor\nggplot(mavoglurant, aes(x = TIME, y = DV, group = ID, color = factor(ID))) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ DOSE) +\n  labs(x = \"TIME\", y = \"DV\", title = \"DV vs. TIME, Stratified by DOSE and Grouped by ID\")\n\n\n\n\n\n\n\n#Keep only observations with OCC = 1\nmavoglurant2 &lt;- subset(mavoglurant, OCC == \"1\")\n\n#Use the dataset with OCC = 1 to create a subset dataset that removes TIME = 0 values for each individual\nNo_TIME_0 &lt;- subset(mavoglurant2, TIME != 0.000)\n\n#Use the dataset without TIME = 0  to create a new dataset that groups by ID to create a new variable Y that is the sum of the DV variable for each individual\nDV_sum &lt;- No_TIME_0 %&gt;% group_by(ID) %&gt;% summarize(Y = sum(DV))\n\n#Use the dataset with OCC = 1 to create a subset dataset that has only TIME = 0\nTIME_0 &lt;- subset(mavoglurant2, TIME == 0.000)\n\n#Combine the dataset without TIME = 0 with the dataset with only ID and Y\nmavoglurant3 &lt;- merge(TIME_0, DV_sum, by = \"ID\")\n\n#Convert RACE and SEX to factor variables in the combined dataset\nmavoglurant3$RACE &lt;- as.factor(mavoglurant3$RACE)\nmavoglurant3$SEX &lt;- as.factor(mavoglurant3$SEX)\n\n#Use the combined dataset and keep only the following variables: Y, DOSE, RATE, AGE, SEX, RACE, WT, HT\nmavoglurant4 &lt;- mavoglurant3[, c(\"Y\", \"DOSE\", \"RATE\", \"AGE\", \"SEX\", \"RACE\", \"WT\", \"HT\")]\n\n#Get an overview and summary of the processed data\nstr(mavoglurant4)\n\n'data.frame':   120 obs. of  8 variables:\n $ Y   : num  2691 2639 2150 1789 3126 ...\n $ DOSE: num  25 25 25 25 25 25 25 25 25 25 ...\n $ RATE: int  75 150 150 150 150 150 150 150 150 150 ...\n $ AGE : int  42 24 31 46 41 27 23 20 23 28 ...\n $ SEX : Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 2 1 1 1 1 1 ...\n $ RACE: Factor w/ 4 levels \"1\",\"2\",\"7\",\"88\": 2 2 1 1 2 2 1 4 2 1 ...\n $ WT  : num  94.3 80.4 71.8 77.4 64.3 ...\n $ HT  : num  1.77 1.76 1.81 1.65 1.56 ...\n\nsummary(mavoglurant4)\n\n       Y               DOSE            RATE            AGE        SEX    \n Min.   : 826.4   Min.   :25.00   Min.   : 75.0   Min.   :18.00   1:104  \n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:150.0   1st Qu.:26.00   2: 16  \n Median :2349.1   Median :37.50   Median :225.0   Median :31.00          \n Mean   :2445.4   Mean   :36.46   Mean   :218.1   Mean   :33.00          \n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:300.0   3rd Qu.:40.25          \n Max.   :5606.6   Max.   :50.00   Max.   :300.0   Max.   :50.00          \n RACE          WT               HT       \n 1 :74   Min.   : 56.60   Min.   :1.520  \n 2 :36   1st Qu.: 73.17   1st Qu.:1.700  \n 7 : 2   Median : 82.10   Median :1.770  \n 88: 8   Mean   : 82.55   Mean   :1.759  \n         3rd Qu.: 90.10   3rd Qu.:1.813  \n         Max.   :115.30   Max.   :1.930  \n\n#Save the processed data as rds file\nsaveRDS(mavoglurant4, file = \"C:/Users/ranni/Desktop/MADA/rannitewfik-MADA-portfolio/fitting-exercise/mavoglurant_processed.rds\")\n\nThe processed dataset has 120 observations and 8 variables: Y, DOSE, RATE, AGE, SEX, RACE, WT, and HT."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#part-1---data-processing",
    "href": "fitting-exercise/fitting-exercise.html#part-1---data-processing",
    "title": "Fitting Exercise",
    "section": "",
    "text": "Data on a drug candidate called Mavoglurant is available in this GitHub repository. First, let’s process the raw data to make it ready for data exploration and analysis.\n\n#Load the required packages\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.3.2\n\n\n#BlackLivesMatter\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ infer        1.0.5     ✔ tidyr        1.3.0\n✔ modeldata    1.2.0     ✔ tune         1.1.2\n✔ parsnip      1.1.1     ✔ workflows    1.1.3\n✔ purrr        1.0.2     ✔ workflowsets 1.0.1\n✔ recipes      1.0.8     ✔ yardstick    1.2.0\n\n\nWarning: package 'dials' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.2\n\n\nWarning: package 'modeldata' was built under R version 4.3.2\n\n\nWarning: package 'parsnip' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'recipes' was built under R version 4.3.2\n\n\nWarning: package 'rsample' was built under R version 4.3.2\n\n\nWarning: package 'tune' was built under R version 4.3.2\n\n\nWarning: package 'workflows' was built under R version 4.3.2\n\n\nWarning: package 'workflowsets' was built under R version 4.3.2\n\n\nWarning: package 'yardstick' was built under R version 4.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ recipes::all_double()  masks gtsummary::all_double()\n✖ recipes::all_factor()  masks gtsummary::all_factor()\n✖ recipes::all_integer() masks gtsummary::all_integer()\n✖ recipes::all_logical() masks gtsummary::all_logical()\n✖ recipes::all_numeric() masks gtsummary::all_numeric()\n✖ purrr::discard()       masks scales::discard()\n✖ dplyr::filter()        masks stats::filter()\n✖ dplyr::lag()           masks stats::lag()\n✖ recipes::step()        masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.2\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(purrr)\n\n#Load the raw data\nmavoglurant &lt;- read.csv(\"mavoglurant.csv\")\n\n#Get an overview and summary of the raw data\nstr(mavoglurant)\n\n'data.frame':   2678 obs. of  17 variables:\n $ ID  : int  793 793 793 793 793 793 793 793 793 793 ...\n $ CMT : int  1 2 2 2 2 2 2 2 2 2 ...\n $ EVID: int  1 0 0 0 0 0 0 0 0 0 ...\n $ EVI2: int  1 0 0 0 0 0 0 0 0 0 ...\n $ MDV : int  1 0 0 0 0 0 0 0 0 0 ...\n $ DV  : num  0 491 605 556 310 237 147 101 72.4 52.6 ...\n $ LNDV: num  0 6.2 6.41 6.32 5.74 ...\n $ AMT : num  25 0 0 0 0 0 0 0 0 0 ...\n $ TIME: num  0 0.2 0.25 0.367 0.533 0.7 1.2 2.2 3.2 4.2 ...\n $ DOSE: num  25 25 25 25 25 25 25 25 25 25 ...\n $ OCC : int  1 1 1 1 1 1 1 1 1 1 ...\n $ RATE: int  75 0 0 0 0 0 0 0 0 0 ...\n $ AGE : int  42 42 42 42 42 42 42 42 42 42 ...\n $ SEX : int  1 1 1 1 1 1 1 1 1 1 ...\n $ RACE: int  2 2 2 2 2 2 2 2 2 2 ...\n $ WT  : num  94.3 94.3 94.3 94.3 94.3 94.3 94.3 94.3 94.3 94.3 ...\n $ HT  : num  1.77 1.77 1.77 1.77 1.77 ...\n\nsummary(mavoglurant)\n\n       ID             CMT             EVID              EVI2       \n Min.   :793.0   Min.   :1.000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:832.0   1st Qu.:2.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :860.0   Median :2.000   Median :0.00000   Median :0.0000  \n Mean   :858.8   Mean   :1.926   Mean   :0.07394   Mean   :0.1613  \n 3rd Qu.:888.0   3rd Qu.:2.000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :915.0   Max.   :2.000   Max.   :1.00000   Max.   :4.0000  \n      MDV                DV               LNDV            AMT        \n Min.   :0.00000   Min.   :   0.00   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:0.00000   1st Qu.:  23.52   1st Qu.:3.158   1st Qu.: 0.000  \n Median :0.00000   Median :  74.20   Median :4.306   Median : 0.000  \n Mean   :0.09373   Mean   : 179.93   Mean   :4.085   Mean   : 2.763  \n 3rd Qu.:0.00000   3rd Qu.: 283.00   3rd Qu.:5.645   3rd Qu.: 0.000  \n Max.   :1.00000   Max.   :1730.00   Max.   :7.456   Max.   :50.000  \n      TIME             DOSE            OCC             RATE       \n Min.   : 0.000   Min.   :25.00   Min.   :1.000   Min.   :  0.00  \n 1st Qu.: 0.583   1st Qu.:25.00   1st Qu.:1.000   1st Qu.:  0.00  \n Median : 2.250   Median :37.50   Median :1.000   Median :  0.00  \n Mean   : 5.851   Mean   :37.37   Mean   :1.378   Mean   : 16.55  \n 3rd Qu.: 6.363   3rd Qu.:50.00   3rd Qu.:2.000   3rd Qu.:  0.00  \n Max.   :48.217   Max.   :50.00   Max.   :2.000   Max.   :300.00  \n      AGE            SEX             RACE              WT        \n Min.   :18.0   Min.   :1.000   Min.   : 1.000   Min.   : 56.60  \n 1st Qu.:26.0   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.: 73.30  \n Median :31.0   Median :1.000   Median : 1.000   Median : 82.60  \n Mean   :32.9   Mean   :1.128   Mean   : 7.415   Mean   : 83.16  \n 3rd Qu.:40.0   3rd Qu.:1.000   3rd Qu.: 2.000   3rd Qu.: 90.60  \n Max.   :50.0   Max.   :2.000   Max.   :88.000   Max.   :115.30  \n       HT       \n Min.   :1.520  \n 1st Qu.:1.710  \n Median :1.780  \n Mean   :1.762  \n 3rd Qu.:1.820  \n Max.   :1.930  \n\n#Plot DV as a function of TIME, stratified by DOSE and using ID as a grouping factor\nggplot(mavoglurant, aes(x = TIME, y = DV, group = ID, color = factor(ID))) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ DOSE) +\n  labs(x = \"TIME\", y = \"DV\", title = \"DV vs. TIME, Stratified by DOSE and Grouped by ID\")\n\n\n\n\n\n\n\n#Keep only observations with OCC = 1\nmavoglurant2 &lt;- subset(mavoglurant, OCC == \"1\")\n\n#Use the dataset with OCC = 1 to create a subset dataset that removes TIME = 0 values for each individual\nNo_TIME_0 &lt;- subset(mavoglurant2, TIME != 0.000)\n\n#Use the dataset without TIME = 0  to create a new dataset that groups by ID to create a new variable Y that is the sum of the DV variable for each individual\nDV_sum &lt;- No_TIME_0 %&gt;% group_by(ID) %&gt;% summarize(Y = sum(DV))\n\n#Use the dataset with OCC = 1 to create a subset dataset that has only TIME = 0\nTIME_0 &lt;- subset(mavoglurant2, TIME == 0.000)\n\n#Combine the dataset without TIME = 0 with the dataset with only ID and Y\nmavoglurant3 &lt;- merge(TIME_0, DV_sum, by = \"ID\")\n\n#Convert RACE and SEX to factor variables in the combined dataset\nmavoglurant3$RACE &lt;- as.factor(mavoglurant3$RACE)\nmavoglurant3$SEX &lt;- as.factor(mavoglurant3$SEX)\n\n#Use the combined dataset and keep only the following variables: Y, DOSE, RATE, AGE, SEX, RACE, WT, HT\nmavoglurant4 &lt;- mavoglurant3[, c(\"Y\", \"DOSE\", \"RATE\", \"AGE\", \"SEX\", \"RACE\", \"WT\", \"HT\")]\n\n#Get an overview and summary of the processed data\nstr(mavoglurant4)\n\n'data.frame':   120 obs. of  8 variables:\n $ Y   : num  2691 2639 2150 1789 3126 ...\n $ DOSE: num  25 25 25 25 25 25 25 25 25 25 ...\n $ RATE: int  75 150 150 150 150 150 150 150 150 150 ...\n $ AGE : int  42 24 31 46 41 27 23 20 23 28 ...\n $ SEX : Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 2 1 1 1 1 1 ...\n $ RACE: Factor w/ 4 levels \"1\",\"2\",\"7\",\"88\": 2 2 1 1 2 2 1 4 2 1 ...\n $ WT  : num  94.3 80.4 71.8 77.4 64.3 ...\n $ HT  : num  1.77 1.76 1.81 1.65 1.56 ...\n\nsummary(mavoglurant4)\n\n       Y               DOSE            RATE            AGE        SEX    \n Min.   : 826.4   Min.   :25.00   Min.   : 75.0   Min.   :18.00   1:104  \n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:150.0   1st Qu.:26.00   2: 16  \n Median :2349.1   Median :37.50   Median :225.0   Median :31.00          \n Mean   :2445.4   Mean   :36.46   Mean   :218.1   Mean   :33.00          \n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:300.0   3rd Qu.:40.25          \n Max.   :5606.6   Max.   :50.00   Max.   :300.0   Max.   :50.00          \n RACE          WT               HT       \n 1 :74   Min.   : 56.60   Min.   :1.520  \n 2 :36   1st Qu.: 73.17   1st Qu.:1.700  \n 7 : 2   Median : 82.10   Median :1.770  \n 88: 8   Mean   : 82.55   Mean   :1.759  \n         3rd Qu.: 90.10   3rd Qu.:1.813  \n         Max.   :115.30   Max.   :1.930  \n\n#Save the processed data as rds file\nsaveRDS(mavoglurant4, file = \"C:/Users/ranni/Desktop/MADA/rannitewfik-MADA-portfolio/fitting-exercise/mavoglurant_processed.rds\")\n\nThe processed dataset has 120 observations and 8 variables: Y, DOSE, RATE, AGE, SEX, RACE, WT, and HT."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#part-2---data-exploration",
    "href": "fitting-exercise/fitting-exercise.html#part-2---data-exploration",
    "title": "Fitting Exercise",
    "section": "Part 2 - Data Exploration",
    "text": "Part 2 - Data Exploration\nNow let’s explore each variable in the processed dataset, starting with the outcome variable, Y.\n\n#Attach the processed data\nattach(mavoglurant4)\n\n#Summary statistics and histogram for the outcome variable Y\nmavoglurant4$Y &lt;- as.numeric(mavoglurant4$Y)\nsummary(Y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  826.4  1700.5  2349.1  2445.4  3050.2  5606.6 \n\nggplot(mavoglurant4, aes(x = Y)) +\n  geom_histogram() +\n  labs(x = \"Total Drug, Y\", y = \"Frequency\", title = \"Histogram of Total Drug, Y\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean and median for the outcome variable (total drug, Y) are the same (854). The histogram of Y shows a relatively normal distribution for most of the data (Y &lt; 4000). There are a few observations for Y &gt; 4000.\nNow let’s explore the variable DOSE.\n\n#Summary tables for the variable DOSE\ntable(DOSE, useNA = \"always\")\n\nDOSE\n  25 37.5   50 &lt;NA&gt; \n  59   12   49    0 \n\nprop.table(table(DOSE))\n\nDOSE\n       25      37.5        50 \n0.4916667 0.1000000 0.4083333 \n\n\nFor the variable DOSE, 49.2% of the observations are “25”, 10.0% are “37.5”, and 40.8% are “50”. There are no missing observations for DOSE.\nNow let’s explore the variable RATE.\n\n#Summary tables for the variable RATE\ntable(RATE, useNA = \"always\")\n\nRATE\n  75  150  225  300 &lt;NA&gt; \n   1   58   12   49    0 \n\nprop.table(table(RATE))\n\nRATE\n         75         150         225         300 \n0.008333333 0.483333333 0.100000000 0.408333333 \n\n\nFor the variable RATE, 0.8% of the observations are “75”, 48.3% are “150”, 10.0% are “225”, and 40.8% are “300”. There are no missing observations for RATE.\nNow let’s explore the variable AGE.\n\n#Summary statistics and histogram for the variable AGE\nmavoglurant4$AGE &lt;- as.numeric(mavoglurant4$AGE)\nsummary(AGE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   26.00   31.00   33.00   40.25   50.00 \n\nggplot(mavoglurant4, aes(x = AGE)) +\n  geom_histogram() +\n  labs(x = \"Age\", y = \"Frequency\", title = \"Histogram of Age\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean (33) and median (31) for the variable AGE are similar. The histogram of AGE shows a relatively normal distribution for most of the data (AGE &lt; 4000). There are a few observations for AGE &gt; 4000.\nNow let’s explore the variable SEX.\n\n#Summary tables for the variable SEX\ntable(SEX, useNA = \"always\")\n\nSEX\n   1    2 &lt;NA&gt; \n 104   16    0 \n\nprop.table(table(SEX))\n\nSEX\n        1         2 \n0.8666667 0.1333333 \n\n\nFor the variable SEX, 86.7% of the observations are “1”, and 13.3% are “2”. There are no missing observations for SEX.\nNow let’s explore the variable RACE\n\n#Summary tables for the variable RACE\ntable(RACE, useNA = \"always\")\n\nRACE\n   1    2    7   88 &lt;NA&gt; \n  74   36    2    8    0 \n\nprop.table(table(RACE))\n\nRACE\n         1          2          7         88 \n0.61666667 0.30000000 0.01666667 0.06666667 \n\n\nFor the variable RACE, 61.7% of the observations are “1”, 30.0% are “2”, 1.7% are “7”, and 6.7% are “88”. There are no missing observations for RACE, although “88” may represent missing/unknown.\nNow let’s explore the variable WT.\n\n#Summary statistics and histogram for the variable WT\nmavoglurant4$WT &lt;- as.numeric(mavoglurant4$WT)\nsummary(WT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  56.60   73.17   82.10   82.55   90.10  115.30 \n\nggplot(mavoglurant4, aes(x = WT)) +\n  geom_histogram() +\n  labs(x = \"Weight\", y = \"Frequency\", title = \"Histogram of Weight\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean (83) and median (82) for the variable WT are similar. The histogram of WT shows a relatively normal distribution for the entire data.\nNow let’s explore the variable HT.\n\n#Summary statistics and histogram for the variable HT\nmavoglurant4$HT &lt;- as.numeric(mavoglurant4$HT)\nsummary(HT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.520   1.700   1.770   1.759   1.813   1.930 \n\nggplot(mavoglurant4, aes(x = HT)) +\n  geom_histogram() +\n  labs(x = \"Height\", y = \"Frequency\", title = \"Histogram of Height\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean and median for the variable HT are the same (1.8). The histogram of HT shows a left-skewing distribution.\nHere is a summary table of the outcome and predictors, stratified by dose.\n\n#Create a summary table of the variables\nmavoglurant4 %&gt;% tbl_summary(by = DOSE, statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;%\n  modify_caption(\"Summary Table of Mavoglurant Data, Stratified by Dose\")\n\n\n\n\n\nSummary Table of Mavoglurant Data, Stratified by Dose\n\n\n\n\n\n\n\n\nCharacteristic\n25, N = 591\n37.5, N = 121\n50, N = 491\n\n\n\n\nY\n1,783 (601)\n2,464 (488)\n3,239 (787)\n\n\nRATE\n\n\n\n\n\n\n\n\n    75\n1 (1.7%)\n0 (0%)\n0 (0%)\n\n\n    150\n58 (98%)\n0 (0%)\n0 (0%)\n\n\n    225\n0 (0%)\n12 (100%)\n0 (0%)\n\n\n    300\n0 (0%)\n0 (0%)\n49 (100%)\n\n\nAGE\n32 (9)\n36 (10)\n33 (9)\n\n\nSEX\n\n\n\n\n\n\n\n\n    1\n49 (83%)\n10 (83%)\n45 (92%)\n\n\n    2\n10 (17%)\n2 (17%)\n4 (8.2%)\n\n\nRACE\n\n\n\n\n\n\n\n\n    1\n32 (54%)\n10 (83%)\n32 (65%)\n\n\n    2\n21 (36%)\n2 (17%)\n13 (27%)\n\n\n    7\n1 (1.7%)\n0 (0%)\n1 (2.0%)\n\n\n    88\n5 (8.5%)\n0 (0%)\n3 (6.1%)\n\n\nWT\n81 (12)\n81 (11)\n84 (13)\n\n\nHT\n1.76 (0.09)\n1.75 (0.10)\n1.76 (0.08)\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n\n\n\nFinally, let’s create some scatterplots between the main outcome (total drug, Y) and the continuous predictors, as well as a correlation table.\n\n#Create scatterplots between Y and the continuous predictors\npairs(cbind(Y, AGE, WT, HT))\n\n\n\n\n\n\n\n#Create a correlation matrix  of the continuous variables\ncor(cbind(Y, AGE, WT, HT))\n\n              Y         AGE         WT         HT\nY    1.00000000  0.01256372 -0.2128719 -0.1583297\nAGE  0.01256372  1.00000000  0.1196740 -0.3518581\nWT  -0.21287194  0.11967399  1.0000000  0.5997505\nHT  -0.15832972 -0.35185806  0.5997505  1.0000000\n\n#Create a boxplot of DOSE and Y\nggplot(mavoglurant4, aes(x = factor(DOSE), y = Y)) +\n  geom_boxplot() +\n  labs(x = \"Dose\", y = \"Drug Total\", title = \"Boxplot of Dose and Total Drug\")\n\n\n\n\n\n\n\n#Create a boxplot of RATE and Y\nggplot(mavoglurant4, aes(x = factor(RATE), y = Y)) +\n  geom_boxplot() +\n  labs(x = \"Rate\", y = \"Drug Total\", title = \"Boxplot of Rate and Total Drug\")\n\n\n\n\n\n\n\n#Create a boxplot of SEX and Y\nggplot(mavoglurant4, aes(x = factor(SEX), y = Y)) +\n  geom_boxplot() +\n  labs(x = \"Sex\", y = \"Drug Total\", title = \"Boxplot of Sex and Total Drug\")\n\n\n\n\n\n\n\n#Create a boxplot of RACE and Y\nggplot(mavoglurant4, aes(x = factor(RACE), y = Y)) +\n  geom_boxplot() +\n  labs(x = \"Race\", y = \"Drug Total\", title = \"Boxplot of Race and Total Drug\")\n\n\n\n\n\n\n\n\nBased on the scatterplots and the correlation matrix, there doesn’t seem to be a strong correlation between the outcome (total drug, Y) and any of the continuous predictors. The boxplots of total drug, Y, and the categorical variables shows a relatively normal distribution."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#part-3---model-fitting",
    "href": "fitting-exercise/fitting-exercise.html#part-3---model-fitting",
    "title": "Fitting Exercise",
    "section": "Part 3 - Model Fitting",
    "text": "Part 3 - Model Fitting\nFinally, let’s do some model fitting. First, let’s fit a linear model to the outcome, Y, using DOSE as the predictor.\n\n#Fit a linear model to the outcome Y using DOSE as the predictor\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinear_fit1 &lt;- linear_model %&gt;% fit(Y ~ DOSE, data = mavoglurant4)\ntidy(linear_fit1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    323.     199.        1.62 1.07e- 1\n2 DOSE            58.2      5.19     11.2  2.69e-20\n\n#Compute RMSE and R-squared\nmetrics1 &lt;- linear_fit1 %&gt;% \n  predict(mavoglurant4) %&gt;% \n  bind_cols(mavoglurant4) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\n#Print RMSE and R-squared\nprint(metrics1)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n3 mae     standard     517.   \n\n\nIn Model 1, the variable DOSE is significantly associated with the outcome Y (p-value &lt; 0.001). The total drug (Y) increases by 58.21 units for every unit change in DOSE. R-squared = 0.5156 indicates that 51.56% of the variability in total drug (Y) is explained by the model. RMSE = 666.46 represents the average distance between the predicted values from the model and the actual values in the dataset.\nNow let’s fit a linear model to the outcome, Y, using all other variables as the predictors.\n\n#Fit a linear model to the outcome Y using all other variables as the predictors\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinear_fit2 &lt;- linear_model %&gt;% fit(Y ~ ., data = mavoglurant4)\ntidy(linear_fit2)\n\n# A tibble: 10 × 5\n   term        estimate std.error statistic  p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)  3400.     1819.       1.87  0.0643  \n 2 DOSE          147.       50.9      2.89  0.00467 \n 3 RATE          -14.4       8.36    -1.72  0.0888  \n 4 AGE             1.78      7.80     0.228 0.820   \n 5 SEX2         -339.      215.      -1.57  0.119   \n 6 RACE2         125.      129.       0.972 0.333   \n 7 RACE7        -398.      444.      -0.897 0.372   \n 8 RACE88        -69.1     243.      -0.285 0.777   \n 9 WT            -23.7       6.35    -3.73  0.000302\n10 HT           -717.     1095.      -0.655 0.514   \n\n#Compute RMSE and R-squared\nmetrics2 &lt;- linear_fit2 %&gt;% \n  predict(mavoglurant4) %&gt;% \n  bind_cols(mavoglurant4) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\n#Print RMSE and R-squared\nprint(metrics2)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     583.   \n2 rsq     standard       0.629\n3 mae     standard     435.   \n\n\nIn Model 2, the variable DOSE is significantly associated with the outcome Y (p-value = 0.005) when the other predictors are included in the model. Adjusting for the other variables, the total drug (Y) increases by 146.87 units for every unit change in DOSE. R-squared = 0.6292 indicates that 62.92% of the variability in total drug (Y) is explained by the model. RMSE = 583.09 represents the average distance between the predicted values from the model and the actual values in the dataset.\nBoth Model 1 and Model 2, DOSE is significantly associated with Y, which makes sense because we would expect an increase in total drug (Y) when increasing dose. The model with all predictors seems to be better than the model with just DOSE.\nNext let’s fit a logistic model to the outcome, SEX, using DOSE as the predictor\n\n#Fit a logistic model to the outcome SEX using DOSE as the predictor\nlogistic_model &lt;- logistic_reg() %&gt;% set_engine(\"glm\")\nlogistic_fit1 &lt;- logistic_model %&gt;% fit(SEX ~ DOSE, data = mavoglurant4)\ntidy(logistic_fit1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -0.765     0.854     -0.896   0.370\n2 DOSE         -0.0318    0.0243    -1.31    0.192\n\n#Compute accuracy\naccuracy1 &lt;- logistic_fit1 %&gt;% \n  predict(mavoglurant4) %&gt;% \n  bind_cols(mavoglurant4) %&gt;% \n  metrics(truth = SEX, estimate = .pred_class) %&gt;% \n  filter(.metric == \"accuracy\") \n\n#Compute ROC-AUC\nauc1 &lt;-  logistic_fit1 %&gt;%\n  predict(mavoglurant4, type = \"prob\") %&gt;%\n  bind_cols(mavoglurant4) %&gt;%\n  roc_auc(truth = SEX, .pred_1)\n\n#Print accuracy and ROC-AUC\nprint(accuracy1)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.867\n\nprint(auc1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.592\n\n\nIn Model 3, the variable DOSE is not significantly associated with the outcome SEX (p-value = 0.19). The odds of the outcome for a dose is 0.97 times the odds of the outcome for a dose one unit less. The accuracy is 0.87, and the ROC-AUC is 0.59.\nFinally, let’s fit a logistic model to the outcome, SEX, using all other variables as the predictors.\n\n#Fit a logistic model to the outcome SEX using all other variables as the predictors\nlogistic_model &lt;- logistic_reg() %&gt;% set_engine(\"glm\")\nlogistic_fit2 &lt;- logistic_model %&gt;% fit(SEX ~ ., data = mavoglurant4)\ntidy(logistic_fit2)\n\n# A tibble: 10 × 5\n   term         estimate  std.error statistic  p.value\n   &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)  60.3      18.1        3.34    0.000839\n 2 Y            -0.00104   0.000964  -1.08    0.282   \n 3 DOSE         -0.849   192.        -0.00442 0.996   \n 4 RATE          0.136    32.0        0.00426 0.997   \n 5 AGE           0.0834    0.0607     1.37    0.169   \n 6 RACE2        -1.92      1.38      -1.40    0.162   \n 7 RACE7         0.119     3.84       0.0309  0.975   \n 8 RACE88       -1.50      2.19      -0.683   0.495   \n 9 WT           -0.0627    0.0795    -0.788   0.431   \n10 HT          -33.2      11.1       -2.99    0.00276 \n\n#Compute accuracy\naccuracy2 &lt;- logistic_fit2 %&gt;% \n  predict(mavoglurant4) %&gt;% \n  bind_cols(mavoglurant4) %&gt;% \n  metrics(truth = SEX, estimate = .pred_class) %&gt;% \n  filter(.metric %in% c(\"accuracy\"))\n\n#Compute ROC-AUC\nauc2 &lt;-  logistic_fit2 %&gt;%\n  predict(mavoglurant4, type = \"prob\") %&gt;%\n  bind_cols(mavoglurant4) %&gt;%\n  roc_auc(truth = SEX, .pred_1)\n\n#Print accuracy and ROC-AUC\nprint(accuracy2)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.942\n\nprint(auc2)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.980\n\n\nIn Model 4, the variable DOSE is not significantly associated with the outcome SEX (p-value = 0.996) when the other predictors are included in the model. Adjusting for the other variables, the odds of the outcome for a dose is 0.43 times the odds of the outcome for a dose one unit less. The accuracy is 0.94, and the ROC-AUC is 0.98.\nBoth Model 3 and Model 4, DOSE is not significantly associated with SEX, which makes sense because we would not necessarily expect a relationship between those two variables. The model with all predictors is more accurate than the model with just DOSE."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#part-4---model-improvement",
    "href": "fitting-exercise/fitting-exercise.html#part-4---model-improvement",
    "title": "Fitting Exercise",
    "section": "Part 4 - Model Improvement",
    "text": "Part 4 - Model Improvement\nLet’s do some more changes to the data. First, let’s remove the RATE and RACE variables since some values are coded “7” or “88”, which most likely indicates missing values. Then let’s set a seed and split the data randomly into a 75% train and 25% test set.\n\n#Create a subset dataset without RACE\nmavoglurant5 &lt;- mavoglurant4[, c(\"Y\", \"DOSE\", \"AGE\", \"SEX\", \"WT\", \"HT\")]\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Split the data randomly into 75% train and 25% test set\ndata_split &lt;- initial_split(mavoglurant5, prop = 3/4)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\nNow let’s do some model fitting with the training data. First, let’s fit a linear model to the outcome, Y, using DOSE as the predictor.\n\n#Fit a linear model to the outcome Y using DOSE as the predictor\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinear_fit1 &lt;- linear_model %&gt;% fit(Y ~ DOSE, data = train_data)\ntidy(linear_fit1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    535.     244.        2.19 3.08e- 2\n2 DOSE            53.4      6.29      8.50 4.41e-13\n\n#Compute RMSE and R-squared\nmetrics1 &lt;- linear_fit1 %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\n#Print RMSE and R-squared\nprint(metrics1)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     703.   \n2 rsq     standard       0.451\n3 mae     standard     546.   \n\n\nIn Model 1, the variable DOSE is significantly associated with the outcome Y (p-value &lt; 0.001). The total drug (Y) increases by 53.42 units for every unit change in DOSE. R-squared = 0.4508 indicates that 45.08% of the variability in total drug (Y) is explained by the model. RMSE = 702.81 represents the average distance between the predicted values from the model and the actual values in the dataset.\nNow let’s fit a linear model to the outcome, Y, using all other variables as the predictors.\n\n#Fit a linear model to the outcome Y using all other variables as the predictors\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinear_fit2 &lt;- linear_model %&gt;% fit(Y ~ ., data = train_data)\ntidy(linear_fit2)\n\n# A tibble: 6 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4397.      2170.      2.03   4.59e- 2\n2 DOSE           55.3        5.83    9.49   6.09e-15\n3 AGE            -0.417      9.50   -0.0439 9.65e- 1\n4 SEX2         -569.       285.     -1.99   4.95e- 2\n5 WT            -22.6        7.65   -2.96   4.00e- 3\n6 HT          -1130.      1358.     -0.832  4.08e- 1\n\n#Compute RMSE and R-squared\nmetrics2 &lt;- linear_fit2 %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\n#Print RMSE and R-squared\nprint(metrics2)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     627.   \n2 rsq     standard       0.562\n3 mae     standard     486.   \n\n\nIn Model 2, the variable DOSE is significantly associated with the outcome Y (p-value &lt; 0.001) when the other predictors are included in the model. Adjusting for the other variables, the total drug (Y) increases by 55.34 units for every unit change in DOSE. R-squared = 0.5623 indicates that 56.23% of the variability in total drug (Y) is explained by the model. RMSE = 627.44 represents the average distance between the predicted values from the model and the actual values in the dataset.\nBoth Model 1 and Model 2, DOSE is significantly associated with Y, which makes sense because we would expect an increase in total drug (Y) when increasing dose. The model with all predictors seems to be better than the model with just DOSE.\nNext let’s compare the RMSE values for the null model, Model 1, and Model 2.\n\n#Fit a null model to the outcome Y\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nnull_model &lt;- linear_model %&gt;% fit(Y ~ 1, data = train_data)\ntidy(null_model)\n\n# A tibble: 1 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    2509.      101.      25.0 5.83e-42\n\n#Compute RMSE for the null model\nmetrics_null &lt;- null_model %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\nWarning: A correlation computation is required, but `estimate` is constant and\nhas 0 standard deviation, resulting in a divide by 0 error. `NA` will be\nreturned.\n\n#Print RMSE for the null model, Model 1, and Model 2\nprint(metrics_null)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        948.\n2 rsq     standard         NA \n3 mae     standard        765.\n\nprint(metrics1)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     703.   \n2 rsq     standard       0.451\n3 mae     standard     546.   \n\nprint(metrics2)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     627.   \n2 rsq     standard       0.562\n3 mae     standard     486.   \n\n\nUsing the training data and comparing the RMSE values for the three models, the best-performing model is Model 2 with all predictors (RMSE = 627.44), and the worst-performing model is the null model (RMSE = 948.35).\nNow let’s do a 10-fold cross-validation to compute performance on unseen data. First, let’s define the training control as cross-validation and K = 10 folds. Then let’s compute the RMSE values for Model 1 and Model using 10-fold cross-validation.\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Define the training control as cross-validation and K = 10 folds\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n#Fit a linear model to the outcome Y using DOSE as the predictor\nmodel1 &lt;- train(Y ~ DOSE, data = train_data, method = \"lm\", trCrontol = train_control)\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\n#Fit a linear model to the outcome Y using all other variables as the predictors\nmodel2 &lt;- train(Y ~ ., data = train_data, method = \"lm\", trCrontrol = train_control)\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\n#Print RMSE for Model 1 and Model 2\nprint(model1)\n\nLinear Regression \n\n90 samples\n 1 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 90, 90, 90, 90, 90, 90, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  691.7967  0.4971554  550.4468\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\nprint(model2)\n\nLinear Regression \n\n90 samples\n 5 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 90, 90, 90, 90, 90, 90, ... \nResampling results:\n\n  RMSE      Rsquared  MAE     \n  696.2251  0.541752  544.7691\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nUsing 10-fold cross-validation on the training data and comparing the RMSE values for the two models, Model 1 with just DOSE (RMSE = 691.80) performs slightly better than Model 2 with all predictors (RMSE = 696.23). In the previous analysis without cross-validation, Model 2 (RMSE = 627.44) performed better than Model 1 (RMSE = 702.81).\nFinally, let’s re-create the cross-validation folds and re-fit the linear models using a different value for the random seed.\n\n#Set a seed\nrngseed = 5678\nset.seed(rngseed)\n\n#Split the data randomly into 75% train and 25% test set\ndata_split &lt;- initial_split(mavoglurant5, prop = 3/4)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n#Fit a linear model to the outcome Y using DOSE as the predictor\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinear_fit1 &lt;- linear_model %&gt;% fit(Y ~ DOSE, data = train_data)\ntidy(linear_fit1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    395.     238.        1.66 1.01e- 1\n2 DOSE            56.2      6.23      9.03 3.56e-14\n\n#Fit a linear model to the outcome Y using all other variables as the predictors\nlinear_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinear_fit2 &lt;- linear_model %&gt;% fit(Y ~ ., data = train_data)\ntidy(linear_fit2)\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  3526.     2032.       1.74  8.64e- 2\n2 DOSE           59.7       5.77    10.3   1.26e-16\n3 AGE            -2.40      9.21    -0.261 7.95e- 1\n4 SEX2         -287.      247.      -1.16  2.49e- 1\n5 WT            -23.2       7.09    -3.28  1.52e- 3\n6 HT           -696.     1238.      -0.562 5.76e- 1\n\n#Compute RMSE\nmetrics1 &lt;- linear_fit1 %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\nmetrics2 &lt;- linear_fit2 %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\n#Print RMSE\nprint(metrics1)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     694.   \n2 rsq     standard       0.481\n3 mae     standard     532.   \n\nprint(metrics2)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     616.   \n2 rsq     standard       0.591\n3 mae     standard     458.   \n\n#Define the training control as cross-validation and K = 10 folds\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n#Fit a linear model to the outcome Y using DOSE as the predictor\nmodel1 &lt;- train(Y ~ DOSE, data = train_data, method = \"lm\", trCrontol = train_control)\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontol' will be disregarded\n\n#Fit a linear model to the outcome Y using all other variables as the predictors\nmodel2 &lt;- train(Y ~ ., data = train_data, method = \"lm\", trCrontrol = train_control)\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\nWarning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n extra argument 'trCrontrol' will be disregarded\n\n#Print RMSE for Model 1 and Model 2\nprint(model1)\n\nLinear Regression \n\n90 samples\n 1 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 90, 90, 90, 90, 90, 90, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  707.7042  0.4926849  541.0253\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\nprint(model2)\n\nLinear Regression \n\n90 samples\n 5 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 90, 90, 90, 90, 90, 90, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  646.7547  0.5441775  502.9721\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nUsing a different random seed for the training data and comparing the RMSE values for the two models, Model 2 with all predictors (RMSE = 616.21) performs better than Model 1 with just DOSE (RMSE = 693.89). Using the same seed for the 10-fold cross-validation, Model 1 with just DOSE (RMSE = 693.70) performs slightly better than Model 2 with all predictors (RMSE = 697.21). The overall pattern between changes in the RMSE values for model fitting to the training data with and without cross-validation is the same when using different random seeds."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#this-section-was-added-by-emma-hardin-parker",
    "href": "fitting-exercise/fitting-exercise.html#this-section-was-added-by-emma-hardin-parker",
    "title": "Fitting Exercise",
    "section": "THIS SECTION WAS ADDED BY EMMA HARDIN-PARKER",
    "text": "THIS SECTION WAS ADDED BY EMMA HARDIN-PARKER\nPlotting the observed vs. predicted values for the three original model fits, including the null model.\n\n# Augmenting predicted & observed values\na_1 &lt;- augment(linear_fit1, new_data = train_data)\na_2 &lt;- augment(linear_fit2, new_data = train_data)\na_n &lt;- augment(null_model, new_data = train_data)\n\n# Combining into dataframe \ncombined_augmented &lt;- bind_rows(\n  a_1 %&gt;% mutate(Model = \"Linear Fit 1\"),\n  a_2 %&gt;% mutate(Model = \"Linear Fit 2\"),\n  a_n %&gt;% mutate(Model = \"Null Model\")\n)\n\n\n# Create the plot\nobs_pred_plot &lt;- ggplot(combined_augmented, aes(x = Y, y = .pred, color = Model, shape = Model)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n  xlim(0, 5000) +\n  ylim(0, 5000) +\n  labs(\n    title = \"Observed vs Predicted Values\",\n    x = \"Observed\",\n    y = \"Predicted\",\n    color = \"Model\",\n    shape = \"Model\"\n  ) +\n  theme_minimal()\n\n# Display the plot\nprint(obs_pred_plot)\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nUsing bootstrapping to sample the data, fit models to the data, and calculate uncertainty.\nSetting the seed, again.\n\nrngseed = 1234\nset.seed(rngseed)\n\nCreating 100 bootstraps using the bootstrap() function from the rsample package and creating a plot of the observed vs. predicted values along with Bootstrap Confidence Intervals.\n\n# Create 100 bootstraps using the training data\nbootstraps &lt;- bootstraps(data = train_data, times = 100)\n\n# Create empty vector to store predictions \npreds_bs &lt;- list()\n\n# Loop through each bootstrap sample\n\nfor (i in 1:length(bootstraps$splits)) {\n  # Get a single bootstrap sample\n  dat_sample &lt;- rsample::analysis(bootstraps$splits[[i]])\n  \n  # Fit the model to the bootstrap sample\n  linear_fit_bs &lt;- linear_model %&gt;% fit(Y ~ ., data = dat_sample)\n  \n  # Make predictions for the original training data\n  pred &lt;- predict(linear_fit_bs, new_data = train_data)\n  \n  # Store predictions in the list\n  preds_bs[[i]] &lt;- pred\n}\n\n# Convert list of predictions to a matrix\npred_matrix &lt;- do.call(cbind, preds_bs)\n\n# Compute mean and confidence intervals\npreds &lt;- apply(pred_matrix, 1, quantile, c(0.055, 0.5, 0.945)) %&gt;% t()\npreds &lt;- data.frame(preds)\n\n\n# Plotting the figure\nobs_pred_plot_2 &lt;- ggplot() +\n  geom_point(data = train_data, aes(x = Y, y = preds[, 2]), color = \"black\", size = 3) +  # Point estimate\n  geom_point(data = train_data, aes(x = Y, y = preds[, 1]), color = \"blue\", size = 2) +    # Lower bound\n  geom_point(data = train_data, aes(x = Y, y = preds[, 3]), color = \"red\", size = 2) +   # Upper bound\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +                           # 45-degree line\n  xlim(0, 5000) +\n  ylim(0, 5000) +\n  labs(\n    title = \"Observed vs Predicted Values with Bootstrap Confidence Intervals\",\n    x = \"Observed\",\n    y = \"Predicted\",\n    color = \"Estimates\"\n  ) +\n  theme_minimal() +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for Model 2\") \n\n\n# Show the plot\nprint(obs_pred_plot_2)\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n#Create a data frame with the observed and predicted values for Model 2\nmod2_df &lt;- lm(Y ~ ., data = train_data)\nmod2_df2 &lt;- fortify(mod2_df)\n\n#Add the median and confidence intervals to the same data frame as the observed and predicted values for Model 2\nmod2_df2$lower &lt;- preds$X5.5.\nmod2_df2$median &lt;- preds$X50.\nmod2_df2$upper &lt;- preds$X94.5.\n\n#Create a figure that plots observed values on the x-axis and predicted values on the y-axis\nobs_mod2 &lt;- ggplot(mod2_df2, aes(x = Y)) +\n  geom_point(aes(y = .fitted, color = \"Point Estimates\"), alpha = 0.7) +\n  geom_point(aes(y = lower, color = \"Lower Bounds\"), alpha = 0.7) +\n  geom_point(aes(y = median, color = \"Medians\"), alpha = 0.7) +\n  geom_point(aes(y = upper, color = \"Upper Bounds\"), alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  xlim(0, 5000) +\n  ylim(0, 5000) +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for Model 2\") +\n  theme_minimal()\nobs_mod2\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nI ultimately decided to create the plot in two different ways as a way to confirm that I was achieving the intended results. All of the lower bound points, median points, point estimates, and upper bound points tend to follow along the 45 degree line included in the plot. This indicates that the model is performing well, has high level of goodness of fit, and consistency in which the model is not systematically overestimating or underestimating the outcomes."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#final-evaluation",
    "href": "fitting-exercise/fitting-exercise.html#final-evaluation",
    "title": "Fitting Exercise",
    "section": "Final Evaluation",
    "text": "Final Evaluation\nLet’s do a final model evaluation, this time using the test data. First, let’s use the fit of Model 2 on the training data to make predictions for the test data. Then let’s make a plot that shows predicted versus observed values for both the training data and the test data.\n\n#Create a data frame with the observed and predicted values for the training data\nmod2_df &lt;- lm(Y ~ ., data = train_data)\nmod2_df2 &lt;- fortify(mod2_df)\n\n#Make predictions for the test data\npred_test &lt;- as.data.frame(predict(mod2_df, newdata = test_data))\n\n#Add the Y values to the same data frame as the test data predictions\nmod2_df2_test &lt;- fortify(lm(Y ~ ., data = test_data))\n\n#Combine the data frames with the observed and predicted values for the test data\ncombined_test &lt;- cbind(mod2_df2_test, pred_test)\n\n#Merge the data frames with the training data and test data predictions\nmerged &lt;- merge(mod2_df2, combined_test, by = \"Y\", all = TRUE)\n\n#Rename the test data predictions variable\nmerged &lt;- merged %&gt;% rename(pred_test = `predict(mod2_df, newdata = test_data)`)\n\n#Create a figure that plots observed values on the x-axis and predicted values on the y-axis\nggplot(merged, aes(x = Y)) +\n  geom_point(aes(y = .fitted.x, color = \"Training Data\"), alpha = 0.7) +\n  geom_point(aes(y = pred_test, color = \"Test Data\"), alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  xlim(0, 5000) +\n  ylim(0, 5000) +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for the Training and Test Data\") +\n  theme_minimal()\n\nWarning: Removed 31 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 90 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe plot shows that the observed and predicted values for the test data are mixed in with the training data, which is what we want to see. If the test data points were systematically “off”, then it would indicate a problem, such as overfitting to the training data.\nFinally, let’s critique all models. Both Model 1 and Model 2 perform better than the null model based on the RMSE values. Model 1 with only DOSE as the predictor improves the results over the null model (i.e., smaller RMSE). For both Model 1 and Model 2, DOSE is significantly associated with Y, which makes sense because we would expect an increase in total drug (Y) when increasing dose. Because the inclusion of other predictors in the model helps to better explain the outcome pattern, we would not consider Model 1 usable for any real-life purpose. Additional analyses, including the bootstrap method and making predictions for the test data, indicate that Model 2 with all predictors seems to be better than Model 1 with just DOSE."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html",
    "href": "ml-models-exercise/ml-models-exercise.html",
    "title": "ML Models Exercise",
    "section": "",
    "text": "Data on a drug candidate called Mavoglurant is available in this GitHub repository. First, let’s further process the dataset from the “Fitting Exercise” to make it ready for model building.\n\n#Load the required packages\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n\n\nWarning: package 'dials' was built under R version 4.3.2\n\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.2\n\n\nWarning: package 'modeldata' was built under R version 4.3.2\n\n\nWarning: package 'parsnip' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'recipes' was built under R version 4.3.2\n\n\nWarning: package 'rsample' was built under R version 4.3.2\n\n\nWarning: package 'tune' was built under R version 4.3.2\n\n\nWarning: package 'workflows' was built under R version 4.3.2\n\n\nWarning: package 'workflowsets' was built under R version 4.3.2\n\n\nWarning: package 'yardstick' was built under R version 4.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(ggplot2)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(glmnet)\n\nWarning: package 'glmnet' was built under R version 4.3.3\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.3.2\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Load the data\nmavoglurant &lt;- readRDS(\"mavoglurant_processed.rds\")\n\n#Get an overview and summary of the data\nstr(mavoglurant)\n\n'data.frame':   120 obs. of  8 variables:\n $ Y   : num  2691 2639 2150 1789 3126 ...\n $ DOSE: num  25 25 25 25 25 25 25 25 25 25 ...\n $ RATE: int  75 150 150 150 150 150 150 150 150 150 ...\n $ AGE : int  42 24 31 46 41 27 23 20 23 28 ...\n $ SEX : Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 2 1 1 1 1 1 ...\n $ RACE: Factor w/ 4 levels \"1\",\"2\",\"7\",\"88\": 2 2 1 1 2 2 1 4 2 1 ...\n $ WT  : num  94.3 80.4 71.8 77.4 64.3 ...\n $ HT  : num  1.77 1.76 1.81 1.65 1.56 ...\n\nsummary(mavoglurant)\n\n       Y               DOSE            RATE            AGE        SEX    \n Min.   : 826.4   Min.   :25.00   Min.   : 75.0   Min.   :18.00   1:104  \n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:150.0   1st Qu.:26.00   2: 16  \n Median :2349.1   Median :37.50   Median :225.0   Median :31.00          \n Mean   :2445.4   Mean   :36.46   Mean   :218.1   Mean   :33.00          \n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:300.0   3rd Qu.:40.25          \n Max.   :5606.6   Max.   :50.00   Max.   :300.0   Max.   :50.00          \n RACE          WT               HT       \n 1 :74   Min.   : 56.60   Min.   :1.520  \n 2 :36   1st Qu.: 73.17   1st Qu.:1.700  \n 7 : 2   Median : 82.10   Median :1.770  \n 88: 8   Mean   : 82.55   Mean   :1.759  \n         3rd Qu.: 90.10   3rd Qu.:1.813  \n         Max.   :115.30   Max.   :1.930  \n\n#Convert the SEX variable to numeric\nmavoglurant$SEX &lt;- as.numeric(mavoglurant$SEX)\n\n#Drop the RATE variable\nmavoglurant &lt;- mavoglurant %&gt;% select(-RATE)\n\n#Combine the \"7\" and \"88\" categories in the RACE variable into a single category called \"3\"\nmavoglurant &lt;- mavoglurant %&gt;% mutate(RACE = ifelse(RACE %in% c(7, 88), 3, RACE))\n\nNext let’s make a pairwise correlation plot for the continuous variables.\n\n#Create a subset dataset with only continuous variables\ncontinuous &lt;- mavoglurant %&gt;% select(Y, AGE, WT, HT)\n\n#Create a correlation matrix\ncorrmatrix &lt;- cor(continuous)\n\n#Create a correlation plot\ncorrplot(corrmatrix, method = \"number\")\n\n\n\n\n\n\n\n\nThe correlation plot shows that none of the pairwise correlations is excessive (i.e., above an absolute value of 0.9). We shouldn’t have much of a problem with collinearity.\nNow let’s add a new variable BMI, computed from the HT and WT variables.\n\n#Create a new variable BMI from HT and WT\nmavoglurant$BMI &lt;- mavoglurant$WT / (mavoglurant$HT)^2\n\nWe’re done with data processing and exploration, and we’re ready to fit some models."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#part-1---process-explore-the-data",
    "href": "ml-models-exercise/ml-models-exercise.html#part-1---process-explore-the-data",
    "title": "ML Models Exercise",
    "section": "",
    "text": "Data on a drug candidate called Mavoglurant is available in this GitHub repository. First, let’s further process the dataset from the “Fitting Exercise” to make it ready for model building.\n\n#Load the required packages\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n\n\nWarning: package 'dials' was built under R version 4.3.2\n\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.2\n\n\nWarning: package 'modeldata' was built under R version 4.3.2\n\n\nWarning: package 'parsnip' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'recipes' was built under R version 4.3.2\n\n\nWarning: package 'rsample' was built under R version 4.3.2\n\n\nWarning: package 'tune' was built under R version 4.3.2\n\n\nWarning: package 'workflows' was built under R version 4.3.2\n\n\nWarning: package 'workflowsets' was built under R version 4.3.2\n\n\nWarning: package 'yardstick' was built under R version 4.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(ggplot2)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(glmnet)\n\nWarning: package 'glmnet' was built under R version 4.3.3\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.3.2\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Load the data\nmavoglurant &lt;- readRDS(\"mavoglurant_processed.rds\")\n\n#Get an overview and summary of the data\nstr(mavoglurant)\n\n'data.frame':   120 obs. of  8 variables:\n $ Y   : num  2691 2639 2150 1789 3126 ...\n $ DOSE: num  25 25 25 25 25 25 25 25 25 25 ...\n $ RATE: int  75 150 150 150 150 150 150 150 150 150 ...\n $ AGE : int  42 24 31 46 41 27 23 20 23 28 ...\n $ SEX : Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 2 1 1 1 1 1 ...\n $ RACE: Factor w/ 4 levels \"1\",\"2\",\"7\",\"88\": 2 2 1 1 2 2 1 4 2 1 ...\n $ WT  : num  94.3 80.4 71.8 77.4 64.3 ...\n $ HT  : num  1.77 1.76 1.81 1.65 1.56 ...\n\nsummary(mavoglurant)\n\n       Y               DOSE            RATE            AGE        SEX    \n Min.   : 826.4   Min.   :25.00   Min.   : 75.0   Min.   :18.00   1:104  \n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:150.0   1st Qu.:26.00   2: 16  \n Median :2349.1   Median :37.50   Median :225.0   Median :31.00          \n Mean   :2445.4   Mean   :36.46   Mean   :218.1   Mean   :33.00          \n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:300.0   3rd Qu.:40.25          \n Max.   :5606.6   Max.   :50.00   Max.   :300.0   Max.   :50.00          \n RACE          WT               HT       \n 1 :74   Min.   : 56.60   Min.   :1.520  \n 2 :36   1st Qu.: 73.17   1st Qu.:1.700  \n 7 : 2   Median : 82.10   Median :1.770  \n 88: 8   Mean   : 82.55   Mean   :1.759  \n         3rd Qu.: 90.10   3rd Qu.:1.813  \n         Max.   :115.30   Max.   :1.930  \n\n#Convert the SEX variable to numeric\nmavoglurant$SEX &lt;- as.numeric(mavoglurant$SEX)\n\n#Drop the RATE variable\nmavoglurant &lt;- mavoglurant %&gt;% select(-RATE)\n\n#Combine the \"7\" and \"88\" categories in the RACE variable into a single category called \"3\"\nmavoglurant &lt;- mavoglurant %&gt;% mutate(RACE = ifelse(RACE %in% c(7, 88), 3, RACE))\n\nNext let’s make a pairwise correlation plot for the continuous variables.\n\n#Create a subset dataset with only continuous variables\ncontinuous &lt;- mavoglurant %&gt;% select(Y, AGE, WT, HT)\n\n#Create a correlation matrix\ncorrmatrix &lt;- cor(continuous)\n\n#Create a correlation plot\ncorrplot(corrmatrix, method = \"number\")\n\n\n\n\n\n\n\n\nThe correlation plot shows that none of the pairwise correlations is excessive (i.e., above an absolute value of 0.9). We shouldn’t have much of a problem with collinearity.\nNow let’s add a new variable BMI, computed from the HT and WT variables.\n\n#Create a new variable BMI from HT and WT\nmavoglurant$BMI &lt;- mavoglurant$WT / (mavoglurant$HT)^2\n\nWe’re done with data processing and exploration, and we’re ready to fit some models."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#part-2---build-the-models",
    "href": "ml-models-exercise/ml-models-exercise.html#part-2---build-the-models",
    "title": "ML Models Exercise",
    "section": "Part 2 - Build the Models",
    "text": "Part 2 - Build the Models\nLet’s explore three models: linear regression with all predictors, LASSO regression, and random forest. First, let’s look at the linear regression model.\n\n#Linear regression\n\n##Fit the model\nmod &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nwflow1 &lt;- workflow() %&gt;% \n    add_model(mod) %&gt;% \n    add_formula(Y ~ .)\n\nfit1 &lt;- wflow1 %&gt;% fit(data = mavoglurant)\n\n##Make predictions\npredictions_linear &lt;- augment(fit1, new_data = mavoglurant) %&gt;%\n  mutate(observed = mavoglurant$Y) %&gt;%\n  rename(predicted = .pred)\n\n##Compute RMSE\nRMSE_linear &lt;- fit1 %&gt;% \n  predict(mavoglurant) %&gt;% \n  bind_cols(mavoglurant) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\nprint(RMSE_linear)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     581.   \n2 rsq     standard       0.631\n3 mae     standard     456.   \n\n##Plot observed vs. predicted values\nggplot(predictions_linear, aes(x = observed, y = predicted)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  xlim(0, 5000) +\n  ylim(0, 5000) +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for the Linear Regression Model\") +\n  theme_minimal()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nNext let’s look at the LASSO regression model.\n\n#LASSO regression\n\n##Fit the model\nlasso_recipe &lt;- recipe(Y ~ ., data = mavoglurant)\n\nlasso_spec &lt;- linear_reg(penalty = 0.1) %&gt;% set_engine(\"glmnet\")\n\nwflow2 &lt;- workflow() %&gt;%\n  add_recipe(lasso_recipe) %&gt;%\n  add_model(lasso_spec)\n\nfit2 &lt;- wflow2 %&gt;% fit(data = mavoglurant)\n\n##Make predictions\npredictions_lasso &lt;- augment(fit2, new_data = mavoglurant) %&gt;%\n  mutate(observed = mavoglurant$Y) %&gt;%\n  rename(predicted = .pred)\n\n##Compute RMSE\nRMSE_lasso &lt;- fit2 %&gt;% \n  predict(mavoglurant) %&gt;% \n  bind_cols(mavoglurant) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\nprint(RMSE_lasso)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     581.   \n2 rsq     standard       0.631\n3 mae     standard     455.   \n\n##Plot observed vs. predicted values\nggplot(predictions_lasso, aes(x = observed, y = predicted)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  xlim(0, 5000) +\n  ylim(0, 5000) +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for the Lasso Regression Model\") +\n  theme_minimal()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nFinally, let’s look at the random forest model.\n\n#Random forest\n\n##Fit the model\nrf_recipe &lt;- recipe(Y ~ ., data = mavoglurant)\n\nrf_spec &lt;- rand_forest(mode = \"regression\") %&gt;% set_engine(\"ranger\", seed = rngseed)\n\nwflow3 &lt;- workflow() %&gt;%\n  add_recipe(rf_recipe) %&gt;%\n  add_model(rf_spec)\n\nfit3 &lt;- wflow3 %&gt;% fit(data = mavoglurant)\n\n##Make predictions\npredictions_rf &lt;- augment(fit3, new_data = mavoglurant) %&gt;%\n  mutate(observed = mavoglurant$Y) %&gt;%\n  rename(predicted = .pred)\n\n##Compute RMSE\nRMSE_rf &lt;- fit3 %&gt;% \n  predict(mavoglurant) %&gt;% \n  bind_cols(mavoglurant) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\nprint(RMSE_rf)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     362.   \n2 rsq     standard       0.901\n3 mae     standard     282.   \n\n##Plot observed vs. predicted values\nggplot(predictions_rf, aes(x = observed, y = predicted)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  xlim(0, 5000) +\n  ylim(0, 5000) +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for the Random Forest Model\") +\n  theme_minimal()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nComparing the RMSE values for the three models, Model 3 with the random forest model (RMSE = 361.66) performs better than Model 1 with linear regression (RMSE = 581.42) and Model 2 with LASSO regression (RMSE = 581.47). The RMSE values and observed versus predicted plots for Model 1 and Model 2 are almost the same results. The Model 3 plot shows that the predictions are overall closer to the observations for the random forest model. Although they easily overfit, random forest models are very flexible and can capture many of the patterns seen in the data.\nLet’s try tuning the LASSO and random forest models."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#part-3---tune-the-models",
    "href": "ml-models-exercise/ml-models-exercise.html#part-3---tune-the-models",
    "title": "ML Models Exercise",
    "section": "Part 3 - Tune the Models",
    "text": "Part 3 - Tune the Models\nFirst let’s tune the LASSO model.\n\n#Define the workflow\nlasso_recipe &lt;- recipe(Y ~ ., data = mavoglurant)\n\nlasso_spec &lt;- linear_reg(penalty = tune()) %&gt;% set_engine(\"glmnet\")\n\nwflow2 &lt;- workflow() %&gt;%\n  add_recipe(lasso_recipe) %&gt;%\n  add_model(lasso_spec)\n\n#Define the penalty grid\npenalty_grid &lt;- tibble(penalty = 10^seq(-5, 2, length.out = 50))\n\n#Tune the model\ntuned_lasso &lt;- tune_grid(wflow2,\n  resamples = apparent(mavoglurant),\n  grid = penalty_grid)\n\n#Look at tuning diagnostics\nautoplot(tuned_lasso)\n\n\n\n\n\n\n\n\nThe plot shows what happened during the tuning process for the LASSO model. RMSE is a function of the penalty parameter. The LASSO model does best (lowest RMSE value) for low penalty values, and the RMSE value increases as the amount of regularization (penalty parameter) increases. At the lowest penalty, the RMSE value is the same as for the linear model (RMSE = 581). When the penalty parameter is set to its lowest value, the LASSO model essentially becomes equivalent to ordinary linear regression because it does not impose any penalties on the coefficients. As the penalty parameter increases in LASSO tuning, the model becomes simpler, more coefficients are shrunk to zero, and the RMSE generally increases.\nThen let’s tune the random forest model.\n\n#Define the workflow\nrf_recipe &lt;- recipe(Y ~ ., data = mavoglurant)\n\nrf_spec &lt;- rand_forest(mode = \"regression\", mtry = tune(), min_n = tune(), trees = 300) %&gt;% \n  set_engine(\"ranger\", seed = rngseed)\n\nwflow3 &lt;- workflow() %&gt;%\n  add_recipe(rf_recipe) %&gt;%\n  add_model(rf_spec)\n\n#Define the tuning grid\ntuning_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = 7)\n\n#Tune the model\ntuned_rf &lt;- tune_grid(wflow3,\n  resamples = apparent(mavoglurant),\n  grid = tuning_grid)\n\n#Look at tuning diagnostics\nautoplot(tuned_rf)\n\n\n\n\n\n\n\n\nThe plot shows how RMSE changes as the tuning parameters change. The best results (lowest RMSE values) are for higher values of randomly selected predictors (mtry) and lower values of minimal node size (min_n).\nNow let’s do proper tuning by using 5-fold cross-validation repeated 5 times to evaluate model performance during the tuning process. Let’s start with the LASSO model.\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Tune the model with cross-validation\ntuned_lasso_cv &lt;- tune_grid(wflow2,\n  resamples = vfold_cv(mavoglurant, v = 5, repeats = 5),\n  grid = penalty_grid)\n\n#Look at tuning diagnostics\nautoplot(tuned_lasso_cv)\n\n\n\n\n\n\n\n\nThe plot for the LASSO model tuned with cross-validation shows that the LASSO model does best (lowest RMSE value) for low penalty values, and the RMSE value increases as the amount of regularization (penalty parameter) increases.\nFinally, let’s tune the random forest model with cross-validation.\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Tune the model with cross-validation\ntuned_rf_cv &lt;- tune_grid(wflow3,\n  resamples = vfold_cv(mavoglurant, v = 5, repeats = 5),\n  grid = tuning_grid)\n\n#Look at tuning diagnostics\nautoplot(tuned_rf_cv)\n\n\n\n\n\n\n\n\nThe plot for the random forest model tuned with cross-validation shows that the best results (lowest RMSE values) are for higher values of randomly selected predictors (mtry) and lower values of minimal node size (min_n). Compared to the random forest model, the RMSE values are lower for the LASSO model. LASSO regularization helps to mitigate overfitting by penalizing model complexity. Random forests can still be sensitive to noisy data or when the number of trees in the forest is too high. We conclude that the LASSO model performs better."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Ranni Tewfik Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at C:/Users/ranni/Desktop/MADA/rannitewfik-MADA-portfolio\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts             \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2       \n2 Blood                 0             1 FALSE          4 A: 4, B: 2, O: 2, AB: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 IQ                    0             1 108.  10.4  90 103 105 116  125 ▂▅▇▅▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nWeight as a function of IQ\n\np5 &lt;- mydata %&gt;% ggplot(aes(x=Weight, y=IQ)) + geom_point() + geom_smooth(method='lm')\nplot(p5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-IQ.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nBoxplot: Blood type on x-axis, height on y-axis\n\np6 &lt;- mydata %&gt;% ggplot(aes(x=Blood, y=Height)) + geom_boxplot()\nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"Boxplot_Blood type-Height.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at C:/Users/ranni/Desktop/MADA/rannitewfik-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Ranni Tewfik Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Ranni Tewfik Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Ranni Tewfik Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Ranni Tewfik Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Ranni Tewfik Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "data-exercise/data-exercise.html#part-1---generating-synthetic-data",
    "href": "data-exercise/data-exercise.html#part-1---generating-synthetic-data",
    "title": "Data Exercise",
    "section": "",
    "text": "First, let’s generate a synthetic dataset with 100 observations and 5 variables: age, sex, hypertension, cholesterol, and heart_health.\n\n#Load required packages\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.3.2\n\n#Set seed for reproducibility\nset.seed(123)\n\n#Set sample size\nn &lt;- 100\n\n#Create ages between 18 and 79\nage &lt;- sample(18:79, n, replace = TRUE)\n\n#Create sexes (male or female)\nsex &lt;- sample(c(\"male\", \"female\"), n, replace = TRUE)\n\n#Create hypertension status (yes or no)\nhypertension &lt;- sample(c(\"yes\", \"no\"), n, replace = TRUE)\n\n#Create total cholesterol levels between 125 and 275, assuming males have higher total cholesterol than females\ncholesterol &lt;- ifelse(sex == \"male\", rnorm(n, mean = 220, sd = 20), rnorm(n, mean = 200, sd = 20))\ncholesterol &lt;- pmax(pmin(cholesterol, 275), 125)\n\n#Create heart health scores between 25 and 90, assuming those with higher total cholesterol have higher heart health scores\nheart_health &lt;- 50 + 0.2 * cholesterol + rnorm(n, mean = 0, sd = 10)\nheart_health &lt;- pmax(pmin(heart_health, 90), 25)\n\n#Create a data frame to store the generated data\nheart &lt;- data.frame(age = age, sex = sex, hypertension = hypertension, cholesterol = cholesterol, heart_health = heart_health)\n\n#Get an overview and summary of the data\nstr(heart)\n\n'data.frame':   100 obs. of  5 variables:\n $ age         : int  48 32 68 31 20 59 67 71 60 54 ...\n $ sex         : chr  \"male\" \"female\" \"female\" \"male\" ...\n $ hypertension: chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ cholesterol : num  232 182 200 220 218 ...\n $ heart_health: num  90 89.7 86.9 90 90 ...\n\nsummary(heart)\n\n      age            sex            hypertension        cholesterol   \n Min.   :20.00   Length:100         Length:100         Min.   :158.2  \n 1st Qu.:32.00   Class :character   Class :character   1st Qu.:194.2  \n Median :46.00   Mode  :character   Mode  :character   Median :209.5  \n Mean   :47.56                                         Mean   :209.3  \n 3rd Qu.:60.50                                         3rd Qu.:225.9  \n Max.   :79.00                                         Max.   :263.9  \n  heart_health  \n Min.   :57.23  \n 1st Qu.:84.12  \n Median :90.00  \n Mean   :85.84  \n 3rd Qu.:90.00  \n Max.   :90.00"
  },
  {
    "objectID": "data-exercise/data-exercise.html#part-3---exploring-data",
    "href": "data-exercise/data-exercise.html#part-3---exploring-data",
    "title": "Data Exercise",
    "section": "Part 3 - Exploring Data",
    "text": "Part 3 - Exploring Data\nLet’s explore each variable in the dataset, starting with the outcome variable, heart_health.\n\n#Summary statistics and histogram for heart_health\nheart$heart_health &lt;- as.numeric(heart$heart_health)\nsummary(heart$heart_health)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  57.23   84.12   90.00   85.84   90.00   90.00 \n\nggplot(heart, aes(x = heart_health)) +\n  geom_histogram() +\n  labs(x = \"Heart Health Score\", y = \"Frequency\", title = \"Histogram of Heart Health Score\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean (86) is less than the median (90) for the variable heart_health. The histogram of heart_health shows a left-skewing distribution with most observations at around x = 90.\nNext let’s explore the variable age.\n\n#Summary statistics and histogram for age\nheart$age &lt;- as.numeric(heart$age)\nsummary(heart$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  20.00   32.00   46.00   47.56   60.50   79.00 \n\nggplot(heart, aes(x = age)) +\n  geom_histogram() +\n  labs(x = \"Age in Years\", y = \"Frequency\", title = \"Histogram of Age\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean (48) is slightly greater than the median (46) for the variable age. The histogram of age shows a relatively normal distribution.\nNow let’s explore the variable cholesterol.\n\n#Summary statistics and histogram for cholesterol\nheart$cholesterol &lt;- as.numeric(heart$cholesterol)\nsummary(heart$cholesterol)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  158.2   194.2   209.5   209.3   225.9   263.9 \n\nggplot(heart, aes(x = cholesterol)) +\n  geom_histogram() +\n  labs(x = \"Cholesterol\", y = \"Frequency\", title = \"Histogram of Cholesterol\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean and median for the variable cholesterol are the same (209). The histogram of cholesterol shows a relatively normal distribution.\nNext let’s explore the variable sex.\n\n#Summary tables for the variable sex\ntable(heart$sex, useNA = \"always\")\n\n\nfemale   male   &lt;NA&gt; \n    55     45      0 \n\nprop.table(table(heart$sex))\n\n\nfemale   male \n  0.55   0.45 \n\n\n55% of the observations are female, and 45% are male. There are no missing observations for sex.\nFinally, let’s explore the variable hypertension.\n\n#Summary tables for the variable hypertension\ntable(heart$hypertension, useNA = \"always\")\n\n\n  no  yes &lt;NA&gt; \n  48   52    0 \n\nprop.table(table(heart$hypertension))\n\n\n  no  yes \n0.48 0.52 \n\n\n52% of the observations have hypertension, and 48% do not have hypertension. There are no missing observations for sex.\nHere is a summary table of the outcome and predictors, stratified by sex.\n\n#Create a summary table of the variables, stratified by sex\nheart %&gt;% tbl_summary(by = sex, statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;%\n  modify_caption(\"Summary Table of Heart Health Data, Stratified by Sex\")\n\n\n\n\n\nSummary Table of Heart Health Data, Stratified by Sex\n\n\nCharacteristic\nfemale, N = 551\nmale, N = 451\n\n\n\n\nage\n47 (17)\n48 (16)\n\n\nhypertension\n28 (51%)\n24 (53%)\n\n\ncholesterol\n199 (19)\n221 (19)\n\n\nheart_health\n84.6 (7.7)\n87.4 (5.2)\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n\n\n\nHere is another summary table of the outcome and predictors, stratified by hypertension.\n\n#Create a summary table of the variables, stratified by hypertension\nheart %&gt;% tbl_summary(by = hypertension, statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;%\n  modify_caption(\"Summary Table of Heart Health Data, Stratified by Hypertension\")\n\n\n\n\n\nSummary Table of Heart Health Data, Stratified by Hypertension\n\n\n\n\n\n\n\nCharacteristic\nno, N = 481\nyes, N = 521\n\n\n\n\nage\n46 (17)\n49 (17)\n\n\nsex\n\n\n\n\n\n\n    female\n27 (56%)\n28 (54%)\n\n\n    male\n21 (44%)\n24 (46%)\n\n\ncholesterol\n206 (23)\n212 (21)\n\n\nheart_health\n85.3 (7.0)\n86.4 (6.6)\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n\n\n\nNow let’s create some scatterplots between the main outcome (heart_health) and the continuous predictors, as well as a correlation table.\n\n#Create scatterplots between heart_health and the continuous predictors\npairs(cbind(heart_health, age, cholesterol))\n\n\n\n\n\n\n\n#Create a correlation matrix of the continuous variables\ncor(cbind(heart_health, age, cholesterol))\n\n             heart_health        age cholesterol\nheart_health    1.0000000 -0.1070062   0.3763685\nage            -0.1070062  1.0000000  -0.1262592\ncholesterol     0.3763685 -0.1262592   1.0000000\n\n\nBased on the scatterplots and the correlation matrix, there doesn’t seem to be a strong correlation between the outcome (heart_health) and any of the continuous predictors (age, cholesterol).\nLet’s also look at the boxplots between the main outcome and the categorical variables.\n\n#Create a boxplot of sex and heart_health\nggplot(heart, aes(x = factor(sex), y = heart_health)) +\n  geom_boxplot() +\n  labs(x = \"Sex\", y = \"Heart Health Score\", title = \"Boxplot of Sex and Heart Health Score\")\n\n\n\n\n\n\n\n#Create a boxplot of hypertension and heart_health\nggplot(heart, aes(x = factor(hypertension), y = heart_health)) +\n  geom_boxplot() +\n  labs(x = \"Hypertension\", y = \"Heart Health Score\", title = \"Boxplot of Hypertension and Heart Health Score\")\n\n\n\n\n\n\n\n\nThe boxplots of heart_health and the categorical variables (sex, hypertension) show negatively-skewed distributions.\nLet’s also look at the stratified boxplots.\n\n#Create a boxplot of sex and heart_health, stratified by hypertension\nggplot(heart, aes(x = factor(sex), y = heart_health, fill = factor(hypertension))) +\n  geom_boxplot() +\n  labs(x = \"Sex\", y = \"Heart Health Score\", title = \"Boxplot of Sex and Heart Health Score, Stratified by Hypertension\") +\n  scale_fill_discrete(name = \"Hypertension\")\n\n\n\n\n\n\n\n#Create a boxplot of hypertension and heart_health, stratified by sex\nggplot(heart, aes(x = factor(hypertension), y = heart_health, fill = factor(sex))) +\n  geom_boxplot() +\n  labs(x = \"Hypertension\", y = \"Heart Health Score\", title = \"Boxplot of Hypertension and Heart Health Score, Stratified by Sex\") +\n  scale_fill_discrete(name = \"Sex\")\n\n\n\n\n\n\n\n\nThe stratified boxlplots show negatively-skewed distributions across all strata.\nNext let’s plot heart health as a function of age for males and females.\n\n#Plot heart health as a function of age for males and females\nggplot(heart_males, aes(x = age, y = heart_health)) + geom_point() + ggtitle(\"Age and Heart Health Score Among Males\")\n\n\n\n\n\n\n\nggplot(heart_females, aes(x = age, y = heart_health)) + geom_point() + ggtitle(\"Age and Heart Health Score Among Females\")\n\n\n\n\n\n\n\n\nThere is no noticeable correlation between age and heart health score for either males or females. There are several data points at heart health = 90, which is more than expected. This study sample may be healthier than the average population.\nThen let’s plot heart health as a function of cholesterol for males and females.\n\n#Plot heart health as a function of cholesterol for males and females\nggplot(heart_males, aes(x = cholesterol, y = heart_health)) + geom_point() + ggtitle(\"Total Cholesterol and Heart Health Score Among Males\")\n\n\n\n\n\n\n\nggplot(heart_females, aes(x = cholesterol, y = heart_health)) + geom_point() + ggtitle(\"Total Cholesterol and Heart Health Score Among Females\")\n\n\n\n\n\n\n\n\nThere is a positive correlation between total cholesterol and heart health score for males but no noticeable correlation or females. There are several data points at heart health = 90, which is more than expected. This study sample may be healthier than the average population.\nNow let’s plot heart health as a function of age for those with hypertension.\n\n#Plot heart health as a function of age for those with hypertension\nggplot(heart_hypertension, aes(x = age, y = heart_health)) + geom_point() + ggtitle(\"Age and Heart Health Score Among Those With Hypertension\")\n\n\n\n\n\n\n\n\nThere is no noticeable correlation between age and heart health score for those with hypertension. There are several data points at heart health = 90, which is more than expected. This study sample may be healthier than the average population.\nFinally, let’s plot heart health as a function of cholesterol for those with hypertension.\n\n#Plot heart health as a function of cholesterol for those with hypertension\nggplot(heart_hypertension, aes(x = cholesterol, y = heart_health)) + geom_point() + ggtitle(\"Total Cholesterol and Heart Health Score Among Those With Hypertension\")\n\n\n\n\n\n\n\n\nThere is a positive correlation between total cholesterol and heart health score for those with hypertension. There are several data points at heart health = 90, which is more than expected. This study sample may be healthier than the average population."
  },
  {
    "objectID": "data-exercise/data-exercise.html#part-4---fitting-data",
    "href": "data-exercise/data-exercise.html#part-4---fitting-data",
    "title": "Data Exercise",
    "section": "Part 4 - Fitting Data",
    "text": "Part 4 - Fitting Data\nLet’s fit a few simple models to the data. Let’s start with fitting heart health as the outcome and age and cholesterol as the predictors for males.\n\n#Fit heart health as the outcome and age and cholesterol as the predictors for males\nmales &lt;- lm(heart_health ~ age + cholesterol, data = heart_males)\nsummary(males)\n\n\nCall:\nlm(formula = heart_health ~ age + cholesterol, data = heart_males)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.072  -1.271   1.130   2.627   7.094 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 62.31682    8.77786   7.099 1.05e-08 ***\nage          0.02096    0.04466   0.469  0.64128    \ncholesterol  0.10870    0.03826   2.841  0.00691 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.848 on 42 degrees of freedom\nMultiple R-squared:  0.1646,    Adjusted R-squared:  0.1248 \nF-statistic: 4.138 on 2 and 42 DF,  p-value: 0.0229\n\n\nAmong males, total cholesterol is significantly associated with heart health score (p-value = 0.01), controlling for age; however, age is not significantly associated with hearth health score, controlling for total cholesterol (p-value = 0.64). The low R-squared value indicates that only 16% of the variation in heart health score is explained by the model.\nLet’s do the same model fitting for females.\n\n#Fit heart health as the outcome and age and cholesterol as the predictors for females\nfemales &lt;- lm(heart_health ~ age + cholesterol, data = heart_females)\nsummary(females)\n\n\nCall:\nlm(formula = heart_health ~ age + cholesterol, data = heart_females)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.401  -2.929   3.104   4.628   7.485 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 67.57861   12.25700   5.513 1.12e-06 ***\nage         -0.06280    0.06099  -1.030   0.3079    \ncholesterol  0.10005    0.05554   1.801   0.0774 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.405 on 52 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.06671 \nF-statistic:  2.93 on 2 and 52 DF,  p-value: 0.06227\n\n\nAmong females, neither age (p-value = 0.31) nor total cholesterol (p-value = 0.08) is significantly associated with heart health, controlling for other variables. The low R-squared value indicates that only 10% of the variation in heart health score is explained by the model.\nFinally, let’s fit heart health as the outcome and age and cholesterol as the predictors for those with hypertension.\n\n#Fit heart health as the outcome and age and cholesterol as the predictors for those with hypertension\nhtn &lt;- lm(heart_health ~ age + cholesterol, data = heart_hypertension)\nsummary(htn)\n\n\nCall:\nlm(formula = heart_health ~ age + cholesterol, data = heart_hypertension)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.578  -1.344   2.032   4.033   6.385 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 64.19107    9.56511   6.711 1.86e-08 ***\nage         -0.03406    0.05325  -0.640   0.5255    \ncholesterol  0.11228    0.04255   2.639   0.0111 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.276 on 49 degrees of freedom\nMultiple R-squared:  0.1338,    Adjusted R-squared:  0.09839 \nF-statistic: 3.783 on 2 and 49 DF,  p-value: 0.02966\n\n\nAmong those with hypertension, total cholesterol is significantly associated with heart health score (p-value = 0.01), controlling for age; however, age is not significantly associated with hearth health score, controlling for total cholesterol (p-value = 0.53). The low R-squared value indicates that only 13% of the variation in heart health score is explained by the model."
  }
]