---
title: "ML Models Exercise"
---


## Part 1 - Process & Explore the Data


Data on a drug candidate called Mavoglurant is available in this [GitHub](https://github.com/metrumresearchgroup/BayesPBPK-tutorial) repository. First, let's further process the dataset from the "Fitting Exercise" to make it ready for model building.


```{r}
#Load the required packages
library(dplyr)
library(tidymodels)
library(ggplot2)
library(corrplot)
library(glmnet)
library(ranger)

#Set a seed
rngseed = 1234
set.seed(rngseed)

#Load the data
mavoglurant <- readRDS("mavoglurant_processed.rds")

#Get an overview and summary of the data
str(mavoglurant)
summary(mavoglurant)

#Convert the SEX variable to numeric
mavoglurant$SEX <- as.numeric(mavoglurant$SEX)

#Drop the RATE variable
mavoglurant <- mavoglurant %>% select(-RATE)

#Combine the "7" and "88" categories in the RACE variable into a single category called "3"
mavoglurant <- mavoglurant %>% mutate(RACE = ifelse(RACE %in% c(7, 88), 3, RACE))
```


Next let's make a pairwise correlation plot for the continuous variables.


```{r}
#Create a subset dataset with only continuous variables
continuous <- mavoglurant %>% select(Y, AGE, WT, HT)

#Create a correlation matrix
corrmatrix <- cor(continuous)

#Create a correlation plot
corrplot(corrmatrix, method = "number")
```


The correlation plot shows that none of the pairwise correlations is excessive (i.e., above an absolute value of 0.9). We shouldn't have much of a problem with collinearity.


Now let's add a new variable BMI, computed from the HT and WT variables.


```{r}
#Create a new variable BMI from HT and WT
mavoglurant$BMI <- mavoglurant$WT / (mavoglurant$HT)^2
```


We're done with data processing and exploration, and we're ready to fit some models.


## Part 2 - Build the Models


Let's explore three models: linear regression with all predictors, LASSO regression, and random forest. First, let's look at the linear regression model.


```{r}
#Linear regression

##Fit the model
mod <- linear_reg() %>% set_engine("lm")

wflow1 <- workflow() %>% 
	add_model(mod) %>% 
	add_formula(Y ~ .)

fit1 <- wflow1 %>% fit(data = mavoglurant)

##Make predictions
predictions_linear <- augment(fit1, new_data = mavoglurant) %>%
  mutate(observed = mavoglurant$Y) %>%
  rename(predicted = .pred)

##Compute RMSE
RMSE_linear <- fit1 %>% 
  predict(mavoglurant) %>% 
  bind_cols(mavoglurant) %>% 
  metrics(truth = Y, estimate = .pred)

print(RMSE_linear)

##Plot observed vs. predicted values
ggplot(predictions_linear, aes(x = observed, y = predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  xlim(0, 5000) +
  ylim(0, 5000) +
  labs(x = "Observed Values", y = "Predicted Values", title = "Observed vs. Predicted Values for the Linear Regression Model") +
  theme_minimal()
```


Next let's look at the LASSO regression model.


```{r}
#LASSO regression

##Fit the model
lasso_recipe <- recipe(Y ~ ., data = mavoglurant)

lasso_spec <- linear_reg(penalty = 0.1) %>% set_engine("glmnet")

wflow2 <- workflow() %>%
  add_recipe(lasso_recipe) %>%
  add_model(lasso_spec)

fit2 <- wflow2 %>% fit(data = mavoglurant)

##Make predictions
predictions_lasso <- augment(fit2, new_data = mavoglurant) %>%
  mutate(observed = mavoglurant$Y) %>%
  rename(predicted = .pred)

##Compute RMSE
RMSE_lasso <- fit2 %>% 
  predict(mavoglurant) %>% 
  bind_cols(mavoglurant) %>% 
  metrics(truth = Y, estimate = .pred)

print(RMSE_lasso)

##Plot observed vs. predicted values
ggplot(predictions_lasso, aes(x = observed, y = predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  xlim(0, 5000) +
  ylim(0, 5000) +
  labs(x = "Observed Values", y = "Predicted Values", title = "Observed vs. Predicted Values for the Lasso Regression Model") +
  theme_minimal()
```


Finally, let's look at the random forest model.


```{r}
#Random forest

##Fit the model
rf_recipe <- recipe(Y ~ ., data = mavoglurant)

rf_spec <- rand_forest(mode = "regression") %>% set_engine("ranger", seed = rngseed)

wflow3 <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_spec)

fit3 <- wflow3 %>% fit(data = mavoglurant)

##Make predictions
predictions_rf <- augment(fit3, new_data = mavoglurant) %>%
  mutate(observed = mavoglurant$Y) %>%
  rename(predicted = .pred)

##Compute RMSE
RMSE_rf <- fit3 %>% 
  predict(mavoglurant) %>% 
  bind_cols(mavoglurant) %>% 
  metrics(truth = Y, estimate = .pred)

print(RMSE_rf)

##Plot observed vs. predicted values
ggplot(predictions_rf, aes(x = observed, y = predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  xlim(0, 5000) +
  ylim(0, 5000) +
  labs(x = "Observed Values", y = "Predicted Values", title = "Observed vs. Predicted Values for the Random Forest Model") +
  theme_minimal()
```


Comparing the RMSE values for the three models, Model 3 with the random forest model (RMSE = 361.66) performs better than Model 1 with linear regression (RMSE = 581.42) and Model 2 with LASSO regression (RMSE = 581.47). The RMSE values and observed versus predicted plots for Model 1 and Model 2 are almost the same results. The Model 3 plot shows that the predictions are overall closer to the observations for the random forest model. Although they easily overfit, random forest models are very flexible and can capture many of the patterns seen in the data.


Let's try tuning the LASSO and random forest models.


## Part 3 - Tune the Models


First let's tune the LASSO model.


```{r}
#Define the workflow
lasso_recipe <- recipe(Y ~ ., data = mavoglurant)

lasso_spec <- linear_reg(penalty = tune()) %>% set_engine("glmnet")

wflow2 <- workflow() %>%
  add_recipe(lasso_recipe) %>%
  add_model(lasso_spec)

#Define the penalty grid
penalty_grid <- tibble(penalty = 10^seq(-5, 2, length.out = 50))

#Tune the model
tuned_lasso <- tune_grid(wflow2,
  resamples = apparent(mavoglurant),
  grid = penalty_grid)

#Look at tuning diagnostics
autoplot(tuned_lasso)
```


The plot shows what happened during the tuning process for the LASSO model. RMSE is a function of the penalty parameter. The LASSO model does best (lowest RMSE value) for low penalty values, and the RMSE value increases as the amount of regularization (penalty parameter) increases. At the lowest penalty, the RMSE value is the same as for the linear model (RMSE = 581). When the penalty parameter is set to its lowest value, the LASSO model essentially becomes equivalent to ordinary linear regression because it does not impose any penalties on the coefficients. As the penalty parameter increases in LASSO tuning, the model becomes simpler, more coefficients are shrunk to zero, and the RMSE generally increases.


Then let's tune the random forest model.


```{r}
#Define the workflow
rf_recipe <- recipe(Y ~ ., data = mavoglurant)

rf_spec <- rand_forest(mode = "regression", mtry = tune(), min_n = tune(), trees = 300) %>% 
  set_engine("ranger", seed = rngseed)

wflow3 <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_spec)

#Define the tuning grid
tuning_grid <- grid_regular(
  mtry(range = c(1, 7)),
  min_n(range = c(1, 21)),
  levels = 7)

#Tune the model
tuned_rf <- tune_grid(wflow3,
  resamples = apparent(mavoglurant),
  grid = tuning_grid)

#Look at tuning diagnostics
autoplot(tuned_rf)
```


The plot shows how RMSE changes as the tuning parameters change. The best results (lowest RMSE values) are for higher values of randomly selected predictors (mtry) and lower values of minimal node size (min_n).


Now let's do proper tuning by using 5-fold cross-validation repeated 5 times to evaluate model performance during the tuning process. Let's start with the LASSO model.


```{r}
#Set a seed
rngseed = 1234
set.seed(rngseed)

#Tune the model with cross-validation
tuned_lasso_cv <- tune_grid(wflow2,
  resamples = vfold_cv(mavoglurant, v = 5, repeats = 5),
  grid = penalty_grid)

#Look at tuning diagnostics
autoplot(tuned_lasso_cv)
```


The plot for the LASSO model tuned with cross-validation shows that the LASSO model does best (lowest RMSE value) for low penalty values, and the RMSE value increases as the amount of regularization (penalty parameter) increases.


Finally, let's tune the random forest model with cross-validation.


```{r}
#Set a seed
rngseed = 1234
set.seed(rngseed)

#Tune the model with cross-validation
tuned_rf_cv <- tune_grid(wflow3,
  resamples = vfold_cv(mavoglurant, v = 5, repeats = 5),
  grid = tuning_grid)

#Look at tuning diagnostics
autoplot(tuned_rf_cv)
```


The plot for the random forest model tuned with cross-validation shows that the best results (lowest RMSE values) are for higher values of randomly selected predictors (mtry) and lower values of minimal node size (min_n). Compared to the random forest model, the RMSE values are lower for the LASSO model. LASSO regularization helps to mitigate overfitting by penalizing model complexity. Random forests can still be sensitive to noisy data or when the number of trees in the forest is too high. We conclude that the LASSO model performs better.

